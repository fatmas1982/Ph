{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/python3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import csv\n",
    "from tensorflow.python.client import timeline\n",
    "import pandas as pd\n",
    "import math\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "\n",
    "#path_data_source=dir_path+\"/data/data_source/\"\n",
    "path_data_source=\"/home/fsg/Desktop/\"\n",
    "\n",
    "#path_data_base=dir_path+\"/data/database/csv/\"\n",
    "\n",
    "\n",
    "path_data_base=\"/home/fsg/Desktop/csv/\"\n",
    "\n",
    "\n",
    "files_path_data_source=\"files/\"\n",
    "\n",
    "#files_path_data_source=\"demo/\"\n",
    "\n",
    "#sub_path_data_source=\"small/\"\n",
    "\n",
    "file_path=path_data_source+files_path_data_source\n",
    "\n",
    "file_names = [os.path.join(file_path, f) \n",
    "                      for f in os.listdir(file_path) \n",
    "                      if f.endswith(\".txt\")]\n",
    "\n",
    "\n",
    "path_tf=\"sub_tf/\"\n",
    "path_idf=\"sub_idf/\"\n",
    "path_tfidf=\"sub_tfidf/\"\n",
    "path_sim_permutation=\"semantics/permutation/\"\n",
    "path_sim=\"semantics/sim/\"\n",
    "path_topic_document=\"topic_document/\"\n",
    "\n",
    "file_path_tf=path_data_base+path_tf\n",
    "file_path_idf=path_data_base+path_idf\n",
    "file_path_tfidf=path_data_base+path_tfidf\n",
    "file_path_sim_permutation=path_data_base+path_sim_permutation\n",
    "file_path_sim=path_data_base+path_sim\n",
    "file_path_topic_document=path_data_base+path_topic_document\n",
    "\n",
    "file_names_tf = [os.path.join(file_path_tf, f) \n",
    "                      for f in os.listdir(file_path_tf) \n",
    "                      if f.endswith(\".csv\")]\n",
    "file_names_idf = [os.path.join(file_path_idf, f) \n",
    "                      for f in os.listdir(file_path_idf) \n",
    "                      if f.endswith(\".csv\")]\n",
    "\n",
    "\n",
    "file_names_tfidf = [os.path.join(file_path_tfidf, f) \n",
    "                      for f in os.listdir(file_path_tfidf) \n",
    "                      if f.endswith(\".csv\")]\n",
    "\n",
    "\n",
    "file_names_permutation = [os.path.join(file_path_sim_permutation, f) \n",
    "                      for f in os.listdir(file_path_sim_permutation) \n",
    "                      if f.endswith(\".csv\")]\n",
    "\n",
    "\n",
    "file_names_sim = [os.path.join(file_path_sim, f) \n",
    "                      for f in os.listdir(file_path_sim) \n",
    "                      if f.endswith(\".csv\")]\n",
    "\n",
    "file_names_topic_document = [os.path.join(file_path_topic_document, f) \n",
    "                      for f in os.listdir(file_path_topic_document) \n",
    "                      if f.endswith(\".csv\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gpuN=\"0\"#sys.argv[1]\n",
    "#all_gpus =\"2\"#sys.argv[2]\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = gpuN\n",
    "#gpu_name='/gpu:'+gpuN\n",
    "\n",
    "task_no=\"11\"#sys.argv[1]\n",
    "all_gpus =\"2\"#sys.argv[2]\n",
    "cuda=\"0,1\"#sys.argv[3]\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess, re, os, sys #https://github.com/yaroslavvb/stuff/blob/master/notebook_util.py\n",
    "def run_command(cmd):\n",
    "    \"\"\"Run command, return output as string.\"\"\"\n",
    "    \n",
    "    output = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True).communicate()[0]\n",
    "    return output.decode(\"ascii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_available_gpus():\n",
    "    \"\"\"Returns list of available GPU ids.\"\"\"\n",
    "    \n",
    "    output = run_command(\"nvidia-smi -L\")\n",
    "    # lines of the form GPU 0: TITAN X\n",
    "    gpu_regex = re.compile(r\"GPU (?P<gpu_id>\\d+):\")\n",
    "    result = []\n",
    "    for line in output.strip().split(\"\\n\"):\n",
    "        m = gpu_regex.match(line)\n",
    "        assert m, \"Couldnt parse \"+line\n",
    "        result.append(int(m.group(\"gpu_id\")))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_txt(txt,file):\n",
    "    text_file = open(file, \"w\")\n",
    "    text_file.write(txt)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gpu_memory_map(gpu_memory_file,gpu_output_file):\n",
    "    \"\"\"Returns map of GPU id to memory allocated on that GPU.\"\"\"\n",
    "\n",
    "    output = run_command(\"nvidia-smi\")\n",
    "    save_txt(output,gpu_output_file)\n",
    "    #print(\"nvidia-smi\",output)\n",
    "    gpu_output = output[output.find(\"GPU Memory\"):]\n",
    "    #print(\"GPU Memory\",gpu_output)\n",
    "    save_txt(gpu_output,gpu_memory_file)\n",
    "   \n",
    "    # lines of the form\n",
    "    # |    0      8734    C   python                                       11705MiB |\n",
    "    memory_regex = re.compile(r\"[|]\\s+?(?P<gpu_id>\\d+)\\D+?(?P<pid>\\d+).+[ ](?P<gpu_memory>\\d+)MiB\")\n",
    "    #print(\"memory_regex\",memory_regex)\n",
    "    rows = gpu_output.split(\"\\n\")\n",
    "    #print(\"rows\",rows)\n",
    "    result = {gpu_id: 0 for gpu_id in list_available_gpus()}\n",
    "    #print(\"result\",result)\n",
    "    for row in gpu_output.split(\"\\n\"):\n",
    "        m = memory_regex.search(row)\n",
    "        if not m:\n",
    "            continue\n",
    "        gpu_id = int(m.group(\"gpu_id\"))\n",
    "        gpu_memory = int(m.group(\"gpu_memory\"))\n",
    "        result[gpu_id] += gpu_memory\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_cvs_by_pands(path_database,file_database,index_col, header):\n",
    "    import csv\n",
    "    import pandas as pd\n",
    "    return pd.read_csv(path_database+file_database,index_col=index_col,header=header)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_cvs_by_pands_full_path(full_path,index_col, header):\n",
    "    import csv\n",
    "    import pandas as pd\n",
    "    return pd.read_csv(full_path,index_col=index_col,header=header)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_df_to_csv(df,path_database,sub_path,new_file_name):\n",
    "     df.to_csv(path_database+sub_path+new_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_to_dict(df):\n",
    "        \n",
    "    dic={}\n",
    "    keys=df.keys()\n",
    "    #print(keys)\n",
    "    \n",
    "    values= df.T.values.tolist()\n",
    "    #print(len(values))\n",
    "    for i in range(len(keys)):\n",
    "        #print(keys[i])\n",
    "        dic[keys[i]]=magic(values[i])\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def magic(numList):         # [1,2,3]\n",
    "    s = map(str, numList)   # ['1','2','3']\n",
    "    s = ''.join(s)          # '123'\n",
    "    s = int(s)              # 123\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_to_dict(list_full_path):\n",
    "    dic={}\n",
    "    for file_path in list_full_path:\n",
    "        index=full_name_file(file_path)\n",
    "        dic[index]=file_path\n",
    "\n",
    "    return dic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic as wnic\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def similarity_by_infocontent(sense1, sense2, option):\n",
    "    #sense1=\"Synset('\"+sense1+\"')\"\n",
    "    #sense2=\"Synset('\"+sense2+\"')\"\n",
    "    #print(sense1,sense2)\n",
    "    sense1 = wn.synset(sense1)\n",
    "    sense2 = wn.synset(sense2)\n",
    "    #print(sense1,sense2)\n",
    "    \"\"\" Returns similarity scores by information content. \"\"\"\n",
    "    #if sense1.pos != sense2.pos: # infocontent sim can't do diff POS.\n",
    "        #return 0\n",
    "\n",
    "    info_contents = ['ic-bnc-add1.dat', 'ic-bnc-resnik-add1.dat', \n",
    "                     'ic-bnc-resnik.dat', 'ic-bnc.dat', \n",
    "\n",
    "                     'ic-brown-add1.dat', 'ic-brown-resnik-add1.dat', \n",
    "                     'ic-brown-resnik.dat', 'ic-brown.dat', \n",
    "\n",
    "                     'ic-semcor-add1.dat', 'ic-semcor.dat',\n",
    "\n",
    "                     'ic-semcorraw-add1.dat', 'ic-semcorraw-resnik-add1.dat', \n",
    "                     'ic-semcorraw-resnik.dat', 'ic-semcorraw.dat', \n",
    "\n",
    "                     'ic-shaks-add1.dat', 'ic-shaks-resnik.dat', \n",
    "                     'ic-shaks-resnink-add1.dat', 'ic-shaks.dat', \n",
    "\n",
    "                     'ic-treebank-add1.dat', 'ic-treebank-resnik-add1.dat', \n",
    "                     'ic-treebank-resnik.dat', 'ic-treebank.dat']\n",
    "\n",
    "    if option in ['res', 'resnik']:\n",
    "        #return wn.res_similarity(sense1, sense2, wnic.ic('ic-bnc-resnik-add1.dat'))\n",
    "        #print('simRe snik (c1,c2) = -log p(lso(c1,c2)) = IC(lso(c1,c2)')\n",
    "        return wn.res_similarity(sense1, sense2, wnic.ic('ic-treebank-resnik-add1.dat'))\n",
    "    #return min(wn.res_similarity(sense1, sense2, wnic.ic(ic)) \\\n",
    "    #             for ic in info_contents)\n",
    "\n",
    "    elif option in ['jcn', \"jiang-conrath\"]:\n",
    "        #return wn.jcn_similarity(sense1, sense2, wnic.ic('ic-bnc-add1.dat'))\n",
    "        #print('sim(jcn) (c1,c2 )= (IC(c1) + IC(c2 )) - 2IC(lso(c1,c2 ))')\n",
    "        return wn.jcn_similarity(sense1, sense2, wnic.ic('ic-treebank.dat'))\n",
    "\n",
    "    elif option in ['lin']:\n",
    "        #return wn.lin_similarity(sense1, sense2, wnic.ic('ic-bnc-add1.dat'))\n",
    "        #print('sim(lin) (c1,c2)=(2IC(lso(c1,c2 )))/(IC(c1)+IC(c2))')\n",
    "        return wn.lin_similarity(sense1, sense2, wnic.ic('ic-treebank.dat'))\n",
    "\n",
    "def sim(sense1, sense2, option=\"path\"):\n",
    "    \"\"\" Calculates similarity based on user's choice. \"\"\"\n",
    "    option = option.lower()\n",
    "    if option.lower() in [\"path\", \"path_similarity\", \n",
    "                        \"wup\", \"wupa\", \"wu-palmer\", \"wu-palmer\",\n",
    "                        'lch', \"leacock-chordorow\"]:\n",
    "        return similarity_by_path(sense1, sense2, option) \n",
    "    elif option.lower() in [\"res\", \"resnik\",\n",
    "                          \"jcn\",\"jiang-conrath\",\n",
    "                          \"lin\"]:\n",
    "        return similarity_by_infocontent(sense1, sense2, option)\n",
    "\n",
    "def max_similarity(context_sentence, ambiguous_word, option=\"path\", \n",
    "                   pos=None, best=True):\n",
    "    \"\"\"\n",
    "    Perform WSD by maximizing the sum of maximum similarity between possible \n",
    "    synsets of all words in the context sentence and the possible synsets of the \n",
    "    ambiguous words (see http://goo.gl/XMq2BI):\n",
    "    {argmax}_{synset(a)}(\\sum_{i}^{n}{{max}_{synset(i)}(sim(i,a))}\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for i in wn.synsets(ambiguous_word):\n",
    "        try:\n",
    "            if pos and pos != str(i.pos()):\n",
    "                continue\n",
    "        except:\n",
    "            if pos and pos != str(i.pos):\n",
    "                continue\n",
    "        result[i] = sum(max([sim(i,k,option) for k in wn.synsets(j)]+[0]) \\\n",
    "                        for j in word_tokenize(context_sentence))\n",
    "\n",
    "    if option in [\"res\",\"resnik\"]: # lower score = more similar\n",
    "        result = sorted([(v,k) for k,v in result.items()])\n",
    "    else: # higher score = more similar\n",
    "        result = sorted([(v,k) for k,v in result.items()],reverse=True)\n",
    "    #print (result)\n",
    "    if best: return result[0][1];\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isfile_empty(file_path_name):\n",
    "    f=open(file_path_name, 'r') \n",
    "    is_blank = len(f.read().strip()) == 0\n",
    "    return is_blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def full_name_file(full_name_path):#like cs.csv\n",
    "    d=full_name_path.split(\"/\")\n",
    "    ##print(d)\n",
    "    name=d[len(d)-1]#.split(\".\")\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_last_file_list(file_path,extention):\n",
    "    \n",
    "    file_names = [os.path.join(file_path, f) \n",
    "                      for f in os.listdir(file_path) \n",
    "                      if f.endswith(extention) and not isfile_empty(file_path+f)]\n",
    "    return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#permutation\n",
    "def permutation(path_data_base,path_sim,path_sim_permutation):\n",
    "    # open file Sim \n",
    "    sim_list=read_last_file_list(path_data_base+path_sim,\"csv\")\n",
    "    #print(len(sim_list))\n",
    "    for y in range(len(sim_list)):\n",
    "\n",
    "        file_name=full_name_file(str(sim_list[y]))\n",
    "        #print(file_name)\n",
    "        Topic_name=file_name[:-4]\n",
    "        #print(Topic_name)\n",
    "        topic_df=read_cvs_by_pands(path_data_base,path_sim+file_name,None\n",
    "                          ,None)\n",
    "\n",
    "\n",
    "        #print(len(topic_df))\n",
    "        main_Topic=[]\n",
    "        sub_topic=[]\n",
    "        true_list=[]\n",
    "        list_removed=[]\n",
    "        for i in range (len(topic_df)):\n",
    "            #print(i)\n",
    "            #print(topic_df.iloc[i][0])\n",
    "            if topic_df.iloc[i][0]==Topic_name:\n",
    "                #exclud from list wordwith the same name file\n",
    "                main_Topic=list(topic_df.iloc[i])\n",
    "                true_list.append(main_Topic)\n",
    "                #print(type(main_Topic))\n",
    "                #print(main_Topic)\n",
    "                sub_topic =topic_df[topic_df.index != i].values.tolist()\n",
    "                break\n",
    "\n",
    "                # list after remove main topic\n",
    "        if len(sub_topic)>1:\n",
    "            for term in range(len(sub_topic)):\n",
    "                if term not in list_removed:\n",
    "\n",
    "                    #print(\"-------------------------------------\")\n",
    "                    #print(\"============Term==========\",sub_topic[term][0])\n",
    "\n",
    "                    is_term_good=True\n",
    "                    for next_term in sub_topic[term+1:]:\n",
    "\n",
    "                        #print(next_term[0])\n",
    "                        result=similarity_by_infocontent(sub_topic[term][0],next_term[0],'res')\n",
    "                        if result<1:\n",
    "                            if sub_topic[term][1]>next_term[1]:\n",
    "                                list_removed.append(next_term[0])\n",
    "                            else:\n",
    "                                list_removed.append(sub_topic[term][0])\n",
    "\n",
    "                            is_term_good=False\n",
    "                            #print(sub_topic[term][0],next_term[0],result,\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
    "                            break\n",
    "                    if is_term_good:\n",
    "                        true_list.append(sub_topic[term])\n",
    "                else:\n",
    "                    print(\"------------term removed-------------------------\",term)\n",
    "            #print(true_list)\n",
    "            perm_df=pd.DataFrame(true_list)\n",
    "            #print(\" print new file to perm\")\n",
    "            save_df_to_csv(perm_df,path_data_base,path_sim_permutation,file_name)\n",
    "        else:\n",
    "            #print(\" copy origenal file to perm\")\n",
    "            save_df_to_csv(topic_df,path_data_base,path_sim_permutation,file_name)\n",
    "\n",
    "\n",
    "        #compare each item withothers \n",
    "        #each term with next term \n",
    "        #if relation is less than one \n",
    "        #remove term with origenal small value\n",
    "        #and new list in next compare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_idf(D,d,base):\n",
    "    return math.log((D/d), base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def idf_df(df,D,base):\n",
    "    #[7/df['0']]\n",
    "    y = [log_idf(D,x,base) for x in df[0]]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_sord_tf_file(path_data_source,sub_path_data_source_tf,i):\n",
    "    #df=read_cvs_by_pands(path_data_source,sub_path_data_source_tf+\"cs\"+str(i)+\".csv\",0,header=0)\n",
    "    #print(\"nnnnnnnnnnnn\",path_data_source+sub_path_data_source_tf+i)\n",
    "    df=read_cvs_by_pands(path_data_source,sub_path_data_source_tf+i,0,header=0)\n",
    "    #ee.T.sort_index(inplace=True)\n",
    "    df = df.T.sort_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_sord_idf_file(path_data_source,sub_path_data_source_idf,i):\n",
    "    #df=read_cvs_by_pands(path_data_source,sub_path_data_source_idf+\"cs\"+str(i)+\".csv\",0,header=0)\n",
    "    df=read_cvs_by_pands(path_data_source,sub_path_data_source_idf+i,0,header=0)\n",
    "    #ee.T.sort_index(inplace=True)\n",
    "    df = df.T.sort_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#d1={'a': 1, 'b': 1, 'c': 1}\n",
    "def dict_to_DF(dic):\n",
    "    df=pd.DataFrame([dic])\n",
    "    return df\n",
    "\n",
    "#dict_to_DF(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_df_to_csv(df,path_database,sub_path,new_file_name):\n",
    "     df.to_csv(path_database+sub_path+new_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idf_one(path_data_source,sub_path_data_source_tf,sub_path_data_source_idf,i,sub_path_data_source_tfidf):\n",
    "    df_tf=read_sord_tf_file(path_data_source,sub_path_data_source_tf,i)\n",
    "    df_idf=read_sord_idf_file(path_data_source,sub_path_data_source_idf,i)\n",
    "\n",
    "    if len(df_idf) != 0:\n",
    "        idf=idf_df(df_idf,len(df_idf),10)\n",
    "\n",
    "        full_tfidf=[]\n",
    "        for index in range(len(idf)):\n",
    "            #print(index)\n",
    "            tfidf=df_tf[0][index]*idf[index]\n",
    "            full_tfidf.append(tfidf)\n",
    "\n",
    "\n",
    "    df_tf_idf=pd.DataFrame(full_tfidf)\n",
    "    df_tf_idf.index=df_tf.index\n",
    "    #df_tf_idf.to_csv(path_data_source+sub_path_data_source_tfidf+\"cs\"+str(i)+\".csv\")\n",
    "    df_tf_idf.to_csv(path_data_source+sub_path_data_source_tfidf+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idfs(list_tf,list_idf):\n",
    "    #print(list_tf,list_idf)\n",
    "    for tf_file in list_tf:\n",
    "        tf_file_name=full_name_file(tf_file)\n",
    "        #print(tf_file_name)\n",
    "        for idf_file in list_idf:\n",
    "            idf_file_name=full_name_file(idf_file)\n",
    "            if tf_file_name == idf_file_name:\n",
    "                #print(idf_file_name)\n",
    "                tf_idf_one(path_data_base,path_tf,path_idf,tf_file_name,path_tfidf)\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_file_exist(file_path,file_name):\n",
    "    file_names = [os.path.join(file_path, f) \n",
    "                      for f in os.listdir(file_path) \n",
    "                      if f.endswith(\".csv\")]\n",
    "    #print(file_names)\n",
    "    if file_name in file_names:\n",
    "        #print(\"d\")\n",
    "        return True\n",
    "    else:\n",
    "        #print(\"dd\")\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#is_file_exist('/home/fsg/Desktop/csv/semantics/permutation/','/home/fsg/Desktop/csv/semantics/permutation/situation.n.03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def isfile_empty(file_path_name):\n",
    "    import pandas as pd\n",
    "    df1=pd.read_csv(file_path_name)\n",
    "    is_blank=len(df1)==0\n",
    "    return is_blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#isfile_empty('/home/fsg/Desktop/csv/semantics/permutation/1530s.n.01.csv') #situation.n.03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#to calculate H\n",
    "#load list V(TF-IDF)\n",
    "#loop perfile\n",
    "#hold term\n",
    "#open  the same name in topic \"permutation\"\n",
    "#scalar multiblication: multible each \n",
    "#value of this word in V file by all items (terms)\n",
    "#in topic file then sum and put result  in new file for topic document \n",
    "#def W_topic_doc(file_names_tfidf,file_path_sim_permutation,path_topic_document,path_data_base):\n",
    "def W_topic_doc(tfidf_file,path_topic_document,path_data_base):\n",
    "    #premu_dic=list_to_dict(file_path_sim_permutation)\n",
    "    #tfidf_dict=list_to_dict(file_names_tfidf)\n",
    "    #print(tfidf_dict)\n",
    "    \n",
    "    #for tfidf_file in file_names_tfidf:\n",
    "        \n",
    "    file_name=full_name_file(tfidf_file)\n",
    "    #print(\"File: \",file_name,\"\\n\")\n",
    "    tfidf_df=read_cvs_by_pands_full_path(tfidf_file,0,0)\n",
    "    df_index=tfidf_df.index\n",
    "    result_W={}\n",
    "    for index in df_index:\n",
    "        #print(\"Term will be Topic: \",index,\"\\n\")\n",
    "        if is_file_exist(file_path_sim_permutation,file_path_sim_permutation+index.replace(\"/\", \"_\")+'.csv'):\n",
    "        \n",
    "            full_topic_path=file_path_sim_permutation+index.replace(\"/\", \"_\")+'.csv'#premu_dic.get(index+'.csv')\n",
    "            print(full_topic_path)\n",
    "            if not isfile_empty(full_topic_path):\n",
    "\n",
    "                topic_df=read_cvs_by_pands_full_path(full_topic_path,0,0)\n",
    "                #print(\"fffffffffffffff\",tfidf_df.loc[index][0])\n",
    "\n",
    "                #print(\"Topic-Terms:\\n\",topic_df['0'],\"\\n\")\n",
    "                try:\n",
    "                    sum_terms_topic=sum([tfidf_df.loc[index][0] * x for x in topic_df['1']])\n",
    "                    result_W[index]=sum_terms_topic\n",
    "                except:\n",
    "                    result_W[index]=0\n",
    "                    \n",
    "            else:\n",
    "                result_W[index]=0\n",
    "            #print(\"Sum: \",sum_terms_topic,\"\\n\")\n",
    "        else:\n",
    "             result_W[index]=0\n",
    "    W_df=dict_to_DF(result_W)\n",
    "    #print(\"Save\",file_name)\n",
    "    save_df_to_csv(W_df,path_data_base,path_topic_document,file_name)\n",
    "    #print(\"All: \",result_W,\"\\n\")\n",
    "#return result_W  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#file_names_tfidf=read_last_file_list(file_path_tfidf,\"csv\")\n",
    "#W_topic_doc(file_names_tfidf,path_topic_document,path_data_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gpu_full_process(file_names_tfidf):\n",
    "    \n",
    "   \n",
    "    #with tf.device(gpu_name):\n",
    "    #print(gpu_name)\n",
    "    print(\"************Started Session GPU *************\")\n",
    "    #print(\"sublist\",file_names_tfidf,\"gpu_name\",gpu_name)\n",
    "    W_topic_doc(file_names_tfidf,path_topic_document,path_data_base)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#gpu_full_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sub_list_file(file_list_task,all_gpus):\n",
    "    import math\n",
    "    sub_len=math.ceil(len(file_list_task)/all_gpus)\n",
    "    global_list_len=math.ceil(len(file_list_task)/sub_len)\n",
    "    \n",
    "    global_list=[]\n",
    "    index=0\n",
    "    for x in range(global_list_len):\n",
    "        sublist=[]\n",
    "        for i in range(sub_len):\n",
    "            if index < len(file_list_task):\n",
    "                sublist.append(file_list_task[index])\n",
    "                index +=1\n",
    "                \n",
    "        global_list.append(sublist)\n",
    "\n",
    "    return global_list\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(device_count={\"GPU\": 2,\"CPU\":1},\n",
    "                        allow_soft_placement=True,\n",
    "                        inter_op_parallelism_threads=16,\n",
    "                        intra_op_parallelism_threads=16,\n",
    "                        use_per_session_threads=True,\n",
    "                        log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distrupited_task_gpu(no_task,total_no_gpu):\n",
    "    \n",
    "    return no_task%total_no_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************Started Session CPU *************\n",
      "************Started Session GPU *************\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'/home/fsg/Desktop/csv/sub_tfidf/split11.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-5fd3c7cba7fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_path_tfidf\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"split\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtask_no\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".csv\"\u001b[0m \u001b[0;31m#\"cs\"+task_no+\".txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mgpu_full_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#with tf.device(gpu_name):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-f90475c4d5ac>\u001b[0m in \u001b[0;36mgpu_full_process\u001b[0;34m(file_names_tfidf)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"************Started Session GPU *************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#print(\"sublist\",file_names_tfidf,\"gpu_name\",gpu_name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mW_topic_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_names_tfidf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath_topic_document\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath_data_base\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-5c384288491b>\u001b[0m in \u001b[0;36mW_topic_doc\u001b[0;34m(tfidf_file, path_topic_document, path_data_base)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_name_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#print(\"File: \",file_name,\"\\n\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtfidf_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_cvs_by_pands_full_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mdf_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfidf_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mresult_W\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-9eeec86ec1a3>\u001b[0m in \u001b[0;36mread_cvs_by_pands_full_path\u001b[0;34m(full_path, index_col, header)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'/home/fsg/Desktop/csv/sub_tfidf/split11.csv' does not exist"
     ]
    }
   ],
   "source": [
    "\n",
    "#start_gpu_memory=gpu_memory_map(\"gpu_befor_session_memory.txt\",\"gpu_befor_session_out.txt\")\n",
    "with tf.Session(config=config) as sess:\n",
    "    print(\"************Started Session CPU *************\")\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #start_gpu_memory=gpu_memory_map(\"gpu_after_start_session_memory.txt\",\"gpu_after_start_session_out.txt\")\n",
    "    #print(\"gpu_memory_map_after_session\",start_gpu_memory) \n",
    "    \n",
    "    with tf.device('/cpu'):\n",
    "        \n",
    "        #file_names_tfidf=read_last_file_list(file_path_tfidf,\"csv\")\n",
    "        #print(\"file_list_task\",file_names_tfidf)\n",
    "        #sub_file_list_task=sub_list_file(file_names_tfidf,int(all_gpus))[int(gpuN)]\n",
    "        #print(\"sub_file_list_task\",sub_file_list_task)\n",
    "        gpu_no=distrupited_task_gpu(int(task_no),int(all_gpus))\n",
    "        gpu_name='/gpu:'+str(gpu_no)\n",
    "        \n",
    "        with tf.device(gpu_name):\n",
    "            filename=file_path_tfidf+\"split\"+task_no+\".csv\" #\"cs\"+task_no+\".txt\" \n",
    "            gpu_full_process(filename)    \n",
    "\n",
    "        #with tf.device(gpu_name):\n",
    "            #start = timeit.default_timer()\n",
    "        #gpu_full_process(sub_file_list_task)\n",
    "            #Your statements here\n",
    "            #print(addd(\"1\",\"2\"))\n",
    "            #stop = timeit.default_timer()\n",
    "            #print('Total Time:',stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
