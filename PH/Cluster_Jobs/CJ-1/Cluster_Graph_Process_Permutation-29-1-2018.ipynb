{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/python3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import csv\n",
    "from tensorflow.python.client import timeline\n",
    "import pandas as pd\n",
    "import math\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gpuN=\"0\"#sys.argv[1]\n",
    "#for i in range(len(sys.argv)):\n",
    "    #print(\"i\",i,sys.argv[i])\n",
    "\n",
    "#task_no=\"0\"#sys.argv[0]\n",
    "\n",
    "#all_gpus =\"2\"#sys.argv[1]\n",
    "#task_file_name=\"3\"#sys.argv[2]\n",
    "#cuda=\"0,1\"#sys.argv[3]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#gpuN=\"0\"#sys.argv[1]\n",
    "task_no=sys.argv[1]\n",
    "#print(\"task_no\",task_no)\n",
    "all_gpus =sys.argv[2]\n",
    "#print(\"all_gpus\",all_gpus)\n",
    "task_file_name=sys.argv[3]\n",
    "#print(\"task_file_name\",task_file_name)\n",
    "cuda=sys.argv[4]\n",
    "processor=sys.argv[5]\n",
    "dataset=sys.argv[6]\n",
    "#print(\"cuda\",cuda)\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "\n",
    "#path_data_source=dir_path+\"/data/data_source/\"\n",
    "#path_data_source=\"/home/fsg/Desktop/\"\n",
    "\n",
    "#path_data_base=dir_path+\"/data/database/csv/\"\n",
    "\n",
    "\n",
    "#path_data_base=\"/home/fsg/Desktop/csv/\"\n",
    "\n",
    "\n",
    "#files_path_data_source=\"files/\"\n",
    "\n",
    "#files_path_data_source=\"demo/\"\n",
    "\n",
    "#sub_path_data_source=\"small/\"\n",
    "\n",
    "dir_path =\"/home/helwan003u1\"\n",
    "\n",
    "path_data_source=dir_path+\"/data/data_source/\"#wiki/wikipedia2text/\"\n",
    "\n",
    "path_data_base=dir_path+\"/data/database/\"+processor+dataset+\"/\"\n",
    "\n",
    "files_path_data_source=dataset+\"/\"\n",
    "\n",
    "file_path=path_data_source+files_path_data_source\n",
    "\n",
    "file_names = [os.path.join(file_path, f) \n",
    "                      for f in os.listdir(file_path) \n",
    "                      if f.endswith(\".txt\")]\n",
    "\n",
    "\n",
    "path_tf=\"sub_tf/\"\n",
    "path_idf=\"sub_idf/\"\n",
    "path_tfidf=\"sub_tfidf/\"\n",
    "path_sim_permutation=\"semantics/permutation/\"\n",
    "path_sim=\"semantics/sim/\"\n",
    "path_topic_document=\"topic_document/\"\n",
    "\n",
    "file_path_tf=path_data_base+path_tf\n",
    "file_path_idf=path_data_base+path_idf\n",
    "file_path_tfidf=path_data_base+path_tfidf\n",
    "file_path_sim_permutation=path_data_base+path_sim_permutation\n",
    "file_path_sim=path_data_base+path_sim\n",
    "file_path_topic_document=path_data_base+path_topic_document\n",
    "\n",
    "file_names_tf = [os.path.join(file_path_tf, f) \n",
    "                      for f in os.listdir(file_path_tf) \n",
    "                      if f.endswith(\".csv\")]\n",
    "file_names_idf = [os.path.join(file_path_idf, f) \n",
    "                      for f in os.listdir(file_path_idf) \n",
    "                      if f.endswith(\".csv\")]\n",
    "\n",
    "\n",
    "file_names_tfidf = [os.path.join(file_path_tfidf, f) \n",
    "                      for f in os.listdir(file_path_tfidf) \n",
    "                      if f.endswith(\".csv\")]\n",
    "\n",
    "\n",
    "file_names_permutation = [os.path.join(file_path_sim_permutation, f) \n",
    "                      for f in os.listdir(file_path_sim_permutation) \n",
    "                      if f.endswith(\".csv\")]\n",
    "\n",
    "\n",
    "file_names_sim = [os.path.join(file_path_sim, f) \n",
    "                      for f in os.listdir(file_path_sim) \n",
    "                      if f.endswith(\".csv\")]\n",
    "\n",
    "file_names_topic_document = [os.path.join(file_path_topic_document, f) \n",
    "                      for f in os.listdir(file_path_topic_document) \n",
    "                      if f.endswith(\".csv\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess, re, os, sys #https://github.com/yaroslavvb/stuff/blob/master/notebook_util.py\n",
    "def run_command(cmd):\n",
    "    \"\"\"Run command, return output as string.\"\"\"\n",
    "    \n",
    "    output = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True).communicate()[0]\n",
    "    return output.decode(\"ascii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_available_gpus():\n",
    "    \"\"\"Returns list of available GPU ids.\"\"\"\n",
    "    \n",
    "    output = run_command(\"nvidia-smi -L\")\n",
    "    # lines of the form GPU 0: TITAN X\n",
    "    gpu_regex = re.compile(r\"GPU (?P<gpu_id>\\d+):\")\n",
    "    result = []\n",
    "    for line in output.strip().split(\"\\n\"):\n",
    "        m = gpu_regex.match(line)\n",
    "        assert m, \"Couldnt parse \"+line\n",
    "        result.append(int(m.group(\"gpu_id\")))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_txt(txt,file):\n",
    "    text_file = open(file, \"w\")\n",
    "    text_file.write(txt)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gpu_memory_map(gpu_memory_file,gpu_output_file):\n",
    "    \"\"\"Returns map of GPU id to memory allocated on that GPU.\"\"\"\n",
    "\n",
    "    output = run_command(\"nvidia-smi\")\n",
    "    save_txt(output,gpu_output_file)\n",
    "    #print(\"nvidia-smi\",output)\n",
    "    gpu_output = output[output.find(\"GPU Memory\"):]\n",
    "    #print(\"GPU Memory\",gpu_output)\n",
    "    save_txt(gpu_output,gpu_memory_file)\n",
    "   \n",
    "    # lines of the form\n",
    "    # |    0      8734    C   python                                       11705MiB |\n",
    "    memory_regex = re.compile(r\"[|]\\s+?(?P<gpu_id>\\d+)\\D+?(?P<pid>\\d+).+[ ](?P<gpu_memory>\\d+)MiB\")\n",
    "    #print(\"memory_regex\",memory_regex)\n",
    "    rows = gpu_output.split(\"\\n\")\n",
    "    #print(\"rows\",rows)\n",
    "    result = {gpu_id: 0 for gpu_id in list_available_gpus()}\n",
    "    #print(\"result\",result)\n",
    "    for row in gpu_output.split(\"\\n\"):\n",
    "        m = memory_regex.search(row)\n",
    "        if not m:\n",
    "            continue\n",
    "        gpu_id = int(m.group(\"gpu_id\"))\n",
    "        gpu_memory = int(m.group(\"gpu_memory\"))\n",
    "        result[gpu_id] += gpu_memory\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_cvs_by_pands(path_database,file_database,index_col, header):\n",
    "    import csv\n",
    "    import pandas as pd\n",
    "    return pd.read_csv(path_database+file_database,index_col=index_col,header=header)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_cvs_by_pands_full_path(full_path,index_col, header):\n",
    "    import csv\n",
    "    import pandas as pd\n",
    "    return pd.read_csv(full_path,index_col=index_col,header=header)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_df_to_csv(df,path_database,sub_path,new_file_name):\n",
    "     df.to_csv(path_database+sub_path+new_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_to_dict(df):\n",
    "        \n",
    "    dic={}\n",
    "    keys=df.keys()\n",
    "    #print(keys)\n",
    "    \n",
    "    values= df.T.values.tolist()\n",
    "    #print(len(values))\n",
    "    for i in range(len(keys)):\n",
    "        #print(keys[i])\n",
    "        dic[keys[i]]=magic(values[i])\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def magic(numList):         # [1,2,3]\n",
    "    s = map(str, numList)   # ['1','2','3']\n",
    "    s = ''.join(s)          # '123'\n",
    "    s = int(s)              # 123\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_to_dict(list_full_path):\n",
    "    dic={}\n",
    "    for file_path in list_full_path:\n",
    "        index=full_name_file(file_path)\n",
    "        dic[index]=file_path\n",
    "\n",
    "    return dic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic as wnic\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def similarity_by_infocontent(sense1, sense2, option):\n",
    "    #sense1=\"Synset('\"+sense1+\"')\"\n",
    "    #sense2=\"Synset('\"+sense2+\"')\"\n",
    "    #print(sense1,sense2)\n",
    "    sense1 = wn.synset(sense1)\n",
    "    sense2 = wn.synset(sense2)\n",
    "    #print(sense1,sense2)\n",
    "    \"\"\" Returns similarity scores by information content. \"\"\"\n",
    "    #if sense1.pos != sense2.pos: # infocontent sim can't do diff POS.\n",
    "        #return 0\n",
    "\n",
    "    info_contents = ['ic-bnc-add1.dat', 'ic-bnc-resnik-add1.dat', \n",
    "                     'ic-bnc-resnik.dat', 'ic-bnc.dat', \n",
    "\n",
    "                     'ic-brown-add1.dat', 'ic-brown-resnik-add1.dat', \n",
    "                     'ic-brown-resnik.dat', 'ic-brown.dat', \n",
    "\n",
    "                     'ic-semcor-add1.dat', 'ic-semcor.dat',\n",
    "\n",
    "                     'ic-semcorraw-add1.dat', 'ic-semcorraw-resnik-add1.dat', \n",
    "                     'ic-semcorraw-resnik.dat', 'ic-semcorraw.dat', \n",
    "\n",
    "                     'ic-shaks-add1.dat', 'ic-shaks-resnik.dat', \n",
    "                     'ic-shaks-resnink-add1.dat', 'ic-shaks.dat', \n",
    "\n",
    "                     'ic-treebank-add1.dat', 'ic-treebank-resnik-add1.dat', \n",
    "                     'ic-treebank-resnik.dat', 'ic-treebank.dat']\n",
    "\n",
    "    if option in ['res', 'resnik']:\n",
    "        #return wn.res_similarity(sense1, sense2, wnic.ic('ic-bnc-resnik-add1.dat'))\n",
    "        #print('simRe snik (c1,c2) = -log p(lso(c1,c2)) = IC(lso(c1,c2)')\n",
    "        return wn.res_similarity(sense1, sense2, wnic.ic('ic-treebank-resnik-add1.dat'))\n",
    "    #return min(wn.res_similarity(sense1, sense2, wnic.ic(ic)) \\\n",
    "    #             for ic in info_contents)\n",
    "\n",
    "    elif option in ['jcn', \"jiang-conrath\"]:\n",
    "        #return wn.jcn_similarity(sense1, sense2, wnic.ic('ic-bnc-add1.dat'))\n",
    "        #print('sim(jcn) (c1,c2 )= (IC(c1) + IC(c2 )) - 2IC(lso(c1,c2 ))')\n",
    "        return wn.jcn_similarity(sense1, sense2, wnic.ic('ic-treebank.dat'))\n",
    "\n",
    "    elif option in ['lin']:\n",
    "        #return wn.lin_similarity(sense1, sense2, wnic.ic('ic-bnc-add1.dat'))\n",
    "        #print('sim(lin) (c1,c2)=(2IC(lso(c1,c2 )))/(IC(c1)+IC(c2))')\n",
    "        return wn.lin_similarity(sense1, sense2, wnic.ic('ic-treebank.dat'))\n",
    "\n",
    "def sim(sense1, sense2, option=\"path\"):\n",
    "    \"\"\" Calculates similarity based on user's choice. \"\"\"\n",
    "    option = option.lower()\n",
    "    if option.lower() in [\"path\", \"path_similarity\", \n",
    "                        \"wup\", \"wupa\", \"wu-palmer\", \"wu-palmer\",\n",
    "                        'lch', \"leacock-chordorow\"]:\n",
    "        return similarity_by_path(sense1, sense2, option) \n",
    "    elif option.lower() in [\"res\", \"resnik\",\n",
    "                          \"jcn\",\"jiang-conrath\",\n",
    "                          \"lin\"]:\n",
    "        return similarity_by_infocontent(sense1, sense2, option)\n",
    "\n",
    "def max_similarity(context_sentence, ambiguous_word, option=\"path\", \n",
    "                   pos=None, best=True):\n",
    "    \"\"\"\n",
    "    Perform WSD by maximizing the sum of maximum similarity between possible \n",
    "    synsets of all words in the context sentence and the possible synsets of the \n",
    "    ambiguous words (see http://goo.gl/XMq2BI):\n",
    "    {argmax}_{synset(a)}(\\sum_{i}^{n}{{max}_{synset(i)}(sim(i,a))}\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for i in wn.synsets(ambiguous_word):\n",
    "        try:\n",
    "            if pos and pos != str(i.pos()):\n",
    "                continue\n",
    "        except:\n",
    "            if pos and pos != str(i.pos):\n",
    "                continue\n",
    "        result[i] = sum(max([sim(i,k,option) for k in wn.synsets(j)]+[0]) \\\n",
    "                        for j in word_tokenize(context_sentence))\n",
    "\n",
    "    if option in [\"res\",\"resnik\"]: # lower score = more similar\n",
    "        result = sorted([(v,k) for k,v in result.items()])\n",
    "    else: # higher score = more similar\n",
    "        result = sorted([(v,k) for k,v in result.items()],reverse=True)\n",
    "    #print (result)\n",
    "    if best: return result[0][1];\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isfile_empty(file_path_name):\n",
    "    f=open(file_path_name, 'r') \n",
    "    is_blank = len(f.read().strip()) == 0\n",
    "    return is_blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def full_name_file(full_name_path):#like cs.csv\n",
    "    d=full_name_path.split(\"/\")\n",
    "    ##print(d)\n",
    "    name=d[len(d)-1]#.split(\".\")\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_last_file_list(file_path,extention):\n",
    "    \n",
    "    file_names = [os.path.join(file_path, f) \n",
    "                      for f in os.listdir(file_path) \n",
    "                      if f.endswith(extention) and not isfile_empty(file_path+f)]\n",
    "    return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#permutation\n",
    "def permutation(task_file_name,path_data_base,path_sim,path_sim_permutation):\n",
    "    print(\"in permutation \",task_file_name)\n",
    "    # open file Sim \n",
    "    #sim_list=read_last_file_list(path_data_base+path_sim,\"csv\")\n",
    "    \n",
    "    #print(len(sim_list))\n",
    "    #for y in range(len(sim_list)):\n",
    "\n",
    "    file_name=full_name_file(task_file_name)\n",
    "    print(file_name)\n",
    "    Topic_name=file_name[:-4]\n",
    "    print(Topic_name)\n",
    "    topic_df=read_cvs_by_pands_full_path(task_file_name,None\n",
    "                      ,None)\n",
    "\n",
    "\n",
    "    #print(len(topic_df))\n",
    "    main_Topic=[]\n",
    "    sub_topic=[]\n",
    "    true_list=[]\n",
    "    list_removed=[]\n",
    "    for i in range (len(topic_df)):\n",
    "        #print(i)\n",
    "        #print(topic_df.iloc[i][0])\n",
    "        if topic_df.iloc[i][0]==Topic_name:\n",
    "            #exclud from list wordwith the same name file\n",
    "            main_Topic=list(topic_df.iloc[i])\n",
    "            true_list.append(main_Topic)\n",
    "            #print(type(main_Topic))\n",
    "            #print(main_Topic)\n",
    "            sub_topic =topic_df[topic_df.index != i].values.tolist()\n",
    "            break\n",
    "\n",
    "            # list after remove main topic\n",
    "    if len(sub_topic)>1:\n",
    "        for term in range(len(sub_topic)):\n",
    "            if term not in list_removed:\n",
    "\n",
    "                #print(\"-------------------------------------\")\n",
    "                #print(\"============Term==========\",sub_topic[term][0])\n",
    "\n",
    "                is_term_good=True\n",
    "                for next_term in sub_topic[term+1:]:\n",
    "\n",
    "                    #print(next_term[0])\n",
    "                    result=similarity_by_infocontent(sub_topic[term][0],next_term[0],'res')\n",
    "                    if result<1:\n",
    "                        if sub_topic[term][1]>next_term[1]:\n",
    "                            list_removed.append(next_term[0])\n",
    "                        else:\n",
    "                            list_removed.append(sub_topic[term][0])\n",
    "\n",
    "                        is_term_good=False\n",
    "                        #print(sub_topic[term][0],next_term[0],result,\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
    "                        break\n",
    "                if is_term_good:\n",
    "                    true_list.append(sub_topic[term])\n",
    "            else:\n",
    "                print(\"------------term removed-------------------------\",term)\n",
    "        #print(true_list)\n",
    "        perm_df=pd.DataFrame(true_list)\n",
    "        #print(\" print new file to perm\")\n",
    "        save_df_to_csv(perm_df,path_data_base,path_sim_permutation,file_name)\n",
    "    else:\n",
    "        #print(\" copy origenal file to perm\")\n",
    "        save_df_to_csv(topic_df,path_data_base,path_sim_permutation,file_name)\n",
    "    print(\"End Permutation\")\n",
    "\n",
    "\n",
    "    #compare each item withothers \n",
    "    #each term with next term \n",
    "    #if relation is less than one \n",
    "    #remove term with origenal small value\n",
    "    #and new list in next compare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_idf(D,d,base):\n",
    "    return math.log((D/d), base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def idf_df(df,D,base):\n",
    "    #[7/df['0']]\n",
    "    y = [log_idf(D,x,base) for x in df[0]]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_sord_tf_file(path_data_source,sub_path_data_source_tf,i):\n",
    "    #df=read_cvs_by_pands(path_data_source,sub_path_data_source_tf+\"cs\"+str(i)+\".csv\",0,header=0)\n",
    "    #print(\"nnnnnnnnnnnn\",path_data_source+sub_path_data_source_tf+i)\n",
    "    df=read_cvs_by_pands(path_data_source,sub_path_data_source_tf+i,0,header=0)\n",
    "    #ee.T.sort_index(inplace=True)\n",
    "    df = df.T.sort_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_sord_idf_file(path_data_source,sub_path_data_source_idf,i):\n",
    "    #df=read_cvs_by_pands(path_data_source,sub_path_data_source_idf+\"cs\"+str(i)+\".csv\",0,header=0)\n",
    "    df=read_cvs_by_pands(path_data_source,sub_path_data_source_idf+i,0,header=0)\n",
    "    #ee.T.sort_index(inplace=True)\n",
    "    df = df.T.sort_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#d1={'a': 1, 'b': 1, 'c': 1}\n",
    "def dict_to_DF(dic):\n",
    "    df=pd.DataFrame([dic])\n",
    "    return df\n",
    "\n",
    "#dict_to_DF(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_df_to_csv(df,path_database,sub_path,new_file_name):\n",
    "     df.to_csv(path_database+sub_path+new_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idf_one(path_data_source,sub_path_data_source_tf,sub_path_data_source_idf,i,sub_path_data_source_tfidf):\n",
    "    df_tf=read_sord_tf_file(path_data_source,sub_path_data_source_tf,i)\n",
    "    df_idf=read_sord_idf_file(path_data_source,sub_path_data_source_idf,i)\n",
    "\n",
    "    if len(df_idf) != 0:\n",
    "        idf=idf_df(df_idf,len(df_idf),10)\n",
    "\n",
    "        full_tfidf=[]\n",
    "        for index in range(len(idf)):\n",
    "            #print(index)\n",
    "            tfidf=df_tf[0][index]*idf[index]\n",
    "            full_tfidf.append(tfidf)\n",
    "\n",
    "\n",
    "    df_tf_idf=pd.DataFrame(full_tfidf)\n",
    "    df_tf_idf.index=df_tf.index\n",
    "    #df_tf_idf.to_csv(path_data_source+sub_path_data_source_tfidf+\"cs\"+str(i)+\".csv\")\n",
    "    df_tf_idf.to_csv(path_data_source+sub_path_data_source_tfidf+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idfs(list_tf,list_idf):\n",
    "    #print(list_tf,list_idf)\n",
    "    for tf_file in list_tf:\n",
    "        tf_file_name=full_name_file(tf_file)\n",
    "        #print(tf_file_name)\n",
    "        for idf_file in list_idf:\n",
    "            idf_file_name=full_name_file(idf_file)\n",
    "            if tf_file_name == idf_file_name:\n",
    "                #print(idf_file_name)\n",
    "                tf_idf_one(path_data_base,path_tf,path_idf,tf_file_name,path_tfidf)\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#to calculate H\n",
    "#load list V(TF-IDF)\n",
    "#loop perfile\n",
    "#hold term\n",
    "#open  the same name in topic \"permutation\"\n",
    "#scalar multiblication: multible each \n",
    "#value of this word in V file by all items (terms)\n",
    "#in topic file then sum and put result  in new file for topic document \n",
    "def W_topic_doc(file_names_tfidf,file_path_sim_permutation,path_topic_document,path_data_base):\n",
    "    premu_dic=list_to_dict(file_path_sim_permutation)\n",
    "    tfidf_dict=list_to_dict(file_names_tfidf)\n",
    "    #print(tfidf_dict)\n",
    "    \n",
    "    for tfidf_file in file_names_tfidf:\n",
    "        \n",
    "        file_name=full_name_file(tfidf_file)\n",
    "        #print(\"File: \",file_name,\"\\n\")\n",
    "        tfidf_df=read_cvs_by_pands_full_path(tfidf_file,0,0)\n",
    "        df_index=tfidf_df.index\n",
    "        result_W={}\n",
    "        for index in df_index:\n",
    "            #print(\"Term will be Topic: \",index,\"\\n\")\n",
    "            full_topic_path=premu_dic.get(index+'.csv')\n",
    "            #print(full_topic_path)\n",
    "            topic_df=read_cvs_by_pands_full_path(full_topic_path,0,0)\n",
    "            #print(\"fffffffffffffff\",tfidf_df.loc[index][0])\n",
    "            \n",
    "            #print(\"Topic-Terms:\\n\",topic_df['0'],\"\\n\")\n",
    "            sum_terms_topic=sum([tfidf_df.loc[index][0] * x for x in topic_df['1']])\n",
    "            result_W[index]=sum_terms_topic\n",
    "            #print(\"Sum: \",sum_terms_topic,\"\\n\")\n",
    "        W_df=dict_to_DF(result_W)\n",
    "        #print(\"Save\",file_name)\n",
    "        save_df_to_csv(W_df,path_data_base,path_topic_document,file_name)\n",
    "        #print(\"All: \",result_W,\"\\n\")\n",
    "#return result_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gpu_full_process(task_file_name):\n",
    "    \n",
    "    \n",
    "    permutation(task_file_name,path_data_base,path_sim,path_sim_permutation)\n",
    "    #file_names_sim_permutation=read_last_file_list(file_path_sim_permutation,\"csv\")\n",
    "\n",
    "    #with tf.device('/gpu'):\n",
    "        \n",
    "        #W_topic_doc(file_names_tfidf,file_names_sim_permutation,path_topic_document,path_data_base)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#gpu_full_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sub_list_file(file_list_task,all_gpus):\n",
    "    import math\n",
    "    sub_len=math.ceil(len(file_list_task)/all_gpus)\n",
    "    global_list_len=math.ceil(len(file_list_task)/sub_len)\n",
    "    \n",
    "    global_list=[]\n",
    "    index=0\n",
    "    for x in range(global_list_len):\n",
    "        sublist=[]\n",
    "        for i in range(sub_len):\n",
    "            if index < len(file_list_task):\n",
    "                sublist.append(file_list_task[index])\n",
    "                index +=1\n",
    "                \n",
    "        global_list.append(sublist)\n",
    "\n",
    "    return global_list\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(device_count={processor.upper():int(all_gpus)},\n",
    "                        allow_soft_placement=True,\n",
    "                        inter_op_parallelism_threads=16,\n",
    "                        intra_op_parallelism_threads=16,\n",
    "                        use_per_session_threads=True,\n",
    "                        log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distrupited_task_gpu(no_task,total_no_gpu):\n",
    "    \n",
    "    return no_task%total_no_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time: 65960.770717491\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#start_gpu_memory=gpu_memory_map(\"gpu_befor_session_memory.txt\",\"gpu_befor_session_out.txt\")\n",
    "with tf.Session(config=config) as sess:\n",
    "    print(\"************Started Session CPU *************\")\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #start_gpu_memory=gpu_memory_map(\"gpu_after_start_session_memory.txt\",\"gpu_after_start_session_out.txt\")\n",
    "    #print(\"gpu_memory_map_after_session\",start_gpu_memory) \n",
    "    gpu_name='/'+processor+':0'\n",
    "    with tf.device(gpu_name):\n",
    "        #gpu_no=distrupited_task_gpu(int(task_no),int(all_gpus))\n",
    "        #gpu_name='/gpu:'+str(gpu_no)\n",
    "        #with tf.device(gpu_name):\n",
    "        gpu_full_process(task_file_name)\n",
    "           \n",
    "\n",
    "        #with tf.device(gpu_name):\n",
    "            #start = timeit.default_timer()\n",
    "       \n",
    "            #Your statements here\n",
    "            #print(addd(\"1\",\"2\"))\n",
    "            #stop = timeit.default_timer()\n",
    "            #print('Total Time:',stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
