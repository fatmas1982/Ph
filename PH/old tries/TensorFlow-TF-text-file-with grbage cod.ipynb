{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/python3.5\n",
    "#https://learningtensorflow.com/ReadingFilesBasic/\n",
    "#https://www.tensorflow.org/tutorials/using_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To compute term frequancy from multi txt file and save it in multi csv and one csv by parellel process using tensorflow on GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import csv\\nfrom tensorflow.python.client import timeline\\nimport nltk\\nfrom nltk.wsd import lesk\\nfrom nltk.corpus import wordnet as wn\\nfrom collections import Counter\\nimport threading\\n\\nimport pandas as pd\\nimport numpy as np\\nfrom nltk.tokenize import sent_tokenize, word_tokenize\\n\\nfrom nltk.corpus import stopwords\\nfrom nltk.tokenize import RegexpTokenizer\\nfrom nltk import PorterStemmer\\nfrom nltk.stem.porter import *\\nfrom nltk.stem.snowball import SnowballStemmer\\nfrom string import digits'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "'''import csv\n",
    "from tensorflow.python.client import timeline\n",
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import Counter\n",
    "import threading\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import digits'''\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pathes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3eb80901d37f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdir_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath_data_source\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/data/data_source/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msub_path_data_source\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"demo_onefile/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "\n",
    "path_data_source=dir_path+\"/data/data_source/\"\n",
    "\n",
    "sub_path_data_source=\"demo_onefile/\"\n",
    "\n",
    "#sub_path_data_source=\"demo/\"\n",
    "#sub_path_data_source=\"small/\"\n",
    "\n",
    "file_path=path_data_source+sub_path_data_source\n",
    "\n",
    "file_names = [os.path.join(file_path, f) \n",
    "                      for f in os.listdir(file_path) \n",
    "                      if f.endswith(\".txt\")]\n",
    "\n",
    "#sub_path_data_source=\"demo/\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#path_database=dir_path+\"/data/database/csv/\"\n",
    "#path_sub_tfidf=\"sub_tfidf/\"\n",
    "#path_full_tfidf=\"full_tfidf/\"\n",
    "#path_tf=\"sub_tf/\"\n",
    "#path_database='/home/fsg/output_csv/database/'\n",
    "#path_database='/media/fsg/74C86089C8604C04/PHD/Softwares/ph/database/' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_source = 'cs.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table Data Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TF_File=\"TF-\"\n",
    "#TF_Full=\"TF-Full.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML Data Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''config = tf.ConfigProto(device_count={\"CPU\": 7},\n",
    "                        allow_soft_placement=True,\n",
    "                        inter_op_parallelism_threads=1,\n",
    "                        intra_op_parallelism_threads=1,\n",
    "                        use_per_session_threads=True)'''\n",
    "#import pprint\n",
    "\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL']='5'\n",
    "config = tf.ConfigProto(device_count={\"GPU\": 2,\"CPU\":1},\n",
    "                        allow_soft_placement=True,\n",
    "                        inter_op_parallelism_threads=16,\n",
    "                        intra_op_parallelism_threads=16,\n",
    "                        use_per_session_threads=True,\n",
    "                        log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "#pprint.pprint({ k:str(getattr(config,k)) for k in sorted(dir(config)) if not k.startswith('_')})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Write Excell sheet\n",
    "'''\n",
    "def save_file_to_database(data_rows,path_database,file_databbase,header_list):\n",
    "    import csv\n",
    "    outfile = open(path_database+file_databbase,'w')\n",
    "    writer=csv.writer(outfile)\n",
    "    #header_list=['uuid','paragraph','doc_id']\n",
    "    i=0\n",
    "    for line in data_rows:\n",
    "        row=[i,line,'paragraph no.'+str(i)]\n",
    "        if i==0:\n",
    "            \n",
    "            writer.writerow(header_list)\n",
    "            writer.writerow(row)\n",
    "        else:\n",
    "            #print('ff')\n",
    "            writer.writerow(row)\n",
    "        i+= 1\n",
    "        #outfile.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Read Excell sheet\n",
    "'''\n",
    "def read_text_from_database(path_database,file_databbase):\n",
    "    import csv\n",
    "    queue_paragraph=[]\n",
    "    #f = open(sys.argv[1], 'rt')\n",
    "    outfile = open(path_database+file_databbase,'rt')\n",
    "    try:\n",
    "                \n",
    "        reader=csv.reader(outfile)\n",
    "        for row in reader:\n",
    "            queue_paragraph.append(row)\n",
    "            #print (row)\n",
    "    finally:\n",
    "        print (\"row\")\n",
    "        outfile.close()\n",
    "        \n",
    "    return queue_paragraph\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_cvs_by_pands(path_database,file_databbase,index_col, header):\n",
    "    import csv\n",
    "    return pd.read_csv(path_database+file_databbase,index_col=index_col,header=header)\n",
    "\n",
    "#read_cvs_by_pands(path_database,paragraph_table,index_col=0,header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_cvs_by_pands(path_database,file_databbase,header,data_rows):\n",
    "    import csv\n",
    "    csv_df=pd.DataFrame(data_rows,columns=header ) \n",
    "    csv_df.to_csv(path_database+file_databbase)\n",
    "\n",
    "    \n",
    "\n",
    "#write_cvs_by_pands(path_database,sentences_paragraph_table,sentences_paragraph_list,sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save pragraphs to files\n",
    "def write_file(pragraph,num_pragraph,path):\n",
    "    file = open(path+str(num_pragraph)+\".txt\",\"w\") \n",
    " \n",
    "    file.write(pragraph) \n",
    "    \n",
    "    file.close() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create sub dataset\n",
    "def sub_dataset(path_data_source,data_source):\n",
    "    pragraphs=txt_pragraphs(read_file(path_data_source+data_source))\n",
    "    counter=0\n",
    "    for pragraph in pragraphs:\n",
    "        print('pragraph no ',counter)\n",
    "        write_file(pragraph,counter,sub_path_data_source)\n",
    "        counter +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre processing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Huge File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(str):\n",
    "    file = open(str,'r')\n",
    "    txt=file.read()\n",
    "    #print(txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Document to pragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def txt_pragraphs(str):\n",
    "    pragraphs = str.split(\"\\n\\n\")\n",
    "    return pragraphs\n",
    "#pragraphs=txt_pragraphs(txt)\n",
    "#type(pragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Split Paragraph to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pragraph_to_setnences(str):\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    return sent_tokenize(str)\n",
    "#setnences=pragraph_to_setnences(pragraphs[n_pragraph])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Process For Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing English stopwords and Punct per Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_stop_words = ['the', 'that', 'to', 'as', 'there', 'has', 'and', 'or', 'is', 'not', 'a', 'of', 'but', 'in', 'by', 'on', 'are', 'it', 'if','what','where','how','when']\n",
    "new_stop_words2=['--','i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now','even','until','then','must']\n",
    "numbers=[1,2,3,4,5,6,7,8,9]\n",
    "#stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "def remove_stopword_sentences(sent):\n",
    "    import nltk\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    import time\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    from string import digits\n",
    "    from nltk.corpus.reader.wordnet import WordNetError\n",
    "    import sys\n",
    "    list_word=[]\n",
    "\n",
    "    try:\n",
    "        tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "    \n",
    "        words=tokenizer.tokenize(sent)\n",
    "    \n",
    "        english_stops = set(stopwords.words('english'))\n",
    "        #stems=[]\n",
    "        \n",
    "        list_word=[word for word in words if word.lower() not in english_stops and word.lower() not in new_stop_words and word.lower() not in new_stop_words2 and  not word.lower().isdigit() and word.lower() not in digits and word.lower() not in  numbers]\n",
    "    \n",
    "    #for word in list_word:\n",
    "        #stems.append(stem(word))\n",
    "        #stems.append(PorterStemmer().stem(word))\n",
    "        #stems.append(stemmer.stem(word))\n",
    "        #stems.append(stemmer.stem(\"computer\"))\n",
    "        #stems.append(word)\n",
    "    except WordNetError as e:\n",
    "        print(\"WordNetError on concept {}\".format(sent))\n",
    "    except AttributeError as e:\n",
    "        print(\"Attribute error on concept {}: {}\".format(sent, e))\n",
    "    except:\n",
    "        print(\"Unexpected error on concept {}: {}\".format(sent, sys.exc_info()[0]))\n",
    "    \n",
    "    return list_word#stems#(stem(setem_word for setem_word in  ([word for word in words if word not in english_stops and word not in new_stop_words])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_list_sentece(pragraph):\n",
    "    words_list=[]\n",
    "    setnences=pragraph_to_setnences(pragraph)\n",
    "    for indexs in range(len(setnences)):    \n",
    "        #print(\"Sentence No. \",indexs,\": \",setnences[indexs],\"\\n\")\n",
    "        words=remove_stopword_sentences(setnences[indexs])\n",
    "        wordsent=''\n",
    "        for index in range(len(words)):\n",
    "            wordsent+=' '+words[index]\n",
    "            #print(\"wordsent:\",wordsent)\n",
    "            \n",
    "        words_list.append(wordsent)\n",
    "        #count = Counter(words)\n",
    "        #print(\"wordsent:\",wordsent)\n",
    "        #print(\" word:\",words)\n",
    "    print(words_list)\n",
    "    return words_list\n",
    "\n",
    "#corpus=word_list_sentece(pragraphs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Sense Disambiguation (WSD): LESK per Sentence\n",
    "\n",
    "Given an ambiguous word and the context in which the word occurs, Lesk returns a Synset with the highest number of overlapping words between the context sentence and different definitions from each Synset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "this function for compute lesk for each word(list of word) in sentence\n",
    "'''\n",
    "def lesk_words_sentence(words,sentence):\n",
    "    import nltk\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    from nltk.wsd import lesk\n",
    "    lesks= []\n",
    "    for word in words:\n",
    "        if lesk(sentence,word, 'n') is not None:\n",
    "            lesks.append(lesk(sentence,word, 'n'))\n",
    "            #print(\"Word is: \",word,\"\\n LESK: \",lesk(sentence,word, 'n'),\"\\n Sentence: \",sentence )\n",
    "        \n",
    "    return lesks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "this function for compute lesk of word in sentence\n",
    "'''\n",
    "\n",
    "def lesk_word_sentence(sentence,word):\n",
    "    import nltk\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    from nltk.wsd import lesk\n",
    "    from nltk.corpus.reader.wordnet import WordNetError\n",
    "    import sys\n",
    "    disambiguated=''\n",
    "    #print(type(disambiguated))\n",
    "    try:\n",
    "        \n",
    "    #lesks= []\n",
    "    #for word in words:\n",
    "    #disambiguated=lesk(context_sentence=sentence, ambiguous_word=word)\n",
    "    \n",
    "        disambiguated=lesk(sentence,word, 'n')\n",
    "        print(type(disambiguated))\n",
    "    #print(disambiguated)\n",
    "    #if disambiguated is not None:\n",
    "        #lesk_synset=disambiguated\n",
    "    #else:\n",
    "    #lesk_synset=0\n",
    "    #print(\"Word is: \",word,\"\\n LESK: \",lesk(sentence,word, 'n'),\"\\n Sentence: \",sentence )\n",
    "    except WordNetError as e:\n",
    "        print(\"WordNetError on concept {}\".format(sentence))\n",
    "    except AttributeError as e:\n",
    "        print(\"Attribute error on concept {}: {}\".format(sentence, e))\n",
    "    except:\n",
    "        print(\"Unexpected error on concept {}: {}\".format(sentence, sys.exc_info()[0]))    \n",
    "    return disambiguated\n",
    "\n",
    "#lesk(\"Computer science is a discipline that spans theory and practice\",\"science\")\n",
    "\n",
    "#sent = 'people should be able to marry a person of their choice'.split()\n",
    "#lesk(sent, 'able')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'nltk.corpus.reader.wordnet.Synset'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nltk.corpus.reader.wordnet.Synset"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "type(lesk_word_sentence(\"I love dog\",\"dog\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_row_csv(path_database,idf,list_data):\n",
    "    import csv\n",
    "    with open(path_database+idf, 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(list_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name_file(full_name_path):\n",
    "    d=full_name_path.split(\"/\")\n",
    "    print(d)\n",
    "    name=d[len(d)-1].split(\".\")\n",
    "    return name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-6639d6c9ccdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mindex_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'file_names' is not defined"
     ]
    }
   ],
   "source": [
    "#with tf.control_dependencies(paragraph_list):\n",
    "def MyLoop(gpuname,file_list_task,no_thread):\n",
    "    import csv\n",
    "    \n",
    "    import nltk\n",
    "    from nltk.wsd import lesk\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    from collections import Counter\n",
    "    from nltk.corpus.reader.wordnet import WordNetError\n",
    "    import sys\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    from nltk import PorterStemmer\n",
    "    #from nltk.stem.porter import *\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "    from string import digits\n",
    "    with tf.device(gpuname):\n",
    "        \n",
    "        path_database=dir_path+\"/data/database/csv/\"\n",
    "        path_sub_tfidf=\"sub_tfidf/\"\n",
    "        path_full_tfidf=\"full_tfidf/\"\n",
    "        path_tf=\"sub_tf/\"\n",
    "        TF_File=\"TF-\"\n",
    "        TF_Full=\"TF-Full.csv\"\n",
    "                \n",
    "        for i in range(len(file_list_task)):    \n",
    "            #with tf.Session(config=config) as sess:\n",
    "            #index_paragraph=0\n",
    "            col=1\n",
    "\n",
    "            index_file=0\n",
    "            #sess.run(tf.global_variables_initializer())\n",
    "            #print(file_names)\n",
    "\n",
    "            #for filename in file_names:\n",
    "                #print(\"index_file\",str(index_file))\n",
    "            word_file_fatma=[]\n",
    "            filename=file_list_task[i]\n",
    "            with open(filename) as inf:\n",
    "                    #print(\"tpe\",type(inf))\n",
    "                txt=inf.read()\n",
    "\n",
    "                paragraph_list=txt_pragraphs(txt)   \n",
    "\n",
    "\n",
    "                for paragraph in paragraph_list: #get pragraphs(documents) from DB\n",
    "                        #print(\"Pragraph type \",type(paragraph))\n",
    "\n",
    "\n",
    "                    #if index_paragraph ==0:\n",
    "                        #index_paragraph += 1\n",
    "                    #else:\n",
    "\n",
    "                    setnences=pragraph_to_setnences(paragraph)#partitions paragraph to sentence\n",
    "\n",
    "\n",
    "                    for setnence in setnences:\n",
    "                                #print(\"  \",setnence)                            \n",
    "\n",
    "                        words=remove_stopword_sentences(setnence)#remove stop words and noise\n",
    "                        try:\n",
    "\n",
    "                            for word in words:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                lesk=lesk_word_sentence(setnence,word)#get LESK of word in sentence\n",
    "\n",
    "\n",
    "                                    #paragraph_word.append(word_sentence)\n",
    "\n",
    "                                if lesk is not None:\n",
    "                                    #print(\"type of lesk in words\",type(lesk),lesk)\n",
    "\n",
    "                                    word_file_fatma.append(lesk.name())\n",
    "\n",
    "                        except WordNetError as e:\n",
    "                                print(\"WordNetError on concept {}\".format(e))\n",
    "                        except AttributeError as e:\n",
    "                                print(\"Attribute error on concept {}\".format(e))\n",
    "                        except:\n",
    "                                print(\"Unexpected error on concept {}\".format(sys.exc_info()[0]))\n",
    "\n",
    "\n",
    "\n",
    "                        '''////////////////END Sentence////////////////# '''\n",
    "\n",
    "\n",
    "                    #write_cvs_by_pands(path_database,word_sentences_table,word_sentences_list,word_sentences_list_data)\n",
    "\n",
    "\n",
    "                    '''////////////////END PARAGRAPH////////////////# '''\n",
    "\n",
    "            #write_cvs_by_pands(path_database,sentences_paragraph_table,sentences_paragraph_list,sentences_paragraph_list_data)\n",
    "\n",
    "            #print(word_file_fatma)\n",
    "\n",
    "            word_file_Freq=Counter(word_file_fatma)\n",
    "            sum_count=sum(word_file_Freq.values())\n",
    "\n",
    "            #print(type(word_file_Freq))\n",
    "            #print(word_file_Freq)\n",
    "            #csv_df=pd.DataFrame([word_file_Freq],columns=word_file_Freq.keys() ) \n",
    "            freq=[]\n",
    "            for i in word_file_Freq.values():\n",
    "                c=i/sum_count\n",
    "                freq.append(c)\n",
    "            csv_df=pd.DataFrame([freq],columns=word_file_Freq.keys() ) \n",
    "\n",
    "\n",
    "            csv_df.to_csv(path_database+path_tf+TF_File+name_file(filename)+\".csv\")\n",
    "                # add to idf file \n",
    "\n",
    "            full_list=[]\n",
    "            full_list.insert(0,name_file(filename))\n",
    "            full_list=full_list+list(word_file_Freq.keys())\n",
    "            # add to single\n",
    "\n",
    "            add_row_csv(path_database+path_sub_tfidf,name_file(filename)+\".csv\",full_list)\n",
    "            # add to total idf file \n",
    "            add_row_csv(path_database+path_full_tfidf,TF_Full,full_list)\n",
    "            index_file +=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "    #return [x.name for x in local_device_protos]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(gpuname,file_list_task,no_thread):\n",
    "    print(\"\\n Thread No:\",no_thread,\"No File\",len(file_list_task),\"\\n\")\n",
    "    for i in range(len(file_list_task)):\n",
    "        with tf.device(\"/job:localhost/task:\"+str(i)+\"/device:\"+gpuname[1:]):\n",
    "\n",
    "            #for i in range(len(file_list_task)):\n",
    "            print(\"/job:localhost/task:\",i,\"device:\",gpuname[1:],\"file-->\",file_list_task[i],\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test2(gpuname,file_list_task,no_thread):\n",
    "    \n",
    "    with tf.device(gpuname):\n",
    "        print(\"\\n Thread No:\",no_thread,\"No File\",len(file_list_task),\"Devices\",gpuname,\"\\n\")\n",
    "        for i in range(len(file_list_task)):\n",
    "\n",
    "            #for i in range(len(file_list_task)):\n",
    "            print(\"/job:localhost/task:\",i,\"device:\",gpuname[1:],\"file-->\",file_list_task[i],\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gpuname' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1048c158c257>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mcoord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCoordinator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mgpuname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mgpu_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_list_task\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtotal_file_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_thread\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;31m#thread = threading.Thread(target=test, args=(gpu_list[i],total_file_list[i],i))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m#threads.append(thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gpuname' is not defined"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "from tensorflow.python.client import timeline\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    threads=[]\n",
    "    index=1\n",
    "    file_names=['cs1','cs2','cs3','cs4','cs5','cs6','cs7']\n",
    "    #print(\"in session\",len(file_names))\n",
    "    \n",
    "    gpu_list=['/gpu:0','/gpu:1']#get_available_gpus()\n",
    "    \n",
    "    sub_file_lsit_size=round(len(file_names)/len(gpu_list))\n",
    "    #print(sub_file_lsit_no)\n",
    "    total_file_list=[]\n",
    "    index_file_name=0\n",
    "   \n",
    "\n",
    "\n",
    "    for i in range(len(gpu_list)):\n",
    "        sub_file_list=[]\n",
    "        '''\n",
    "        to devide total file list on gpu\n",
    "        '''\n",
    "        for x in range(sub_file_lsit_size):\n",
    "            if index_file_name < len(file_names):\n",
    "                sub_file_list.append(file_names[index_file_name])\n",
    "                index_file_name +=1\n",
    "                #print(index_file_name)\n",
    "            \n",
    "        total_file_list.append(sub_file_list)\n",
    "    coord = tf.train.Coordinator()    \n",
    "    for i in range(len(gpu_list)):\n",
    "        \n",
    "        #thread = threading.Thread(target=test, args=(gpu_list[i],total_file_list[i],i))\n",
    "        #threads.append(thread)\n",
    "        #with tf.device(gpu_list[i]):\n",
    "        #test(gpu_list[i],total_file_list[i],i)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #for t in threads: t.start()\n",
    "    #coord.join(threads)\n",
    "    #print(total_file_list)\n",
    "    \n",
    "    \n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    '''for filename in file_names:\n",
    "        # Coordinate the loading of image files.\n",
    "        coord = tf.train.Coordinator()\n",
    "        thread = threading.Thread(target=MyLoop, args=(coord,sess,filename))\n",
    "        # thread.start()\n",
    "        threads.append(thread)\n",
    "        print(\"Thread No.\",index)\n",
    "        index +=1\n",
    "\n",
    "\n",
    "\n",
    "    for t in threads: t.start()\n",
    "    coord.join(threads)'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Hello, distributed TensorFlow!'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "c = tf.constant(\"Hello, distributed TensorFlow!\")\n",
    "server = tf.train.Server.create_local_server()\n",
    "sess = tf.Session(server.target)  # Create a session on the server.\n",
    "sess.run(c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = tf.constant(\"Hello, distributed TensorFlow!\")\n",
    "tf.train.ClusterSpec({\n",
    "    \"worker\": [\n",
    "        \"localhost:2222\"\n",
    "              ],\n",
    "    \"ps\": [\n",
    "        \"localhost:2222\"\n",
    "        \n",
    "    ]})\n",
    "# In task 0:\n",
    "cluster = tf.train.ClusterSpec({\"local\": [\"localhost:2222\", \"localhost:2223\"]})\n",
    "server = tf.train.Server(cluster, job_name=\"local\", task_index=0)\n",
    "server.join()\n",
    "sess = tf.Session(server.target)  # Create a session on the server.\n",
    "sess.run(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "c Tensor(\"truediv_10:0\", shape=(2000, 2000), dtype=float32, device=/device:CPU:0)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "c Tensor(\"truediv_11:0\", shape=(2000, 2000), dtype=float32, device=/device:CPU:1)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "start one 1510852221.6026764\n",
      "end one 1510852221.6026764\n",
      "Single op: 0.9528 sec\n",
      "start two 1510852222.5558467\n",
      "end two 1510852222.5558467\n",
      "Two ops in parallel: 2.00 sec (2.10 times slower)\n"
     ]
    }
   ],
   "source": [
    "# try running cpu intensive test on two devices\n",
    "print(\"start\")\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "def matmul_op():\n",
    "    \"\"\"Multiply two matrices together\"\"\"\n",
    "  \n",
    "    n = 2000\n",
    "    a = tf.ones((n, n), dtype=tf.float32)\n",
    "    c=tf.matmul(a, a)/n\n",
    "    print(\"c\",c)\n",
    "    for i in range(10):\n",
    "        print(i)\n",
    "    return c #tf.matmul(a, a)/n\n",
    "\n",
    "slow_op  = matmul_op\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    one = slow_op()\n",
    "with tf.device(\"/cpu:1\"):\n",
    "    another_one = slow_op()\n",
    "\n",
    "config = tf.ConfigProto(device_count={\"CPU\": 2},\n",
    "                        inter_op_parallelism_threads=2,\n",
    "                        intra_op_parallelism_threads=1)\n",
    "config.graph_options.optimizer_options.opt_level = -1\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "#two = one+another_one\n",
    "\n",
    "# pre-warm the kernels\n",
    "#print(\"ccc\",sess.run(one))\n",
    "\n",
    "start  = time.time()\n",
    "print(\"start one\",start)\n",
    "sess.run(one)\n",
    "end=time.time()\n",
    "print(\"end one\",start)\n",
    "elapsed_time = end - start\n",
    "print(\"Single op: %2.4f sec\"%(elapsed_time))\n",
    "\n",
    "start  = time.time()\n",
    "print(\"start two\",start)\n",
    "sess.run(two)\n",
    "end=time.time()\n",
    "print(\"end two\",start)\n",
    "elapsed_time2 = end-start\n",
    "print(\"Two ops in parallel: %.2f sec (%.2f times slower)\"%(elapsed_time2,\n",
    "elapsed_time2/elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/  gpu:0\n"
     ]
    }
   ],
   "source": [
    "s = '/gpu:0'\n",
    "n = 1\n",
    "a, s = s[:n], s[n:]\n",
    "print(a,\"\",s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x=[]\n",
    "\n",
    "for i in range(1000000):\n",
    "    s=\"cs \"+str(i)\n",
    "    \n",
    "    x.append(s)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cs 0',\n",
       " 'cs 1',\n",
       " 'cs 2',\n",
       " 'cs 3',\n",
       " 'cs 4',\n",
       " 'cs 5',\n",
       " 'cs 6',\n",
       " 'cs 7',\n",
       " 'cs 8',\n",
       " 'cs 9',\n",
       " 'cs 10',\n",
       " 'cs 11',\n",
       " 'cs 12',\n",
       " 'cs 13',\n",
       " 'cs 14',\n",
       " 'cs 15',\n",
       " 'cs 16',\n",
       " 'cs 17',\n",
       " 'cs 18',\n",
       " 'cs 19',\n",
       " 'cs 20',\n",
       " 'cs 21',\n",
       " 'cs 22',\n",
       " 'cs 23',\n",
       " 'cs 24',\n",
       " 'cs 25',\n",
       " 'cs 26',\n",
       " 'cs 27',\n",
       " 'cs 28',\n",
       " 'cs 29',\n",
       " 'cs 30',\n",
       " 'cs 31',\n",
       " 'cs 32',\n",
       " 'cs 33',\n",
       " 'cs 34',\n",
       " 'cs 35',\n",
       " 'cs 36',\n",
       " 'cs 37',\n",
       " 'cs 38',\n",
       " 'cs 39',\n",
       " 'cs 40',\n",
       " 'cs 41',\n",
       " 'cs 42',\n",
       " 'cs 43',\n",
       " 'cs 44',\n",
       " 'cs 45',\n",
       " 'cs 46',\n",
       " 'cs 47',\n",
       " 'cs 48',\n",
       " 'cs 49',\n",
       " 'cs 50',\n",
       " 'cs 51',\n",
       " 'cs 52',\n",
       " 'cs 53',\n",
       " 'cs 54',\n",
       " 'cs 55',\n",
       " 'cs 56',\n",
       " 'cs 57',\n",
       " 'cs 58',\n",
       " 'cs 59',\n",
       " 'cs 60',\n",
       " 'cs 61',\n",
       " 'cs 62',\n",
       " 'cs 63',\n",
       " 'cs 64',\n",
       " 'cs 65',\n",
       " 'cs 66',\n",
       " 'cs 67',\n",
       " 'cs 68',\n",
       " 'cs 69',\n",
       " 'cs 70',\n",
       " 'cs 71',\n",
       " 'cs 72',\n",
       " 'cs 73',\n",
       " 'cs 74',\n",
       " 'cs 75',\n",
       " 'cs 76',\n",
       " 'cs 77',\n",
       " 'cs 78',\n",
       " 'cs 79',\n",
       " 'cs 80',\n",
       " 'cs 81',\n",
       " 'cs 82',\n",
       " 'cs 83',\n",
       " 'cs 84',\n",
       " 'cs 85',\n",
       " 'cs 86',\n",
       " 'cs 87',\n",
       " 'cs 88',\n",
       " 'cs 89',\n",
       " 'cs 90',\n",
       " 'cs 91',\n",
       " 'cs 92',\n",
       " 'cs 93',\n",
       " 'cs 94',\n",
       " 'cs 95',\n",
       " 'cs 96',\n",
       " 'cs 97',\n",
       " 'cs 98',\n",
       " 'cs 99',\n",
       " 'cs 100',\n",
       " 'cs 101',\n",
       " 'cs 102',\n",
       " 'cs 103',\n",
       " 'cs 104',\n",
       " 'cs 105',\n",
       " 'cs 106',\n",
       " 'cs 107',\n",
       " 'cs 108',\n",
       " 'cs 109',\n",
       " 'cs 110',\n",
       " 'cs 111',\n",
       " 'cs 112',\n",
       " 'cs 113',\n",
       " 'cs 114',\n",
       " 'cs 115',\n",
       " 'cs 116',\n",
       " 'cs 117',\n",
       " 'cs 118',\n",
       " 'cs 119',\n",
       " 'cs 120',\n",
       " 'cs 121',\n",
       " 'cs 122',\n",
       " 'cs 123',\n",
       " 'cs 124',\n",
       " 'cs 125',\n",
       " 'cs 126',\n",
       " 'cs 127',\n",
       " 'cs 128',\n",
       " 'cs 129',\n",
       " 'cs 130',\n",
       " 'cs 131',\n",
       " 'cs 132',\n",
       " 'cs 133',\n",
       " 'cs 134',\n",
       " 'cs 135',\n",
       " 'cs 136',\n",
       " 'cs 137',\n",
       " 'cs 138',\n",
       " 'cs 139',\n",
       " 'cs 140',\n",
       " 'cs 141',\n",
       " 'cs 142',\n",
       " 'cs 143',\n",
       " 'cs 144',\n",
       " 'cs 145',\n",
       " 'cs 146',\n",
       " 'cs 147',\n",
       " 'cs 148',\n",
       " 'cs 149',\n",
       " 'cs 150',\n",
       " 'cs 151',\n",
       " 'cs 152',\n",
       " 'cs 153',\n",
       " 'cs 154',\n",
       " 'cs 155',\n",
       " 'cs 156',\n",
       " 'cs 157',\n",
       " 'cs 158',\n",
       " 'cs 159',\n",
       " 'cs 160',\n",
       " 'cs 161',\n",
       " 'cs 162',\n",
       " 'cs 163',\n",
       " 'cs 164',\n",
       " 'cs 165',\n",
       " 'cs 166',\n",
       " 'cs 167',\n",
       " 'cs 168',\n",
       " 'cs 169',\n",
       " 'cs 170',\n",
       " 'cs 171',\n",
       " 'cs 172',\n",
       " 'cs 173',\n",
       " 'cs 174',\n",
       " 'cs 175',\n",
       " 'cs 176',\n",
       " 'cs 177',\n",
       " 'cs 178',\n",
       " 'cs 179',\n",
       " 'cs 180',\n",
       " 'cs 181',\n",
       " 'cs 182',\n",
       " 'cs 183',\n",
       " 'cs 184',\n",
       " 'cs 185',\n",
       " 'cs 186',\n",
       " 'cs 187',\n",
       " 'cs 188',\n",
       " 'cs 189',\n",
       " 'cs 190',\n",
       " 'cs 191',\n",
       " 'cs 192',\n",
       " 'cs 193',\n",
       " 'cs 194',\n",
       " 'cs 195',\n",
       " 'cs 196',\n",
       " 'cs 197',\n",
       " 'cs 198',\n",
       " 'cs 199',\n",
       " 'cs 200',\n",
       " 'cs 201',\n",
       " 'cs 202',\n",
       " 'cs 203',\n",
       " 'cs 204',\n",
       " 'cs 205',\n",
       " 'cs 206',\n",
       " 'cs 207',\n",
       " 'cs 208',\n",
       " 'cs 209',\n",
       " 'cs 210',\n",
       " 'cs 211',\n",
       " 'cs 212',\n",
       " 'cs 213',\n",
       " 'cs 214',\n",
       " 'cs 215',\n",
       " 'cs 216',\n",
       " 'cs 217',\n",
       " 'cs 218',\n",
       " 'cs 219',\n",
       " 'cs 220',\n",
       " 'cs 221',\n",
       " 'cs 222',\n",
       " 'cs 223',\n",
       " 'cs 224',\n",
       " 'cs 225',\n",
       " 'cs 226',\n",
       " 'cs 227',\n",
       " 'cs 228',\n",
       " 'cs 229',\n",
       " 'cs 230',\n",
       " 'cs 231',\n",
       " 'cs 232',\n",
       " 'cs 233',\n",
       " 'cs 234',\n",
       " 'cs 235',\n",
       " 'cs 236',\n",
       " 'cs 237',\n",
       " 'cs 238',\n",
       " 'cs 239',\n",
       " 'cs 240',\n",
       " 'cs 241',\n",
       " 'cs 242',\n",
       " 'cs 243',\n",
       " 'cs 244',\n",
       " 'cs 245',\n",
       " 'cs 246',\n",
       " 'cs 247',\n",
       " 'cs 248',\n",
       " 'cs 249',\n",
       " 'cs 250',\n",
       " 'cs 251',\n",
       " 'cs 252',\n",
       " 'cs 253',\n",
       " 'cs 254',\n",
       " 'cs 255',\n",
       " 'cs 256',\n",
       " 'cs 257',\n",
       " 'cs 258',\n",
       " 'cs 259',\n",
       " 'cs 260',\n",
       " 'cs 261',\n",
       " 'cs 262',\n",
       " 'cs 263',\n",
       " 'cs 264',\n",
       " 'cs 265',\n",
       " 'cs 266',\n",
       " 'cs 267',\n",
       " 'cs 268',\n",
       " 'cs 269',\n",
       " 'cs 270',\n",
       " 'cs 271',\n",
       " 'cs 272',\n",
       " 'cs 273',\n",
       " 'cs 274',\n",
       " 'cs 275',\n",
       " 'cs 276',\n",
       " 'cs 277',\n",
       " 'cs 278',\n",
       " 'cs 279',\n",
       " 'cs 280',\n",
       " 'cs 281',\n",
       " 'cs 282',\n",
       " 'cs 283',\n",
       " 'cs 284',\n",
       " 'cs 285',\n",
       " 'cs 286',\n",
       " 'cs 287',\n",
       " 'cs 288',\n",
       " 'cs 289',\n",
       " 'cs 290',\n",
       " 'cs 291',\n",
       " 'cs 292',\n",
       " 'cs 293',\n",
       " 'cs 294',\n",
       " 'cs 295',\n",
       " 'cs 296',\n",
       " 'cs 297',\n",
       " 'cs 298',\n",
       " 'cs 299',\n",
       " 'cs 300',\n",
       " 'cs 301',\n",
       " 'cs 302',\n",
       " 'cs 303',\n",
       " 'cs 304',\n",
       " 'cs 305',\n",
       " 'cs 306',\n",
       " 'cs 307',\n",
       " 'cs 308',\n",
       " 'cs 309',\n",
       " 'cs 310',\n",
       " 'cs 311',\n",
       " 'cs 312',\n",
       " 'cs 313',\n",
       " 'cs 314',\n",
       " 'cs 315',\n",
       " 'cs 316',\n",
       " 'cs 317',\n",
       " 'cs 318',\n",
       " 'cs 319',\n",
       " 'cs 320',\n",
       " 'cs 321',\n",
       " 'cs 322',\n",
       " 'cs 323',\n",
       " 'cs 324',\n",
       " 'cs 325',\n",
       " 'cs 326',\n",
       " 'cs 327',\n",
       " 'cs 328',\n",
       " 'cs 329',\n",
       " 'cs 330',\n",
       " 'cs 331',\n",
       " 'cs 332',\n",
       " 'cs 333',\n",
       " 'cs 334',\n",
       " 'cs 335',\n",
       " 'cs 336',\n",
       " 'cs 337',\n",
       " 'cs 338',\n",
       " 'cs 339',\n",
       " 'cs 340',\n",
       " 'cs 341',\n",
       " 'cs 342',\n",
       " 'cs 343',\n",
       " 'cs 344',\n",
       " 'cs 345',\n",
       " 'cs 346',\n",
       " 'cs 347',\n",
       " 'cs 348',\n",
       " 'cs 349',\n",
       " 'cs 350',\n",
       " 'cs 351',\n",
       " 'cs 352',\n",
       " 'cs 353',\n",
       " 'cs 354',\n",
       " 'cs 355',\n",
       " 'cs 356',\n",
       " 'cs 357',\n",
       " 'cs 358',\n",
       " 'cs 359',\n",
       " 'cs 360',\n",
       " 'cs 361',\n",
       " 'cs 362',\n",
       " 'cs 363',\n",
       " 'cs 364',\n",
       " 'cs 365',\n",
       " 'cs 366',\n",
       " 'cs 367',\n",
       " 'cs 368',\n",
       " 'cs 369',\n",
       " 'cs 370',\n",
       " 'cs 371',\n",
       " 'cs 372',\n",
       " 'cs 373',\n",
       " 'cs 374',\n",
       " 'cs 375',\n",
       " 'cs 376',\n",
       " 'cs 377',\n",
       " 'cs 378',\n",
       " 'cs 379',\n",
       " 'cs 380',\n",
       " 'cs 381',\n",
       " 'cs 382',\n",
       " 'cs 383',\n",
       " 'cs 384',\n",
       " 'cs 385',\n",
       " 'cs 386',\n",
       " 'cs 387',\n",
       " 'cs 388',\n",
       " 'cs 389',\n",
       " 'cs 390',\n",
       " 'cs 391',\n",
       " 'cs 392',\n",
       " 'cs 393',\n",
       " 'cs 394',\n",
       " 'cs 395',\n",
       " 'cs 396',\n",
       " 'cs 397',\n",
       " 'cs 398',\n",
       " 'cs 399',\n",
       " 'cs 400',\n",
       " 'cs 401',\n",
       " 'cs 402',\n",
       " 'cs 403',\n",
       " 'cs 404',\n",
       " 'cs 405',\n",
       " 'cs 406',\n",
       " 'cs 407',\n",
       " 'cs 408',\n",
       " 'cs 409',\n",
       " 'cs 410',\n",
       " 'cs 411',\n",
       " 'cs 412',\n",
       " 'cs 413',\n",
       " 'cs 414',\n",
       " 'cs 415',\n",
       " 'cs 416',\n",
       " 'cs 417',\n",
       " 'cs 418',\n",
       " 'cs 419',\n",
       " 'cs 420',\n",
       " 'cs 421',\n",
       " 'cs 422',\n",
       " 'cs 423',\n",
       " 'cs 424',\n",
       " 'cs 425',\n",
       " 'cs 426',\n",
       " 'cs 427',\n",
       " 'cs 428',\n",
       " 'cs 429',\n",
       " 'cs 430',\n",
       " 'cs 431',\n",
       " 'cs 432',\n",
       " 'cs 433',\n",
       " 'cs 434',\n",
       " 'cs 435',\n",
       " 'cs 436',\n",
       " 'cs 437',\n",
       " 'cs 438',\n",
       " 'cs 439',\n",
       " 'cs 440',\n",
       " 'cs 441',\n",
       " 'cs 442',\n",
       " 'cs 443',\n",
       " 'cs 444',\n",
       " 'cs 445',\n",
       " 'cs 446',\n",
       " 'cs 447',\n",
       " 'cs 448',\n",
       " 'cs 449',\n",
       " 'cs 450',\n",
       " 'cs 451',\n",
       " 'cs 452',\n",
       " 'cs 453',\n",
       " 'cs 454',\n",
       " 'cs 455',\n",
       " 'cs 456',\n",
       " 'cs 457',\n",
       " 'cs 458',\n",
       " 'cs 459',\n",
       " 'cs 460',\n",
       " 'cs 461',\n",
       " 'cs 462',\n",
       " 'cs 463',\n",
       " 'cs 464',\n",
       " 'cs 465',\n",
       " 'cs 466',\n",
       " 'cs 467',\n",
       " 'cs 468',\n",
       " 'cs 469',\n",
       " 'cs 470',\n",
       " 'cs 471',\n",
       " 'cs 472',\n",
       " 'cs 473',\n",
       " 'cs 474',\n",
       " 'cs 475',\n",
       " 'cs 476',\n",
       " 'cs 477',\n",
       " 'cs 478',\n",
       " 'cs 479',\n",
       " 'cs 480',\n",
       " 'cs 481',\n",
       " 'cs 482',\n",
       " 'cs 483',\n",
       " 'cs 484',\n",
       " 'cs 485',\n",
       " 'cs 486',\n",
       " 'cs 487',\n",
       " 'cs 488',\n",
       " 'cs 489',\n",
       " 'cs 490',\n",
       " 'cs 491',\n",
       " 'cs 492',\n",
       " 'cs 493',\n",
       " 'cs 494',\n",
       " 'cs 495',\n",
       " 'cs 496',\n",
       " 'cs 497',\n",
       " 'cs 498',\n",
       " 'cs 499',\n",
       " 'cs 500',\n",
       " 'cs 501',\n",
       " 'cs 502',\n",
       " 'cs 503',\n",
       " 'cs 504',\n",
       " 'cs 505',\n",
       " 'cs 506',\n",
       " 'cs 507',\n",
       " 'cs 508',\n",
       " 'cs 509',\n",
       " 'cs 510',\n",
       " 'cs 511',\n",
       " 'cs 512',\n",
       " 'cs 513',\n",
       " 'cs 514',\n",
       " 'cs 515',\n",
       " 'cs 516',\n",
       " 'cs 517',\n",
       " 'cs 518',\n",
       " 'cs 519',\n",
       " 'cs 520',\n",
       " 'cs 521',\n",
       " 'cs 522',\n",
       " 'cs 523',\n",
       " 'cs 524',\n",
       " 'cs 525',\n",
       " 'cs 526',\n",
       " 'cs 527',\n",
       " 'cs 528',\n",
       " 'cs 529',\n",
       " 'cs 530',\n",
       " 'cs 531',\n",
       " 'cs 532',\n",
       " 'cs 533',\n",
       " 'cs 534',\n",
       " 'cs 535',\n",
       " 'cs 536',\n",
       " 'cs 537',\n",
       " 'cs 538',\n",
       " 'cs 539',\n",
       " 'cs 540',\n",
       " 'cs 541',\n",
       " 'cs 542',\n",
       " 'cs 543',\n",
       " 'cs 544',\n",
       " 'cs 545',\n",
       " 'cs 546',\n",
       " 'cs 547',\n",
       " 'cs 548',\n",
       " 'cs 549',\n",
       " 'cs 550',\n",
       " 'cs 551',\n",
       " 'cs 552',\n",
       " 'cs 553',\n",
       " 'cs 554',\n",
       " 'cs 555',\n",
       " 'cs 556',\n",
       " 'cs 557',\n",
       " 'cs 558',\n",
       " 'cs 559',\n",
       " 'cs 560',\n",
       " 'cs 561',\n",
       " 'cs 562',\n",
       " 'cs 563',\n",
       " 'cs 564',\n",
       " 'cs 565',\n",
       " 'cs 566',\n",
       " 'cs 567',\n",
       " 'cs 568',\n",
       " 'cs 569',\n",
       " 'cs 570',\n",
       " 'cs 571',\n",
       " 'cs 572',\n",
       " 'cs 573',\n",
       " 'cs 574',\n",
       " 'cs 575',\n",
       " 'cs 576',\n",
       " 'cs 577',\n",
       " 'cs 578',\n",
       " 'cs 579',\n",
       " 'cs 580',\n",
       " 'cs 581',\n",
       " 'cs 582',\n",
       " 'cs 583',\n",
       " 'cs 584',\n",
       " 'cs 585',\n",
       " 'cs 586',\n",
       " 'cs 587',\n",
       " 'cs 588',\n",
       " 'cs 589',\n",
       " 'cs 590',\n",
       " 'cs 591',\n",
       " 'cs 592',\n",
       " 'cs 593',\n",
       " 'cs 594',\n",
       " 'cs 595',\n",
       " 'cs 596',\n",
       " 'cs 597',\n",
       " 'cs 598',\n",
       " 'cs 599',\n",
       " 'cs 600',\n",
       " 'cs 601',\n",
       " 'cs 602',\n",
       " 'cs 603',\n",
       " 'cs 604',\n",
       " 'cs 605',\n",
       " 'cs 606',\n",
       " 'cs 607',\n",
       " 'cs 608',\n",
       " 'cs 609',\n",
       " 'cs 610',\n",
       " 'cs 611',\n",
       " 'cs 612',\n",
       " 'cs 613',\n",
       " 'cs 614',\n",
       " 'cs 615',\n",
       " 'cs 616',\n",
       " 'cs 617',\n",
       " 'cs 618',\n",
       " 'cs 619',\n",
       " 'cs 620',\n",
       " 'cs 621',\n",
       " 'cs 622',\n",
       " 'cs 623',\n",
       " 'cs 624',\n",
       " 'cs 625',\n",
       " 'cs 626',\n",
       " 'cs 627',\n",
       " 'cs 628',\n",
       " 'cs 629',\n",
       " 'cs 630',\n",
       " 'cs 631',\n",
       " 'cs 632',\n",
       " 'cs 633',\n",
       " 'cs 634',\n",
       " 'cs 635',\n",
       " 'cs 636',\n",
       " 'cs 637',\n",
       " 'cs 638',\n",
       " 'cs 639',\n",
       " 'cs 640',\n",
       " 'cs 641',\n",
       " 'cs 642',\n",
       " 'cs 643',\n",
       " 'cs 644',\n",
       " 'cs 645',\n",
       " 'cs 646',\n",
       " 'cs 647',\n",
       " 'cs 648',\n",
       " 'cs 649',\n",
       " 'cs 650',\n",
       " 'cs 651',\n",
       " 'cs 652',\n",
       " 'cs 653',\n",
       " 'cs 654',\n",
       " 'cs 655',\n",
       " 'cs 656',\n",
       " 'cs 657',\n",
       " 'cs 658',\n",
       " 'cs 659',\n",
       " 'cs 660',\n",
       " 'cs 661',\n",
       " 'cs 662',\n",
       " 'cs 663',\n",
       " 'cs 664',\n",
       " 'cs 665',\n",
       " 'cs 666',\n",
       " 'cs 667',\n",
       " 'cs 668',\n",
       " 'cs 669',\n",
       " 'cs 670',\n",
       " 'cs 671',\n",
       " 'cs 672',\n",
       " 'cs 673',\n",
       " 'cs 674',\n",
       " 'cs 675',\n",
       " 'cs 676',\n",
       " 'cs 677',\n",
       " 'cs 678',\n",
       " 'cs 679',\n",
       " 'cs 680',\n",
       " 'cs 681',\n",
       " 'cs 682',\n",
       " 'cs 683',\n",
       " 'cs 684',\n",
       " 'cs 685',\n",
       " 'cs 686',\n",
       " 'cs 687',\n",
       " 'cs 688',\n",
       " 'cs 689',\n",
       " 'cs 690',\n",
       " 'cs 691',\n",
       " 'cs 692',\n",
       " 'cs 693',\n",
       " 'cs 694',\n",
       " 'cs 695',\n",
       " 'cs 696',\n",
       " 'cs 697',\n",
       " 'cs 698',\n",
       " 'cs 699',\n",
       " 'cs 700',\n",
       " 'cs 701',\n",
       " 'cs 702',\n",
       " 'cs 703',\n",
       " 'cs 704',\n",
       " 'cs 705',\n",
       " 'cs 706',\n",
       " 'cs 707',\n",
       " 'cs 708',\n",
       " 'cs 709',\n",
       " 'cs 710',\n",
       " 'cs 711',\n",
       " 'cs 712',\n",
       " 'cs 713',\n",
       " 'cs 714',\n",
       " 'cs 715',\n",
       " 'cs 716',\n",
       " 'cs 717',\n",
       " 'cs 718',\n",
       " 'cs 719',\n",
       " 'cs 720',\n",
       " 'cs 721',\n",
       " 'cs 722',\n",
       " 'cs 723',\n",
       " 'cs 724',\n",
       " 'cs 725',\n",
       " 'cs 726',\n",
       " 'cs 727',\n",
       " 'cs 728',\n",
       " 'cs 729',\n",
       " 'cs 730',\n",
       " 'cs 731',\n",
       " 'cs 732',\n",
       " 'cs 733',\n",
       " 'cs 734',\n",
       " 'cs 735',\n",
       " 'cs 736',\n",
       " 'cs 737',\n",
       " 'cs 738',\n",
       " 'cs 739',\n",
       " 'cs 740',\n",
       " 'cs 741',\n",
       " 'cs 742',\n",
       " 'cs 743',\n",
       " 'cs 744',\n",
       " 'cs 745',\n",
       " 'cs 746',\n",
       " 'cs 747',\n",
       " 'cs 748',\n",
       " 'cs 749',\n",
       " 'cs 750',\n",
       " 'cs 751',\n",
       " 'cs 752',\n",
       " 'cs 753',\n",
       " 'cs 754',\n",
       " 'cs 755',\n",
       " 'cs 756',\n",
       " 'cs 757',\n",
       " 'cs 758',\n",
       " 'cs 759',\n",
       " 'cs 760',\n",
       " 'cs 761',\n",
       " 'cs 762',\n",
       " 'cs 763',\n",
       " 'cs 764',\n",
       " 'cs 765',\n",
       " 'cs 766',\n",
       " 'cs 767',\n",
       " 'cs 768',\n",
       " 'cs 769',\n",
       " 'cs 770',\n",
       " 'cs 771',\n",
       " 'cs 772',\n",
       " 'cs 773',\n",
       " 'cs 774',\n",
       " 'cs 775',\n",
       " 'cs 776',\n",
       " 'cs 777',\n",
       " 'cs 778',\n",
       " 'cs 779',\n",
       " 'cs 780',\n",
       " 'cs 781',\n",
       " 'cs 782',\n",
       " 'cs 783',\n",
       " 'cs 784',\n",
       " 'cs 785',\n",
       " 'cs 786',\n",
       " 'cs 787',\n",
       " 'cs 788',\n",
       " 'cs 789',\n",
       " 'cs 790',\n",
       " 'cs 791',\n",
       " 'cs 792',\n",
       " 'cs 793',\n",
       " 'cs 794',\n",
       " 'cs 795',\n",
       " 'cs 796',\n",
       " 'cs 797',\n",
       " 'cs 798',\n",
       " 'cs 799',\n",
       " 'cs 800',\n",
       " 'cs 801',\n",
       " 'cs 802',\n",
       " 'cs 803',\n",
       " 'cs 804',\n",
       " 'cs 805',\n",
       " 'cs 806',\n",
       " 'cs 807',\n",
       " 'cs 808',\n",
       " 'cs 809',\n",
       " 'cs 810',\n",
       " 'cs 811',\n",
       " 'cs 812',\n",
       " 'cs 813',\n",
       " 'cs 814',\n",
       " 'cs 815',\n",
       " 'cs 816',\n",
       " 'cs 817',\n",
       " 'cs 818',\n",
       " 'cs 819',\n",
       " 'cs 820',\n",
       " 'cs 821',\n",
       " 'cs 822',\n",
       " 'cs 823',\n",
       " 'cs 824',\n",
       " 'cs 825',\n",
       " 'cs 826',\n",
       " 'cs 827',\n",
       " 'cs 828',\n",
       " 'cs 829',\n",
       " 'cs 830',\n",
       " 'cs 831',\n",
       " 'cs 832',\n",
       " 'cs 833',\n",
       " 'cs 834',\n",
       " 'cs 835',\n",
       " 'cs 836',\n",
       " 'cs 837',\n",
       " 'cs 838',\n",
       " 'cs 839',\n",
       " 'cs 840',\n",
       " 'cs 841',\n",
       " 'cs 842',\n",
       " 'cs 843',\n",
       " 'cs 844',\n",
       " 'cs 845',\n",
       " 'cs 846',\n",
       " 'cs 847',\n",
       " 'cs 848',\n",
       " 'cs 849',\n",
       " 'cs 850',\n",
       " 'cs 851',\n",
       " 'cs 852',\n",
       " 'cs 853',\n",
       " 'cs 854',\n",
       " 'cs 855',\n",
       " 'cs 856',\n",
       " 'cs 857',\n",
       " 'cs 858',\n",
       " 'cs 859',\n",
       " 'cs 860',\n",
       " 'cs 861',\n",
       " 'cs 862',\n",
       " 'cs 863',\n",
       " 'cs 864',\n",
       " 'cs 865',\n",
       " 'cs 866',\n",
       " 'cs 867',\n",
       " 'cs 868',\n",
       " 'cs 869',\n",
       " 'cs 870',\n",
       " 'cs 871',\n",
       " 'cs 872',\n",
       " 'cs 873',\n",
       " 'cs 874',\n",
       " 'cs 875',\n",
       " 'cs 876',\n",
       " 'cs 877',\n",
       " 'cs 878',\n",
       " 'cs 879',\n",
       " 'cs 880',\n",
       " 'cs 881',\n",
       " 'cs 882',\n",
       " 'cs 883',\n",
       " 'cs 884',\n",
       " 'cs 885',\n",
       " 'cs 886',\n",
       " 'cs 887',\n",
       " 'cs 888',\n",
       " 'cs 889',\n",
       " 'cs 890',\n",
       " 'cs 891',\n",
       " 'cs 892',\n",
       " 'cs 893',\n",
       " 'cs 894',\n",
       " 'cs 895',\n",
       " 'cs 896',\n",
       " 'cs 897',\n",
       " 'cs 898',\n",
       " 'cs 899',\n",
       " 'cs 900',\n",
       " 'cs 901',\n",
       " 'cs 902',\n",
       " 'cs 903',\n",
       " 'cs 904',\n",
       " 'cs 905',\n",
       " 'cs 906',\n",
       " 'cs 907',\n",
       " 'cs 908',\n",
       " 'cs 909',\n",
       " 'cs 910',\n",
       " 'cs 911',\n",
       " 'cs 912',\n",
       " 'cs 913',\n",
       " 'cs 914',\n",
       " 'cs 915',\n",
       " 'cs 916',\n",
       " 'cs 917',\n",
       " 'cs 918',\n",
       " 'cs 919',\n",
       " 'cs 920',\n",
       " 'cs 921',\n",
       " 'cs 922',\n",
       " 'cs 923',\n",
       " 'cs 924',\n",
       " 'cs 925',\n",
       " 'cs 926',\n",
       " 'cs 927',\n",
       " 'cs 928',\n",
       " 'cs 929',\n",
       " 'cs 930',\n",
       " 'cs 931',\n",
       " 'cs 932',\n",
       " 'cs 933',\n",
       " 'cs 934',\n",
       " 'cs 935',\n",
       " 'cs 936',\n",
       " 'cs 937',\n",
       " 'cs 938',\n",
       " 'cs 939',\n",
       " 'cs 940',\n",
       " 'cs 941',\n",
       " 'cs 942',\n",
       " 'cs 943',\n",
       " 'cs 944',\n",
       " 'cs 945',\n",
       " 'cs 946',\n",
       " 'cs 947',\n",
       " 'cs 948',\n",
       " 'cs 949',\n",
       " 'cs 950',\n",
       " 'cs 951',\n",
       " 'cs 952',\n",
       " 'cs 953',\n",
       " 'cs 954',\n",
       " 'cs 955',\n",
       " 'cs 956',\n",
       " 'cs 957',\n",
       " 'cs 958',\n",
       " 'cs 959',\n",
       " 'cs 960',\n",
       " 'cs 961',\n",
       " 'cs 962',\n",
       " 'cs 963',\n",
       " 'cs 964',\n",
       " 'cs 965',\n",
       " 'cs 966',\n",
       " 'cs 967',\n",
       " 'cs 968',\n",
       " 'cs 969',\n",
       " 'cs 970',\n",
       " 'cs 971',\n",
       " 'cs 972',\n",
       " 'cs 973',\n",
       " 'cs 974',\n",
       " 'cs 975',\n",
       " 'cs 976',\n",
       " 'cs 977',\n",
       " 'cs 978',\n",
       " 'cs 979',\n",
       " 'cs 980',\n",
       " 'cs 981',\n",
       " 'cs 982',\n",
       " 'cs 983',\n",
       " 'cs 984',\n",
       " 'cs 985',\n",
       " 'cs 986',\n",
       " 'cs 987',\n",
       " 'cs 988',\n",
       " 'cs 989',\n",
       " 'cs 990',\n",
       " 'cs 991',\n",
       " 'cs 992',\n",
       " 'cs 993',\n",
       " 'cs 994',\n",
       " 'cs 995',\n",
       " 'cs 996',\n",
       " 'cs 997',\n",
       " 'cs 998',\n",
       " 'cs 999',\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
