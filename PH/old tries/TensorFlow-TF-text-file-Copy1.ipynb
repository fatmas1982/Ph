{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/python3.5\n",
    "#https://learningtensorflow.com/ReadingFilesBasic/\n",
    "#https://www.tensorflow.org/tutorials/using_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To compute term frequancy from multi txt file and save it in multi csv and one csv by parellel process using tensorflow on GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "'''import csv\n",
    "from tensorflow.python.client import timeline\n",
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import Counter\n",
    "import threading\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import digits'''\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pathes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "\n",
    "path_data_source=dir_path+\"/data/data_source/\"\n",
    "\n",
    "#sub_path_data_source=\"demo_onefile/\"\n",
    "\n",
    "sub_path_data_source=\"demo/\"\n",
    "\n",
    "file_path=path_data_source+sub_path_data_source\n",
    "\n",
    "file_names = [os.path.join(file_path, f) \n",
    "                      for f in os.listdir(file_path) \n",
    "                      if f.endswith(\".txt\")]\n",
    "\n",
    "#sub_path_data_source=\"demo/\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#path_database=dir_path+\"/data/database/csv/\"\n",
    "#path_sub_tfidf=\"sub_tfidf/\"\n",
    "#path_full_tfidf=\"full_tfidf/\"\n",
    "#path_tf=\"sub_tf/\"\n",
    "#path_database='/home/fsg/output_csv/database/'\n",
    "#path_database='/media/fsg/74C86089C8604C04/PHD/Softwares/ph/database/' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_source = 'cs.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table Data Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TF_File=\"TF-\"\n",
    "#TF_Full=\"TF-Full.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML Data Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-abe0311a44d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                         log_device_placement=True)\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-87-abe0311a44d7>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m                         log_device_placement=True)\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''config = tf.ConfigProto(device_count={\"CPU\": 7},\n",
    "                        allow_soft_placement=True,\n",
    "                        inter_op_parallelism_threads=1,\n",
    "                        intra_op_parallelism_threads=1,\n",
    "                        use_per_session_threads=True)'''\n",
    "#import pprint\n",
    "\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL']='5'\n",
    "config = tf.ConfigProto(device_count={\"GPU\": 2,\"CPU\":32},\n",
    "                        allow_soft_placement=True,\n",
    "                        inter_op_parallelism_threads=32,\n",
    "                        intra_op_parallelism_threads=32,\n",
    "                        use_per_session_threads=True,\n",
    "                        log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "#pprint.pprint({ k:str(getattr(config,k)) for k in sorted(dir(config)) if not k.startswith('_')})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Write Excell sheet\n",
    "'''\n",
    "def save_file_to_database(data_rows,path_database,file_databbase,header_list):\n",
    "    import csv\n",
    "    outfile = open(path_database+file_databbase,'w')\n",
    "    writer=csv.writer(outfile)\n",
    "    #header_list=['uuid','paragraph','doc_id']\n",
    "    i=0\n",
    "    for line in data_rows:\n",
    "        row=[i,line,'paragraph no.'+str(i)]\n",
    "        if i==0:\n",
    "            \n",
    "            writer.writerow(header_list)\n",
    "            writer.writerow(row)\n",
    "        else:\n",
    "            #print('ff')\n",
    "            writer.writerow(row)\n",
    "        i+= 1\n",
    "        #outfile.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Read Excell sheet\n",
    "'''\n",
    "def read_text_from_database(path_database,file_databbase):\n",
    "    import csv\n",
    "    queue_paragraph=[]\n",
    "    #f = open(sys.argv[1], 'rt')\n",
    "    outfile = open(path_database+file_databbase,'rt')\n",
    "    try:\n",
    "                \n",
    "        reader=csv.reader(outfile)\n",
    "        for row in reader:\n",
    "            queue_paragraph.append(row)\n",
    "            #print (row)\n",
    "    finally:\n",
    "        print (\"row\")\n",
    "        outfile.close()\n",
    "        \n",
    "    return queue_paragraph\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_cvs_by_pands(path_database,file_databbase,index_col, header):\n",
    "    import csv\n",
    "    return pd.read_csv(path_database+file_databbase,index_col=index_col,header=header)\n",
    "\n",
    "#read_cvs_by_pands(path_database,paragraph_table,index_col=0,header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_cvs_by_pands(path_database,file_databbase,header,data_rows):\n",
    "    import csv\n",
    "    csv_df=pd.DataFrame(data_rows,columns=header ) \n",
    "    csv_df.to_csv(path_database+file_databbase)\n",
    "\n",
    "    \n",
    "\n",
    "#write_cvs_by_pands(path_database,sentences_paragraph_table,sentences_paragraph_list,sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save pragraphs to files\n",
    "def write_file(pragraph,num_pragraph,path):\n",
    "    file = open(path+str(num_pragraph)+\".txt\",\"w\") \n",
    " \n",
    "    file.write(pragraph) \n",
    "    \n",
    "    file.close() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create sub dataset\n",
    "def sub_dataset(path_data_source,data_source):\n",
    "    pragraphs=txt_pragraphs(read_file(path_data_source+data_source))\n",
    "    counter=0\n",
    "    for pragraph in pragraphs:\n",
    "        print('pragraph no ',counter)\n",
    "        write_file(pragraph,counter,sub_path_data_source)\n",
    "        counter +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre processing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Huge File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(str):\n",
    "    file = open(str,'r')\n",
    "    txt=file.read()\n",
    "    #print(txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Document to pragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def txt_pragraphs(str):\n",
    "    pragraphs = str.split(\"\\n\\n\")\n",
    "    return pragraphs\n",
    "#pragraphs=txt_pragraphs(txt)\n",
    "#type(pragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Split Paragraph to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pragraph_to_setnences(str):\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    return sent_tokenize(str)\n",
    "#setnences=pragraph_to_setnences(pragraphs[n_pragraph])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Process For Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing English stopwords and Punct per Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_stop_words = ['the', 'that', 'to', 'as', 'there', 'has', 'and', 'or', 'is', 'not', 'a', 'of', 'but', 'in', 'by', 'on', 'are', 'it', 'if','what','where','how','when']\n",
    "new_stop_words2=['--','i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now','even','until','then','must']\n",
    "numbers=[1,2,3,4,5,6,7,8,9]\n",
    "#stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "def remove_stopword_sentences(sent):\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    from string import digits\n",
    "    from nltk.corpus.reader.wordnet import WordNetError\n",
    "    import sys\n",
    "    try:\n",
    "        tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "    \n",
    "        words=tokenizer.tokenize(sent)\n",
    "    \n",
    "        english_stops = set(stopwords.words('english'))\n",
    "        #stems=[]\n",
    "        list_word=[word for word in words if word.lower() not in english_stops and word.lower() not in new_stop_words and word.lower() not in new_stop_words2 and  not word.lower().isdigit() and word.lower() not in digits and word.lower() not in  numbers]\n",
    "    \n",
    "    #for word in list_word:\n",
    "        #stems.append(stem(word))\n",
    "        #stems.append(PorterStemmer().stem(word))\n",
    "        #stems.append(stemmer.stem(word))\n",
    "        #stems.append(stemmer.stem(\"computer\"))\n",
    "        #stems.append(word)\n",
    "    except WordNetError as e:\n",
    "        print(\"WordNetError on concept {}\".format(sent))\n",
    "    except AttributeError as e:\n",
    "        print(\"Attribute error on concept {}: {}\".format(sent, e.message))\n",
    "    except:\n",
    "        print(\"Unexpected error on concept {}: {}\".format(sent, sys.exc_info()[0]))\n",
    "    \n",
    "    return list_word#stems#(stem(setem_word for setem_word in  ([word for word in words if word not in english_stops and word not in new_stop_words])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#! /usr/bin/python3.5\n",
    "\n",
    "#https://learningtensorflow.com/ReadingFilesBasic/\n",
    "\n",
    "#https://www.tensorflow.org/tutorials/using_gpu\n",
    "\n",
    "#To compute term frequancy from multi txt file and save it in multi csv and one csv by parellel process using tensorflow on GPU\n",
    "\n",
    "​\n",
    "\n",
    "import os\n",
    "\n",
    "​\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "'''import csv\n",
    "\n",
    "from tensorflow.python.client import timeline\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import threading\n",
    "\n",
    "​\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "​\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from string import digits'''\n",
    "\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "\n",
    "​\n",
    "\n",
    "path_data_source=dir_path+\"/data/data_source/\"\n",
    "\n",
    "​\n",
    "\n",
    "#sub_path_data_source=\"demo_onefile/\"\n",
    "\n",
    "​\n",
    "\n",
    "sub_path_data_source=\"demo/\"\n",
    "\n",
    "​\n",
    "\n",
    "file_path=path_data_source+sub_path_data_source\n",
    "\n",
    "​\n",
    "\n",
    "file_names = [os.path.join(file_path, f) \n",
    "\n",
    "                      for f in os.listdir(file_path) \n",
    "\n",
    "                      if f.endswith(\".txt\")]\n",
    "\n",
    "​\n",
    "\n",
    "#sub_path_data_source=\"demo/\"\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "Table Database\n",
    "\n",
    "​\n",
    "\n",
    "#path_database=dir_path+\"/data/database/csv/\"\n",
    "\n",
    "#path_sub_tfidf=\"sub_tfidf/\"\n",
    "\n",
    "#path_full_tfidf=\"full_tfidf/\"\n",
    "\n",
    "#path_tf=\"sub_tf/\"\n",
    "\n",
    "#path_database='/home/fsg/output_csv/database/'\n",
    "\n",
    "#path_database='/media/fsg/74C86089C8604C04/PHD/Softwares/ph/database/' \n",
    "\n",
    "Data Source\n",
    "\n",
    "data_source = 'cs.txt'\n",
    "\n",
    "​\n",
    "\n",
    "Tables\n",
    "Table Data Base\n",
    "\n",
    "#TF_File=\"TF-\"\n",
    "\n",
    "#TF_Full=\"TF-Full.csv\"\n",
    "\n",
    "​\n",
    "\n",
    "XML Data Base\n",
    "\n",
    "​\n",
    "\n",
    "Configuration session\n",
    "\n",
    "'''config = tf.ConfigProto(device_count={\"CPU\": 7},\n",
    "\n",
    "                        allow_soft_placement=True,\n",
    "\n",
    "                        inter_op_parallelism_threads=1,\n",
    "\n",
    "                        intra_op_parallelism_threads=1,\n",
    "\n",
    "                        use_per_session_threads=True)'''\n",
    "\n",
    "#import pprint\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL']='5'\n",
    "\n",
    "config = tf.ConfigProto(device_count={\"GPU\": 2,\"CPU\":32},\n",
    "\n",
    "                        allow_soft_placement=True,\n",
    "\n",
    "                        inter_op_parallelism_threads=32,\n",
    "\n",
    "                        intra_op_parallelism_threads=32,\n",
    "\n",
    "                        use_per_session_threads=True,\n",
    "\n",
    "                        log_device_placement=True)\n",
    "\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "​\n",
    "\n",
    "#pprint.pprint({ k:str(getattr(config,k)) for k in sorted(dir(config)) if not k.startswith('_')})\n",
    "\n",
    "​\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "AttributeError                            Traceback (most recent call last)\n",
    "<ipython-input-87-abe0311a44d7> in <module>()\n",
    "     15                         log_device_placement=True)\n",
    "     16 \n",
    "---> 17 pprint.pprint({ k:str(getattr(config,k)) for k in sorted(dir(config)) if not k.startswith('_')})\n",
    "\n",
    "<ipython-input-87-abe0311a44d7> in <dictcomp>(.0)\n",
    "     15                         log_device_placement=True)\n",
    "     16 \n",
    "---> 17 pprint.pprint({ k:str(getattr(config,k)) for k in sorted(dir(config)) if not k.startswith('_')})\n",
    "\n",
    "AttributeError: \n",
    "\n",
    "Generic function\n",
    "Write data to csv\n",
    "\n",
    "'''\n",
    "\n",
    "Write Excell sheet\n",
    "\n",
    "'''\n",
    "\n",
    "def save_file_to_database(data_rows,path_database,file_databbase,header_list):\n",
    "\n",
    "    import csv\n",
    "\n",
    "    outfile = open(path_database+file_databbase,'w')\n",
    "\n",
    "    writer=csv.writer(outfile)\n",
    "\n",
    "    #header_list=['uuid','paragraph','doc_id']\n",
    "\n",
    "    i=0\n",
    "\n",
    "    for line in data_rows:\n",
    "\n",
    "        row=[i,line,'paragraph no.'+str(i)]\n",
    "\n",
    "        if i==0:\n",
    "\n",
    "            \n",
    "\n",
    "            writer.writerow(header_list)\n",
    "\n",
    "            writer.writerow(row)\n",
    "\n",
    "        else:\n",
    "\n",
    "            #print('ff')\n",
    "\n",
    "            writer.writerow(row)\n",
    "\n",
    "        i+= 1\n",
    "\n",
    "        #outfile.close()\n",
    "\n",
    "            \n",
    "\n",
    "Read data from csv\n",
    "\n",
    "'''\n",
    "\n",
    "Read Excell sheet\n",
    "\n",
    "'''\n",
    "\n",
    "def read_text_from_database(path_database,file_databbase):\n",
    "\n",
    "    import csv\n",
    "\n",
    "    queue_paragraph=[]\n",
    "\n",
    "    #f = open(sys.argv[1], 'rt')\n",
    "\n",
    "    outfile = open(path_database+file_databbase,'rt')\n",
    "\n",
    "    try:\n",
    "\n",
    "                \n",
    "\n",
    "        reader=csv.reader(outfile)\n",
    "\n",
    "        for row in reader:\n",
    "\n",
    "            queue_paragraph.append(row)\n",
    "\n",
    "            #print (row)\n",
    "\n",
    "    finally:\n",
    "\n",
    "        print (\"row\")\n",
    "\n",
    "        outfile.close()\n",
    "\n",
    "        \n",
    "\n",
    "    return queue_paragraph\n",
    "\n",
    "    \n",
    "\n",
    "def read_cvs_by_pands(path_database,file_databbase,index_col, header):\n",
    "\n",
    "    import csv\n",
    "\n",
    "    return pd.read_csv(path_database+file_databbase,index_col=index_col,header=header)\n",
    "\n",
    "​\n",
    "\n",
    "#read_cvs_by_pands(path_database,paragraph_table,index_col=0,header=0)\n",
    "\n",
    "def write_cvs_by_pands(path_database,file_databbase,header,data_rows):\n",
    "\n",
    "    import csv\n",
    "\n",
    "    csv_df=pd.DataFrame(data_rows,columns=header ) \n",
    "\n",
    "    csv_df.to_csv(path_database+file_databbase)\n",
    "\n",
    "​\n",
    "\n",
    "    \n",
    "\n",
    "​\n",
    "\n",
    "#write_cvs_by_pands(path_database,sentences_paragraph_table,sentences_paragraph_list,sales)\n",
    "\n",
    "#save pragraphs to files\n",
    "\n",
    "def write_file(pragraph,num_pragraph,path):\n",
    "\n",
    "    file = open(path+str(num_pragraph)+\".txt\",\"w\") \n",
    "\n",
    " \n",
    "\n",
    "    file.write(pragraph) \n",
    "\n",
    "    \n",
    "\n",
    "    file.close() \n",
    "\n",
    "    \n",
    "\n",
    "#create sub dataset\n",
    "\n",
    "def sub_dataset(path_data_source,data_source):\n",
    "\n",
    "    pragraphs=txt_pragraphs(read_file(path_data_source+data_source))\n",
    "\n",
    "    counter=0\n",
    "\n",
    "    for pragraph in pragraphs:\n",
    "\n",
    "        print('pragraph no ',counter)\n",
    "\n",
    "        write_file(pragraph,counter,sub_path_data_source)\n",
    "\n",
    "        counter +=1\n",
    "\n",
    "    \n",
    "\n",
    "Pre processing pipeline\n",
    "Read Huge File\n",
    "\n",
    "def read_file(str):\n",
    "\n",
    "    file = open(str,'r')\n",
    "\n",
    "    txt=file.read()\n",
    "\n",
    "    #print(txt)\n",
    "\n",
    "    return txt\n",
    "\n",
    "Split Document to pragraphs\n",
    "\n",
    "def txt_pragraphs(str):\n",
    "\n",
    "    pragraphs = str.split(\"\\n\\n\")\n",
    "\n",
    "    return pragraphs\n",
    "\n",
    "#pragraphs=txt_pragraphs(txt)\n",
    "\n",
    "#type(pragraphs)\n",
    "\n",
    "Split Paragraph to Sentences\n",
    "\n",
    "def pragraph_to_setnences(str):\n",
    "\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "    return sent_tokenize(str)\n",
    "\n",
    "#setnences=pragraph_to_setnences(pragraphs[n_pragraph])\n",
    "\n",
    "Word Process For Sentence\n",
    "Removing English stopwords and Punct per Sentence\n",
    "\n",
    "new_stop_words = ['the', 'that', 'to', 'as', 'there', 'has', 'and', 'or', 'is', 'not', 'a', 'of', 'but', 'in', 'by', 'on', 'are', 'it', 'if','what','where','how','when']\n",
    "\n",
    "new_stop_words2=['--','i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now','even','until','then','must']\n",
    "\n",
    "numbers=[1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "#stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "def remove_stopword_sentences(str):\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "    \n",
    "\n",
    "    tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "\n",
    "    \n",
    "\n",
    "    words=tokenizer.tokenize(str)\n",
    "\n",
    "    \n",
    "\n",
    "    english_stops = set(stopwords.words('english'))\n",
    "\n",
    "    stems=[]\n",
    "\n",
    "    list_word=[word for word in words if word.lower() not in english_stops and word.lower() not in new_stop_words and word.lower() not in new_stop_words2 and  not word.lower().isdigit() and word.lower() not in digits and word.lower() not in  numbers]\n",
    "\n",
    "    \n",
    "\n",
    "    #for word in list_word:\n",
    "\n",
    "        #stems.append(stem(word))\n",
    "\n",
    "        #stems.append(PorterStemmer().stem(word))\n",
    "\n",
    "        #stems.append(stemmer.stem(word))\n",
    "\n",
    "        #stems.append(stemmer.stem(\"computer\"))\n",
    "\n",
    "        #stems.append(word)\n",
    "\n",
    "    \n",
    "\n",
    "    return list_word#stems#(stem(setem_word for setem_word in  ([word for word in words if word not in english_stops and word not in new_stop_words])))#paragraph_list=read_text_from_database(path_database,paragraph_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_list_sentece(pragraph):\n",
    "    words_list=[]\n",
    "    setnences=pragraph_to_setnences(pragraph)\n",
    "    for indexs in range(len(setnences)):    \n",
    "        #print(\"Sentence No. \",indexs,\": \",setnences[indexs],\"\\n\")\n",
    "        words=remove_stopword_sentences(setnences[indexs])\n",
    "        wordsent=''\n",
    "        for index in range(len(words)):\n",
    "            wordsent+=' '+words[index]\n",
    "            #print(\"wordsent:\",wordsent)\n",
    "            \n",
    "        words_list.append(wordsent)\n",
    "        #count = Counter(words)\n",
    "        #print(\"wordsent:\",wordsent)\n",
    "        #print(\" word:\",words)\n",
    "    print(words_list)\n",
    "    return words_list\n",
    "\n",
    "#corpus=word_list_sentece(pragraphs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Sense Disambiguation (WSD): LESK per Sentence\n",
    "\n",
    "Given an ambiguous word and the context in which the word occurs, Lesk returns a Synset with the highest number of overlapping words between the context sentence and different definitions from each Synset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "this function for compute lesk for each word(list of word) in sentence\n",
    "'''\n",
    "def lesk_words_sentence(words,sentence):\n",
    "    from nltk.wsd import lesk\n",
    "    lesks= []\n",
    "    for word in words:\n",
    "        if lesk(sentence,word, 'n') is not None:\n",
    "            lesks.append(lesk(sentence,word, 'n'))\n",
    "            #print(\"Word is: \",word,\"\\n LESK: \",lesk(sentence,word, 'n'),\"\\n Sentence: \",sentence )\n",
    "        \n",
    "    return lesks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "this function for compute lesk of word in sentence\n",
    "'''\n",
    "\n",
    "def lesk_word_sentence(sentence,word):\n",
    "    \n",
    "    from nltk.wsd import lesk\n",
    "    lesk_synset=''\n",
    "    #lesks= []\n",
    "    #for word in words:\n",
    "    #disambiguated=lesk(context_sentence=sentence, ambiguous_word=word)\n",
    "    disambiguated=lesk(sentence,word, 'n')\n",
    "    #print(disambiguated)\n",
    "    #if disambiguated is not None:\n",
    "    lesk_synset=disambiguated\n",
    "    #else:\n",
    "    #lesk_synset=0\n",
    "    #print(\"Word is: \",word,\"\\n LESK: \",lesk(sentence,word, 'n'),\"\\n Sentence: \",sentence )\n",
    "        \n",
    "    return lesk_synset\n",
    "\n",
    "#lesk(\"Computer science is a discipline that spans theory and practice\",\"science\")\n",
    "\n",
    "#sent = 'people should be able to marry a person of their choice'.split()\n",
    "#lesk(sent, 'able')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_row_csv(path_database,idf,list_data):\n",
    "    import csv\n",
    "    with open(path_database+idf, 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(list_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name_file(full_name_path):\n",
    "    d=full_name_path.split(\"/\")\n",
    "    print(d)\n",
    "    name=d[len(d)-1].split(\".\")\n",
    "    return name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-6639d6c9ccdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mindex_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'file_names' is not defined"
     ]
    }
   ],
   "source": [
    "#with tf.control_dependencies(paragraph_list):\n",
    "def MyLoop(coord,sess,filename):\n",
    "    import csv\n",
    "    \n",
    "    import nltk\n",
    "    from nltk.wsd import lesk\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    from collections import Counter\n",
    "    \n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    from nltk import PorterStemmer\n",
    "    #from nltk.stem.porter import *\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "    from string import digits\n",
    "    with tf.device(\"/gpu\"):\n",
    "        \n",
    "        path_database=dir_path+\"/data/database/csv/\"\n",
    "        path_sub_tfidf=\"sub_tfidf/\"\n",
    "        path_full_tfidf=\"full_tfidf/\"\n",
    "        path_tf=\"sub_tf/\"\n",
    "        TF_File=\"TF-\"\n",
    "        TF_Full=\"TF-Full.csv\"\n",
    "                \n",
    "#with tf.Session(config=config) as sess:\n",
    "        #index_paragraph=0\n",
    "        col=1\n",
    "                \n",
    "        index_file=0\n",
    "        #sess.run(tf.global_variables_initializer())\n",
    "        #print(file_names)\n",
    "        \n",
    "        #for filename in file_names:\n",
    "            #print(\"index_file\",str(index_file))\n",
    "        word_file_fatma=[]\n",
    "        with open(filename) as inf:\n",
    "                #print(\"tpe\",type(inf))\n",
    "            txt=inf.read()\n",
    "                \n",
    "            paragraph_list=txt_pragraphs(txt)   \n",
    "        \n",
    "        \n",
    "            for paragraph in paragraph_list: #get pragraphs(documents) from DB\n",
    "                    #print(\"Pragraph type \",type(paragraph))\n",
    "                   \n",
    "                    \n",
    "                #if index_paragraph ==0:\n",
    "                    #index_paragraph += 1\n",
    "                #else:\n",
    "                        \n",
    "                setnences=pragraph_to_setnences(paragraph)#partitions paragraph to sentence\n",
    "                        \n",
    "                        \n",
    "                for setnence in setnences:\n",
    "                            #print(\"  \",setnence)                            \n",
    "\n",
    "                    words=remove_stopword_sentences(setnence)#remove stop words and noise\n",
    "\n",
    "                    for word in words:\n",
    "\n",
    "                                \n",
    "                        lesk=lesk_word_sentence(setnence,word)#get LESK of word in sentence\n",
    "\n",
    "                                #paragraph_word.append(word_sentence)\n",
    "\n",
    "                        if lesk is not None:\n",
    "                                    \n",
    "                                   \n",
    "                            word_file_fatma.append(lesk.name())\n",
    "                                \n",
    "\n",
    "                            \n",
    "                    '''////////////////END Sentence////////////////# '''\n",
    "                            \n",
    "                        \n",
    "                #write_cvs_by_pands(path_database,word_sentences_table,word_sentences_list,word_sentences_list_data)\n",
    "\n",
    "                       \n",
    "                '''////////////////END PARAGRAPH////////////////# '''\n",
    "\n",
    "        #write_cvs_by_pands(path_database,sentences_paragraph_table,sentences_paragraph_list,sentences_paragraph_list_data)\n",
    "\n",
    "        #print(word_file_fatma)\n",
    "                \n",
    "        word_file_Freq=Counter(word_file_fatma)\n",
    "        sum_count=sum(word_file_Freq.values())\n",
    "\n",
    "        #print(type(word_file_Freq))\n",
    "        #print(word_file_Freq)\n",
    "        #csv_df=pd.DataFrame([word_file_Freq],columns=word_file_Freq.keys() ) \n",
    "        freq=[]\n",
    "        for i in word_file_Freq.values():\n",
    "            c=i/sum_count\n",
    "            freq.append(c)\n",
    "        csv_df=pd.DataFrame([freq],columns=word_file_Freq.keys() ) \n",
    "                \n",
    "                \n",
    "        csv_df.to_csv(path_database+path_tf+TF_File+name_file(filename)+\".csv\")\n",
    "            # add to idf file \n",
    "                \n",
    "        full_list=[]\n",
    "        full_list.insert(0,name_file(filename))\n",
    "        full_list=full_list+list(word_file_Freq.keys())\n",
    "        # add to single\n",
    "                \n",
    "        add_row_csv(path_database+path_sub_tfidf,name_file(filename)+\".csv\",full_list)\n",
    "        # add to total idf file \n",
    "        add_row_csv(path_database+path_full_tfidf,TF_Full,full_list)\n",
    "        index_file +=1\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "from tensorflow.python.client import timeline\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    threads=[]\n",
    "    index=1\n",
    "    print(\"in session\",len(file_names))\n",
    "    for filename in file_names:\n",
    "        # Coordinate the loading of image files.\n",
    "        coord = tf.train.Coordinator()\n",
    "        thread = threading.Thread(target=MyLoop, args=(coord,sess,filename))\n",
    "        # thread.start()\n",
    "        threads.append(thread)\n",
    "        print(\"Thread No.\",index)\n",
    "        index +=1\n",
    "\n",
    "\n",
    "\n",
    "    for t in threads: t.start()\n",
    "    coord.join(threads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
