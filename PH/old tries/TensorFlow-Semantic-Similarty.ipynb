{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/python3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "#path_data_source=dir_path+\"/data/database/csv/\"\n",
    "path_data_source=\"/home/fsg/Desktop/csv/\"\n",
    "\n",
    "sub_path_data_source_tfidf=\"sub_tfidf/\"\n",
    "sub_path_data_source_sim=\"semantics/sim/\"\n",
    "\n",
    "file_path_tf_idf=path_data_source+sub_path_data_source_tfidf\n",
    "\n",
    "file_path_sim=path_data_source+sub_path_data_source_sim\n",
    "\n",
    "\n",
    "\n",
    "file_names_tf_idf = [os.path.join(file_path_tf_idf, f) \n",
    "                      for f in os.listdir(file_path_tf_idf) \n",
    "                      if f.endswith(\".csv\")]\n",
    "\n",
    "file_names_sim = [os.path.join(file_path_sim, f) \n",
    "                      for f in os.listdir(file_path_sim) \n",
    "                      if f.endswith(\".csv\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic as wnic\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def similarity_by_infocontent(sense1, sense2, option):\n",
    "    #sense1=\"Synset('\"+sense1+\"')\"\n",
    "    #sense2=\"Synset('\"+sense2+\"')\"\n",
    "    #print(sense1,sense2)\n",
    "    sense1 = wn.synset(sense1)\n",
    "    sense2 = wn.synset(sense2)\n",
    "    #print(sense1,sense2)\n",
    "    \"\"\" Returns similarity scores by information content. \"\"\"\n",
    "    #if sense1.pos != sense2.pos: # infocontent sim can't do diff POS.\n",
    "        #return 0\n",
    "\n",
    "    info_contents = ['ic-bnc-add1.dat', 'ic-bnc-resnik-add1.dat', \n",
    "                     'ic-bnc-resnik.dat', 'ic-bnc.dat', \n",
    "\n",
    "                     'ic-brown-add1.dat', 'ic-brown-resnik-add1.dat', \n",
    "                     'ic-brown-resnik.dat', 'ic-brown.dat', \n",
    "\n",
    "                     'ic-semcor-add1.dat', 'ic-semcor.dat',\n",
    "\n",
    "                     'ic-semcorraw-add1.dat', 'ic-semcorraw-resnik-add1.dat', \n",
    "                     'ic-semcorraw-resnik.dat', 'ic-semcorraw.dat', \n",
    "\n",
    "                     'ic-shaks-add1.dat', 'ic-shaks-resnik.dat', \n",
    "                     'ic-shaks-resnink-add1.dat', 'ic-shaks.dat', \n",
    "\n",
    "                     'ic-treebank-add1.dat', 'ic-treebank-resnik-add1.dat', \n",
    "                     'ic-treebank-resnik.dat', 'ic-treebank.dat']\n",
    "\n",
    "    if option in ['res', 'resnik']:\n",
    "        #return wn.res_similarity(sense1, sense2, wnic.ic('ic-bnc-resnik-add1.dat'))\n",
    "        #print('simRe snik (c1,c2) = -log p(lso(c1,c2)) = IC(lso(c1,c2)')\n",
    "        return wn.res_similarity(sense1, sense2, wnic.ic('ic-treebank-resnik-add1.dat'))\n",
    "    #return min(wn.res_similarity(sense1, sense2, wnic.ic(ic)) \\\n",
    "    #             for ic in info_contents)\n",
    "\n",
    "    elif option in ['jcn', \"jiang-conrath\"]:\n",
    "        #return wn.jcn_similarity(sense1, sense2, wnic.ic('ic-bnc-add1.dat'))\n",
    "        #print('sim(jcn) (c1,c2 )= (IC(c1) + IC(c2 )) - 2IC(lso(c1,c2 ))')\n",
    "        return wn.jcn_similarity(sense1, sense2, wnic.ic('ic-treebank.dat'))\n",
    "\n",
    "    elif option in ['lin']:\n",
    "        #return wn.lin_similarity(sense1, sense2, wnic.ic('ic-bnc-add1.dat'))\n",
    "        #print('sim(lin) (c1,c2)=(2IC(lso(c1,c2 )))/(IC(c1)+IC(c2))')\n",
    "        return wn.lin_similarity(sense1, sense2, wnic.ic('ic-treebank.dat'))\n",
    "\n",
    "def sim(sense1, sense2, option=\"path\"):\n",
    "    \"\"\" Calculates similarity based on user's choice. \"\"\"\n",
    "    option = option.lower()\n",
    "    if option.lower() in [\"path\", \"path_similarity\", \n",
    "                        \"wup\", \"wupa\", \"wu-palmer\", \"wu-palmer\",\n",
    "                        'lch', \"leacock-chordorow\"]:\n",
    "        return similarity_by_path(sense1, sense2, option) \n",
    "    elif option.lower() in [\"res\", \"resnik\",\n",
    "                          \"jcn\",\"jiang-conrath\",\n",
    "                          \"lin\"]:\n",
    "        return similarity_by_infocontent(sense1, sense2, option)\n",
    "\n",
    "def max_similarity(context_sentence, ambiguous_word, option=\"path\", \n",
    "                   pos=None, best=True):\n",
    "    \"\"\"\n",
    "    Perform WSD by maximizing the sum of maximum similarity between possible \n",
    "    synsets of all words in the context sentence and the possible synsets of the \n",
    "    ambiguous words (see http://goo.gl/XMq2BI):\n",
    "    {argmax}_{synset(a)}(\\sum_{i}^{n}{{max}_{synset(i)}(sim(i,a))}\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for i in wn.synsets(ambiguous_word):\n",
    "        try:\n",
    "            if pos and pos != str(i.pos()):\n",
    "                continue\n",
    "        except:\n",
    "            if pos and pos != str(i.pos):\n",
    "                continue\n",
    "        result[i] = sum(max([sim(i,k,option) for k in wn.synsets(j)]+[0]) \\\n",
    "                        for j in word_tokenize(context_sentence))\n",
    "\n",
    "    if option in [\"res\",\"resnik\"]: # lower score = more similar\n",
    "        result = sorted([(v,k) for k,v in result.items()])\n",
    "    else: # higher score = more similar\n",
    "        result = sorted([(v,k) for k,v in result.items()],reverse=True)\n",
    "    #print (result)\n",
    "    if best: return result[0][1];\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.1697386575732045"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_by_infocontent('acquiring.n.01', 'acquiring.n.01', 'res')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_cvs_by_pands(path_data_source,file_databbase,index_col, header):\n",
    "    import pandas as pd\n",
    "    return pd.read_csv(path_data_source+file_databbase,index_col=index_col,header=header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_file_exist(file_path,file_name):\n",
    "    file_names = [os.path.join(file_path, f) \n",
    "                      for f in os.listdir(file_path) \n",
    "                      if f.endswith(\".csv\")]\n",
    "    #print(file_names)\n",
    "    if file_name in file_names:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#is_file_exist(file_path_sim,file_path_sim+\"fd.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_row_csv(path_data_source,file_name,list_data):\n",
    "    import csv\n",
    "    with open(path_data_source+file_name, 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(list_data)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_file(path_data_source,file_name):\n",
    "    import csv\n",
    "    outfile = open(path_data_source+file_name,'w')\n",
    "    writer=csv.writer(outfile)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create_file(file_path_sim,\"ff.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrm ace.n.03 term_next ace.n.03\n",
      "terrm ace.n.03 term_next acquiring.n.01\n",
      "terrm ace.n.03 term_next building.n.04\n",
      "terrm ace.n.03 term_next business.n.04\n",
      "terrm ace.n.03 term_next care.n.04\n",
      "finesed\n",
      "terrm acquiring.n.01 term_next acquiring.n.01\n",
      "terrm acquiring.n.01 term_next building.n.04\n",
      "terrm acquiring.n.01 term_next business.n.04\n",
      "terrm acquiring.n.01 term_next care.n.04\n",
      "finesed\n",
      "terrm building.n.04 term_next building.n.04\n",
      "terrm building.n.04 term_next business.n.04\n",
      "terrm building.n.04 term_next care.n.04\n",
      "finesed\n",
      "terrm business.n.04 term_next business.n.04\n",
      "terrm business.n.04 term_next care.n.04\n",
      "finesed\n",
      "terrm care.n.04 term_next care.n.04\n",
      "term == term_next care.n.04 care.n.04 10.690011115357335 ['care.n.04', 10.690011115357335]\n",
      "            is_term_new care.n.04 10.690011115357335 ['care.n.04', 10.690011115357335]\n",
      "finesed\n"
     ]
    }
   ],
   "source": [
    "def sim_terms_one_file(path_data_source,sub_path_data_source_tfidf,file_name):\n",
    "    list_terms=read_cvs_by_pands(path_data_source,sub_path_data_source_tfidf+file_name,0,0).index\n",
    "    #compare between the same file\n",
    "    #print(list_terms)\n",
    "    #index=1\n",
    "    limt=5\n",
    "    for i in range(len(list_terms[:limt])):\n",
    "        term=list_terms[i]\n",
    "        is_term_new=False\n",
    "        if not is_file_exist(file_path_sim,file_path_sim+term+\".csv\"): # if !Fale this new term\n",
    "                #print(\" not term\",term)\n",
    "                #create_file(file_path_sim,term+\".csv\")  \n",
    "                is_term_new=True\n",
    "\n",
    "        for term_next in list_terms[i:limt]:\n",
    "            print(\"terrm\",term,\"term_next\",term_next)\n",
    "\n",
    "            is_term_next_new=False\n",
    "\n",
    "\n",
    "\n",
    "            if not is_file_exist(file_path_sim,file_path_sim+term_next+\".csv\"): #next term is new\n",
    "                    #print(\" not termin\",list_terms[index])\n",
    "                    #create_file(file_path_sim,term_next+\".csv\")\n",
    "                    is_term_next_new=True\n",
    "\n",
    "            #print(term,is_term_new,term_next,is_term_next_new)\n",
    "            sim=0\n",
    "            list_term=[]\n",
    "            list_term_next=[]\n",
    "            if is_term_new or is_term_next_new:\n",
    "                sim=similarity_by_infocontent(term, term_next, 'res')\n",
    "                #print(term,term_next,\"sim\",sim)\n",
    "                if sim <1:\n",
    "                    sim=0\n",
    "                if sim !=0:\n",
    "                    list_term=[term,sim]\n",
    "                    list_term_next=[term_next,sim]\n",
    "\n",
    "            #print(list_term)\n",
    "            #print(list_term_next)\n",
    "            if sim !=0:\n",
    "                if term != term_next:\n",
    "                    print(\"term != term_next\")\n",
    "                    if is_term_new:  \n",
    "                        print(\"             is_term_new\",term)\n",
    "                        create_file(file_path_sim,term+\".csv\")\n",
    "                        add_row_csv(file_path_sim,term+\".csv\",list_term)#add the same sim\n",
    "                        add_row_csv(file_path_sim,term+\".csv\",list_term_next)#add the next sim\n",
    "                        is_term_new=False\n",
    "                    else:\n",
    "                        print(\"             is_term_old\",term)\n",
    "                        add_row_csv(file_path_sim,term+\".csv\",list_term_next)#add the next sim\n",
    "\n",
    "\n",
    "                    if is_term_next_new:\n",
    "                        print(\"             is_term_next_new\",term_next)\n",
    "\n",
    "                        sim_nex=similarity_by_infocontent(term_next, term_next, 'res')\n",
    "                        create_file(file_path_sim,term_next+\".csv\")\n",
    "                        list_next=[term_next,sim_nex]\n",
    "                        add_row_csv(file_path_sim,term_next+\".csv\",list_next)#add term to next\n",
    "                        add_row_csv(file_path_sim,term_next+\".csv\",list_term)#add term to next\n",
    "                        is_term_next_new=False\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        print(\"              is_term_next_old\",term_next,sim)\n",
    "                        add_row_csv(file_path_sim,term_next+\".csv\",list_term)#add term to next\n",
    "                else:\n",
    "                    print(\"term == term_next\",term,term_next,sim,list_term)\n",
    "                    if is_term_new:  \n",
    "                        print(\"            is_term_new\",term,sim,list_term)\n",
    "                        create_file(file_path_sim,term+\".csv\")\n",
    "                        add_row_csv(file_path_sim,term+\".csv\",list_term)#add the same sim\n",
    "                        is_term_new=False\n",
    "\n",
    "\n",
    "        print(\"finesed\")\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "   \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
