{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv   \n",
    "def add_row(row,path_database,file_name):\n",
    "    #fields=['first','second','third']\n",
    "    with open(path_database+file_name, 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(row)\n",
    "        print(\"printed\")\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header_list=['index','text']\n",
    "add_row(header_list,'./',\"R_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters \n",
    " \n",
    "def collection_stats():\n",
    "    # List of documents\n",
    "    documents = reuters.fileids()\n",
    "    #print(str(len(documents)) + \" documents\");\n",
    " \n",
    "    #train_docs = list(filter(lambda doc: doc.startswith(\"train\"),documents));\n",
    "    #print(str(len(train_docs)) + \" total train documents\");\n",
    " \n",
    "    #test_docs = list(filter(lambda doc: doc.startswith(\"test\"),documents));\n",
    "    #print(str(len(test_docs)) + \" total test documents\");\n",
    " \n",
    "    # List of categories\n",
    "    categories = reuters.categories();\n",
    "    #print(str(len(categories)) + \" categories\");\n",
    "    index=0\n",
    "    paragraph_list=[]\n",
    "    for i in range (1):#len(categories)):\n",
    "        category_docs = reuters.fileids(categories[i]);\n",
    "        for x in range (2):#len(category_docs)):\n",
    "            row=[]\n",
    "            document_id = category_docs[x]\n",
    "            row.append(index)\n",
    "            doc=reuters.raw(document_id)\n",
    "            row.append(doc)\n",
    "            add_row(row,'./',\"R_text.csv\")\n",
    "            index =+1\n",
    "            \n",
    "            \n",
    "        \n",
    " \n",
    "    # Documents in a category\n",
    "    #category_docs = reuters.fileids(\"acq\");\n",
    " \n",
    "    # Words for a document\n",
    "    #document_id = category_docs[0]\n",
    "    #document_words = reuters.words(category_docs[0]);\n",
    "    #print(document_words);  \n",
    " \n",
    "    # Raw document\n",
    "    #print(reuters.raw(document_id));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters \n",
    "def collection_stats():\n",
    "    # List of documents\n",
    "    documents = reuters.fileids()\n",
    "   \n",
    "    categories = reuters.categories();\n",
    "    index=0\n",
    "    category_docs = reuters.fileids('crude');\n",
    "    for x in range (3):#len(category_docs)):\n",
    "            row=[]\n",
    "            document_id = category_docs[x]\n",
    "            row.append(index)\n",
    "            doc=reuters.raw(document_id)\n",
    "            row.append(doc)\n",
    "            add_row(row,'./',\"R_text.csv\")\n",
    "            index +=1\n",
    "            print(row)\n",
    "            print(\"------------\")\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters \n",
    "documents = reuters.fileids()\n",
    "#documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acq 2369\n",
      "alum 58\n",
      "barley 51\n",
      "bop 105\n",
      "carcass 68\n",
      "castor-oil 2\n",
      "cocoa 73\n",
      "coconut 6\n",
      "coconut-oil 7\n",
      "coffee 139\n",
      "copper 65\n",
      "copra-cake 3\n",
      "corn 237\n",
      "cotton 59\n",
      "cotton-oil 3\n",
      "cpi 97\n",
      "cpu 4\n",
      "crude 578\n",
      "dfl 3\n",
      "dlr 175\n",
      "dmk 14\n",
      "earn 3964\n",
      "fuel 23\n",
      "gas 54\n",
      "gnp 136\n",
      "gold 124\n",
      "grain 582\n",
      "groundnut 9\n",
      "groundnut-oil 2\n",
      "heat 19\n",
      "hog 22\n",
      "housing 20\n",
      "income 16\n",
      "instal-debt 6\n",
      "interest 478\n",
      "ipi 53\n",
      "iron-steel 54\n",
      "jet 5\n",
      "jobs 67\n",
      "l-cattle 8\n",
      "lead 29\n",
      "lei 15\n",
      "lin-oil 2\n",
      "livestock 99\n",
      "lumber 16\n",
      "meal-feed 49\n",
      "money-fx 717\n",
      "money-supply 174\n",
      "naphtha 6\n",
      "nat-gas 105\n",
      "nickel 9\n",
      "nkr 3\n",
      "nzdlr 4\n",
      "oat 14\n",
      "oilseed 171\n",
      "orange 27\n",
      "palladium 3\n",
      "palm-oil 40\n",
      "palmkernel 3\n",
      "pet-chem 32\n",
      "platinum 12\n",
      "potato 6\n",
      "propane 6\n",
      "rand 3\n",
      "rape-oil 8\n",
      "rapeseed 27\n",
      "reserves 73\n",
      "retail 25\n",
      "rice 59\n",
      "rubber 49\n",
      "rye 2\n",
      "ship 286\n",
      "silver 29\n",
      "sorghum 34\n",
      "soy-meal 26\n",
      "soy-oil 25\n",
      "soybean 111\n",
      "strategic-metal 27\n",
      "sugar 162\n",
      "sun-meal 2\n",
      "sun-oil 7\n",
      "sunseed 16\n",
      "tea 13\n",
      "tin 30\n",
      "trade 485\n",
      "veg-oil 124\n",
      "wheat 283\n",
      "wpi 29\n",
      "yen 59\n",
      "zinc 34\n"
     ]
    }
   ],
   "source": [
    "categories = reuters.categories();\n",
    "#print(categories );\n",
    "for i in range (len(categories)):\n",
    "        category_docs = reuters.fileids(categories[i]);\n",
    "        print(categories[i],len(category_docs) );\n",
    "        #for x in range(len(category_docs)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "category_docs = reuters.fileids(\"cpu\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "category_docs\n",
    " # Words for a document\n",
    "document_id = category_docs[0]\n",
    "document_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "582\n",
      "CFTC APPROVES MGE CORN SYRUP FUTURES CONTRACT\n",
      "  The Commodity Futures Trading\n",
      "  Commission has approved the Minneapolis Grain Exchange's\n",
      "  application to trade high fructose corn syrup-55, HFCS-55,\n",
      "  futures contracts, the commission said.\n",
      "      The contract provides for the delivery of 48,000 lbs, plus\n",
      "  or minus two pct, of bulk HFCS-55 meeting specified standards\n",
      "  regarding its physical and chemical properties.\n",
      "      CFTC said the exchange plans to begin trading a July 1987\n",
      "  HFCS-55 contract on April 6.\n",
      "      CFTC said the soft drink industry currently buys at least\n",
      "  95 pct of all U.S.-produced HFCS-55, a liquid food and beverage\n",
      "  sweetener produced through the processing of corn starch by\n",
      "  corn refiners.\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documents = reuters.fileids('grain')\n",
    "test = [d for d in documents if d.startswith('test/')]\n",
    "train = [d for d in documents if d.startswith('training/')]\n",
    "data = [d for d in documents]\n",
    "train[400]\n",
    "r=[]\n",
    "for s in range(len(data)):\n",
    "    doc=reuters.raw(data[s])\n",
    "    r.append(doc)\n",
    "print(len(r))\n",
    "print(r[400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters \n",
    " \n",
    "def ss():\n",
    "    documents = reuters.fileids('grain')\n",
    "    \n",
    "    data = [d for d in documents]\n",
    "   \n",
    "    \n",
    "    for s in range(len(data)):\n",
    "        r=[]\n",
    "        doc=reuters.raw(data[s])\n",
    "        r.append(s)\n",
    "        r.append(doc)\n",
    "        add_row(r,'./','pragraph_index_reuters3.csv')\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printed\n",
      "printed\n",
      "0\n",
      "printed\n",
      "1\n",
      "printed\n",
      "2\n",
      "printed\n",
      "3\n",
      "printed\n",
      "4\n",
      "printed\n",
      "5\n",
      "printed\n",
      "6\n",
      "printed\n",
      "7\n",
      "printed\n",
      "8\n",
      "printed\n",
      "9\n",
      "printed\n",
      "10\n",
      "printed\n",
      "11\n",
      "printed\n",
      "12\n",
      "printed\n",
      "13\n",
      "printed\n",
      "14\n",
      "printed\n",
      "15\n",
      "printed\n",
      "16\n",
      "printed\n",
      "17\n",
      "printed\n",
      "18\n",
      "printed\n",
      "19\n",
      "printed\n",
      "20\n",
      "printed\n",
      "21\n",
      "printed\n",
      "22\n",
      "printed\n",
      "23\n",
      "printed\n",
      "24\n",
      "printed\n",
      "25\n",
      "printed\n",
      "26\n",
      "printed\n",
      "27\n",
      "printed\n",
      "28\n",
      "printed\n",
      "29\n",
      "printed\n",
      "30\n",
      "printed\n",
      "31\n",
      "printed\n",
      "32\n",
      "printed\n",
      "33\n",
      "printed\n",
      "34\n",
      "printed\n",
      "35\n",
      "printed\n",
      "36\n",
      "printed\n",
      "37\n",
      "printed\n",
      "38\n",
      "printed\n",
      "39\n",
      "printed\n",
      "40\n",
      "printed\n",
      "41\n",
      "printed\n",
      "42\n",
      "printed\n",
      "43\n",
      "printed\n",
      "44\n",
      "printed\n",
      "45\n",
      "printed\n",
      "46\n",
      "printed\n",
      "47\n",
      "printed\n",
      "48\n",
      "printed\n",
      "49\n",
      "printed\n",
      "50\n",
      "printed\n",
      "51\n",
      "printed\n",
      "52\n",
      "printed\n",
      "53\n",
      "printed\n",
      "54\n",
      "printed\n",
      "55\n",
      "printed\n",
      "56\n",
      "printed\n",
      "57\n",
      "printed\n",
      "58\n",
      "printed\n",
      "59\n",
      "printed\n",
      "60\n",
      "printed\n",
      "61\n",
      "printed\n",
      "62\n",
      "printed\n",
      "63\n",
      "printed\n",
      "64\n",
      "printed\n",
      "65\n",
      "printed\n",
      "66\n",
      "printed\n",
      "67\n",
      "printed\n",
      "68\n",
      "printed\n",
      "69\n",
      "printed\n",
      "70\n",
      "printed\n",
      "71\n",
      "printed\n",
      "72\n",
      "printed\n",
      "73\n",
      "printed\n",
      "74\n",
      "printed\n",
      "75\n",
      "printed\n",
      "76\n",
      "printed\n",
      "77\n",
      "printed\n",
      "78\n",
      "printed\n",
      "79\n",
      "printed\n",
      "80\n",
      "printed\n",
      "81\n",
      "printed\n",
      "82\n",
      "printed\n",
      "83\n",
      "printed\n",
      "84\n",
      "printed\n",
      "85\n",
      "printed\n",
      "86\n",
      "printed\n",
      "87\n",
      "printed\n",
      "88\n",
      "printed\n",
      "89\n",
      "printed\n",
      "90\n",
      "printed\n",
      "91\n",
      "printed\n",
      "92\n",
      "printed\n",
      "93\n",
      "printed\n",
      "94\n",
      "printed\n",
      "95\n",
      "printed\n",
      "96\n",
      "printed\n",
      "97\n",
      "printed\n",
      "98\n",
      "printed\n",
      "99\n",
      "printed\n",
      "100\n",
      "printed\n",
      "101\n",
      "printed\n",
      "102\n",
      "printed\n",
      "103\n",
      "printed\n",
      "104\n",
      "printed\n",
      "105\n",
      "printed\n",
      "106\n",
      "printed\n",
      "107\n",
      "printed\n",
      "108\n",
      "printed\n",
      "109\n",
      "printed\n",
      "110\n",
      "printed\n",
      "111\n",
      "printed\n",
      "112\n",
      "printed\n",
      "113\n",
      "printed\n",
      "114\n",
      "printed\n",
      "115\n",
      "printed\n",
      "116\n",
      "printed\n",
      "117\n",
      "printed\n",
      "118\n",
      "printed\n",
      "119\n",
      "printed\n",
      "120\n",
      "printed\n",
      "121\n",
      "printed\n",
      "122\n",
      "printed\n",
      "123\n",
      "printed\n",
      "124\n",
      "printed\n",
      "125\n",
      "printed\n",
      "126\n",
      "printed\n",
      "127\n",
      "printed\n",
      "128\n",
      "printed\n",
      "129\n",
      "printed\n",
      "130\n",
      "printed\n",
      "131\n",
      "printed\n",
      "132\n",
      "printed\n",
      "133\n",
      "printed\n",
      "134\n",
      "printed\n",
      "135\n",
      "printed\n",
      "136\n",
      "printed\n",
      "137\n",
      "printed\n",
      "138\n",
      "printed\n",
      "139\n",
      "printed\n",
      "140\n",
      "printed\n",
      "141\n",
      "printed\n",
      "142\n",
      "printed\n",
      "143\n",
      "printed\n",
      "144\n",
      "printed\n",
      "145\n",
      "printed\n",
      "146\n",
      "printed\n",
      "147\n",
      "printed\n",
      "148\n",
      "printed\n",
      "149\n",
      "printed\n",
      "150\n",
      "printed\n",
      "151\n",
      "printed\n",
      "152\n",
      "printed\n",
      "153\n",
      "printed\n",
      "154\n",
      "printed\n",
      "155\n",
      "printed\n",
      "156\n",
      "printed\n",
      "157\n",
      "printed\n",
      "158\n",
      "printed\n",
      "159\n",
      "printed\n",
      "160\n",
      "printed\n",
      "161\n",
      "printed\n",
      "162\n",
      "printed\n",
      "163\n",
      "printed\n",
      "164\n",
      "printed\n",
      "165\n",
      "printed\n",
      "166\n",
      "printed\n",
      "167\n",
      "printed\n",
      "168\n",
      "printed\n",
      "169\n",
      "printed\n",
      "170\n",
      "printed\n",
      "171\n",
      "printed\n",
      "172\n",
      "printed\n",
      "173\n",
      "printed\n",
      "174\n",
      "printed\n",
      "175\n",
      "printed\n",
      "176\n",
      "printed\n",
      "177\n",
      "printed\n",
      "178\n",
      "printed\n",
      "179\n",
      "printed\n",
      "180\n",
      "printed\n",
      "181\n",
      "printed\n",
      "182\n",
      "printed\n",
      "183\n",
      "printed\n",
      "184\n",
      "printed\n",
      "185\n",
      "printed\n",
      "186\n",
      "printed\n",
      "187\n",
      "printed\n",
      "188\n",
      "printed\n",
      "189\n",
      "printed\n",
      "190\n",
      "printed\n",
      "191\n",
      "printed\n",
      "192\n",
      "printed\n",
      "193\n",
      "printed\n",
      "194\n",
      "printed\n",
      "195\n",
      "printed\n",
      "196\n",
      "printed\n",
      "197\n",
      "printed\n",
      "198\n",
      "printed\n",
      "199\n",
      "printed\n",
      "200\n",
      "printed\n",
      "201\n",
      "printed\n",
      "202\n",
      "printed\n",
      "203\n",
      "printed\n",
      "204\n",
      "printed\n",
      "205\n",
      "printed\n",
      "206\n",
      "printed\n",
      "207\n",
      "printed\n",
      "208\n",
      "printed\n",
      "209\n",
      "printed\n",
      "210\n",
      "printed\n",
      "211\n",
      "printed\n",
      "212\n",
      "printed\n",
      "213\n",
      "printed\n",
      "214\n",
      "printed\n",
      "215\n",
      "printed\n",
      "216\n",
      "printed\n",
      "217\n",
      "printed\n",
      "218\n",
      "printed\n",
      "219\n",
      "printed\n",
      "220\n",
      "printed\n",
      "221\n",
      "printed\n",
      "222\n",
      "printed\n",
      "223\n",
      "printed\n",
      "224\n",
      "printed\n",
      "225\n",
      "printed\n",
      "226\n",
      "printed\n",
      "227\n",
      "printed\n",
      "228\n",
      "printed\n",
      "229\n",
      "printed\n",
      "230\n",
      "printed\n",
      "231\n",
      "printed\n",
      "232\n",
      "printed\n",
      "233\n",
      "printed\n",
      "234\n",
      "printed\n",
      "235\n",
      "printed\n",
      "236\n",
      "printed\n",
      "237\n",
      "printed\n",
      "238\n",
      "printed\n",
      "239\n",
      "printed\n",
      "240\n",
      "printed\n",
      "241\n",
      "printed\n",
      "242\n",
      "printed\n",
      "243\n",
      "printed\n",
      "244\n",
      "printed\n",
      "245\n",
      "printed\n",
      "246\n",
      "printed\n",
      "247\n",
      "printed\n",
      "248\n",
      "printed\n",
      "249\n",
      "printed\n",
      "250\n",
      "printed\n",
      "251\n",
      "printed\n",
      "252\n",
      "printed\n",
      "253\n",
      "printed\n",
      "254\n",
      "printed\n",
      "255\n",
      "printed\n",
      "256\n",
      "printed\n",
      "257\n",
      "printed\n",
      "258\n",
      "printed\n",
      "259\n",
      "printed\n",
      "260\n",
      "printed\n",
      "261\n",
      "printed\n",
      "262\n",
      "printed\n",
      "263\n",
      "printed\n",
      "264\n",
      "printed\n",
      "265\n",
      "printed\n",
      "266\n",
      "printed\n",
      "267\n",
      "printed\n",
      "268\n",
      "printed\n",
      "269\n",
      "printed\n",
      "270\n",
      "printed\n",
      "271\n",
      "printed\n",
      "272\n",
      "printed\n",
      "273\n",
      "printed\n",
      "274\n",
      "printed\n",
      "275\n",
      "printed\n",
      "276\n",
      "printed\n",
      "277\n",
      "printed\n",
      "278\n",
      "printed\n",
      "279\n",
      "printed\n",
      "280\n",
      "printed\n",
      "281\n",
      "printed\n",
      "282\n",
      "printed\n",
      "283\n",
      "printed\n",
      "284\n",
      "printed\n",
      "285\n",
      "printed\n",
      "286\n",
      "printed\n",
      "287\n",
      "printed\n",
      "288\n",
      "printed\n",
      "289\n",
      "printed\n",
      "290\n",
      "printed\n",
      "291\n",
      "printed\n",
      "292\n",
      "printed\n",
      "293\n",
      "printed\n",
      "294\n",
      "printed\n",
      "295\n",
      "printed\n",
      "296\n",
      "printed\n",
      "297\n",
      "printed\n",
      "298\n",
      "printed\n",
      "299\n",
      "printed\n",
      "300\n",
      "printed\n",
      "301\n",
      "printed\n",
      "302\n",
      "printed\n",
      "303\n",
      "printed\n",
      "304\n",
      "printed\n",
      "305\n",
      "printed\n",
      "306\n",
      "printed\n",
      "307\n",
      "printed\n",
      "308\n",
      "printed\n",
      "309\n",
      "printed\n",
      "310\n",
      "printed\n",
      "311\n",
      "printed\n",
      "312\n",
      "printed\n",
      "313\n",
      "printed\n",
      "314\n",
      "printed\n",
      "315\n",
      "printed\n",
      "316\n",
      "printed\n",
      "317\n",
      "printed\n",
      "318\n",
      "printed\n",
      "319\n",
      "printed\n",
      "320\n",
      "printed\n",
      "321\n",
      "printed\n",
      "322\n",
      "printed\n",
      "323\n",
      "printed\n",
      "324\n",
      "printed\n",
      "325\n",
      "printed\n",
      "326\n",
      "printed\n",
      "327\n",
      "printed\n",
      "328\n",
      "printed\n",
      "329\n",
      "printed\n",
      "330\n",
      "printed\n",
      "331\n",
      "printed\n",
      "332\n",
      "printed\n",
      "333\n",
      "printed\n",
      "334\n",
      "printed\n",
      "335\n",
      "printed\n",
      "336\n",
      "printed\n",
      "337\n",
      "printed\n",
      "338\n",
      "printed\n",
      "339\n",
      "printed\n",
      "340\n",
      "printed\n",
      "341\n",
      "printed\n",
      "342\n",
      "printed\n",
      "343\n",
      "printed\n",
      "344\n",
      "printed\n",
      "345\n",
      "printed\n",
      "346\n",
      "printed\n",
      "347\n",
      "printed\n",
      "348\n",
      "printed\n",
      "349\n",
      "printed\n",
      "350\n",
      "printed\n",
      "351\n",
      "printed\n",
      "352\n",
      "printed\n",
      "353\n",
      "printed\n",
      "354\n",
      "printed\n",
      "355\n",
      "printed\n",
      "356\n",
      "printed\n",
      "357\n",
      "printed\n",
      "358\n",
      "printed\n",
      "359\n",
      "printed\n",
      "360\n",
      "printed\n",
      "361\n",
      "printed\n",
      "362\n",
      "printed\n",
      "363\n",
      "printed\n",
      "364\n",
      "printed\n",
      "365\n",
      "printed\n",
      "366\n",
      "printed\n",
      "367\n",
      "printed\n",
      "368\n",
      "printed\n",
      "369\n",
      "printed\n",
      "370\n",
      "printed\n",
      "371\n",
      "printed\n",
      "372\n",
      "printed\n",
      "373\n",
      "printed\n",
      "374\n",
      "printed\n",
      "375\n",
      "printed\n",
      "376\n",
      "printed\n",
      "377\n",
      "printed\n",
      "378\n",
      "printed\n",
      "379\n",
      "printed\n",
      "380\n",
      "printed\n",
      "381\n",
      "printed\n",
      "382\n",
      "printed\n",
      "383\n",
      "printed\n",
      "384\n",
      "printed\n",
      "385\n",
      "printed\n",
      "386\n",
      "printed\n",
      "387\n",
      "printed\n",
      "388\n",
      "printed\n",
      "389\n",
      "printed\n",
      "390\n",
      "printed\n",
      "391\n",
      "printed\n",
      "392\n",
      "printed\n",
      "393\n",
      "printed\n",
      "394\n",
      "printed\n",
      "395\n",
      "printed\n",
      "396\n",
      "printed\n",
      "397\n",
      "printed\n",
      "398\n",
      "printed\n",
      "399\n",
      "printed\n",
      "400\n",
      "printed\n",
      "401\n",
      "printed\n",
      "402\n",
      "printed\n",
      "403\n",
      "printed\n",
      "404\n",
      "printed\n",
      "405\n",
      "printed\n",
      "406\n",
      "printed\n",
      "407\n",
      "printed\n",
      "408\n",
      "printed\n",
      "409\n",
      "printed\n",
      "410\n",
      "printed\n",
      "411\n",
      "printed\n",
      "412\n",
      "printed\n",
      "413\n",
      "printed\n",
      "414\n",
      "printed\n",
      "415\n",
      "printed\n",
      "416\n",
      "printed\n",
      "417\n",
      "printed\n",
      "418\n",
      "printed\n",
      "419\n",
      "printed\n",
      "420\n",
      "printed\n",
      "421\n",
      "printed\n",
      "422\n",
      "printed\n",
      "423\n",
      "printed\n",
      "424\n",
      "printed\n",
      "425\n",
      "printed\n",
      "426\n",
      "printed\n",
      "427\n",
      "printed\n",
      "428\n",
      "printed\n",
      "429\n",
      "printed\n",
      "430\n",
      "printed\n",
      "431\n",
      "printed\n",
      "432\n",
      "printed\n",
      "433\n",
      "printed\n",
      "434\n",
      "printed\n",
      "435\n",
      "printed\n",
      "436\n",
      "printed\n",
      "437\n",
      "printed\n",
      "438\n",
      "printed\n",
      "439\n",
      "printed\n",
      "440\n",
      "printed\n",
      "441\n",
      "printed\n",
      "442\n",
      "printed\n",
      "443\n",
      "printed\n",
      "444\n",
      "printed\n",
      "445\n",
      "printed\n",
      "446\n",
      "printed\n",
      "447\n",
      "printed\n",
      "448\n",
      "printed\n",
      "449\n",
      "printed\n",
      "450\n",
      "printed\n",
      "451\n",
      "printed\n",
      "452\n",
      "printed\n",
      "453\n",
      "printed\n",
      "454\n",
      "printed\n",
      "455\n",
      "printed\n",
      "456\n",
      "printed\n",
      "457\n",
      "printed\n",
      "458\n",
      "printed\n",
      "459\n",
      "printed\n",
      "460\n",
      "printed\n",
      "461\n",
      "printed\n",
      "462\n",
      "printed\n",
      "463\n",
      "printed\n",
      "464\n",
      "printed\n",
      "465\n",
      "printed\n",
      "466\n",
      "printed\n",
      "467\n",
      "printed\n",
      "468\n",
      "printed\n",
      "469\n",
      "printed\n",
      "470\n",
      "printed\n",
      "471\n",
      "printed\n",
      "472\n",
      "printed\n",
      "473\n",
      "printed\n",
      "474\n",
      "printed\n",
      "475\n",
      "printed\n",
      "476\n",
      "printed\n",
      "477\n",
      "printed\n",
      "478\n",
      "printed\n",
      "479\n",
      "printed\n",
      "480\n",
      "printed\n",
      "481\n",
      "printed\n",
      "482\n",
      "printed\n",
      "483\n",
      "printed\n",
      "484\n",
      "printed\n",
      "485\n",
      "printed\n",
      "486\n",
      "printed\n",
      "487\n",
      "printed\n",
      "488\n",
      "printed\n",
      "489\n",
      "printed\n",
      "490\n",
      "printed\n",
      "491\n",
      "printed\n",
      "492\n",
      "printed\n",
      "493\n",
      "printed\n",
      "494\n",
      "printed\n",
      "495\n",
      "printed\n",
      "496\n",
      "printed\n",
      "497\n",
      "printed\n",
      "498\n",
      "printed\n",
      "499\n",
      "printed\n",
      "500\n",
      "printed\n",
      "501\n",
      "printed\n",
      "502\n",
      "printed\n",
      "503\n",
      "printed\n",
      "504\n",
      "printed\n",
      "505\n",
      "printed\n",
      "506\n",
      "printed\n",
      "507\n",
      "printed\n",
      "508\n",
      "printed\n",
      "509\n",
      "printed\n",
      "510\n",
      "printed\n",
      "511\n",
      "printed\n",
      "512\n",
      "printed\n",
      "513\n",
      "printed\n",
      "514\n",
      "printed\n",
      "515\n",
      "printed\n",
      "516\n",
      "printed\n",
      "517\n",
      "printed\n",
      "518\n",
      "printed\n",
      "519\n",
      "printed\n",
      "520\n",
      "printed\n",
      "521\n",
      "printed\n",
      "522\n",
      "printed\n",
      "523\n",
      "printed\n",
      "524\n",
      "printed\n",
      "525\n",
      "printed\n",
      "526\n",
      "printed\n",
      "527\n",
      "printed\n",
      "528\n",
      "printed\n",
      "529\n",
      "printed\n",
      "530\n",
      "printed\n",
      "531\n",
      "printed\n",
      "532\n",
      "printed\n",
      "533\n",
      "printed\n",
      "534\n",
      "printed\n",
      "535\n",
      "printed\n",
      "536\n",
      "printed\n",
      "537\n",
      "printed\n",
      "538\n",
      "printed\n",
      "539\n",
      "printed\n",
      "540\n",
      "printed\n",
      "541\n",
      "printed\n",
      "542\n",
      "printed\n",
      "543\n",
      "printed\n",
      "544\n",
      "printed\n",
      "545\n",
      "printed\n",
      "546\n",
      "printed\n",
      "547\n",
      "printed\n",
      "548\n",
      "printed\n",
      "549\n",
      "printed\n",
      "550\n",
      "printed\n",
      "551\n",
      "printed\n",
      "552\n",
      "printed\n",
      "553\n",
      "printed\n",
      "554\n",
      "printed\n",
      "555\n",
      "printed\n",
      "556\n",
      "printed\n",
      "557\n",
      "printed\n",
      "558\n",
      "printed\n",
      "559\n",
      "printed\n",
      "560\n",
      "printed\n",
      "561\n",
      "printed\n",
      "562\n",
      "printed\n",
      "563\n",
      "printed\n",
      "564\n",
      "printed\n",
      "565\n",
      "printed\n",
      "566\n",
      "printed\n",
      "567\n",
      "printed\n",
      "568\n",
      "printed\n",
      "569\n",
      "printed\n",
      "570\n",
      "printed\n",
      "571\n",
      "printed\n",
      "572\n",
      "printed\n",
      "573\n",
      "printed\n",
      "574\n",
      "printed\n",
      "575\n",
      "printed\n",
      "576\n",
      "printed\n",
      "577\n",
      "printed\n",
      "578\n",
      "printed\n",
      "579\n",
      "printed\n",
      "580\n",
      "printed\n",
      "581\n"
     ]
    }
   ],
   "source": [
    "header_list=['index','text']\n",
    "add_row(header_list,'./',\"pragraph_index_reuters3.csv\")\n",
    "ss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://chrisstrelioff.ws/sandbox/2014/11/13/getting_started_with_latent_dirichlet_allocation_in_python.html\n",
    "from __future__ import division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# document-term matrix\n",
    "X = lda.datasets.load_reuters()\n",
    "print(\"type(X): {}\".format(type(X)))\n",
    "print(\"shape: {}\\n\".format(X.shape))\n",
    "\n",
    "# the vocab\n",
    "vocab = lda.datasets.load_reuters_vocab()\n",
    "print(\"type(vocab): {}\".format(type(vocab)))\n",
    "print(\"len(vocab): {}\\n\".format(len(vocab)))\n",
    "\n",
    "# titles for each story\n",
    "titles = lda.datasets.load_reuters_titles()\n",
    "print(\"type(titles): {}\".format(type(titles)))\n",
    "print(\"len(titles): {}\\n\".format(len(titles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_id = 0\n",
    "word_id = 3117\n",
    "\n",
    "print(\"doc id: {} word id: {}\".format(doc_id, word_id))\n",
    "print(\"-- count: {}\".format(X[doc_id, word_id]))\n",
    "print(\"-- word : {}\".format(vocab[word_id]))\n",
    "print(\"-- doc  : {}\".format(titles[doc_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = lda.LDA(n_topics=400, n_iter=500, random_state=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/\n",
    "# Run in terminal or command prompt\n",
    "# python3 -m spacy download en\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, gensim\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "print(df.target_names.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = df.content.values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# Run in terminal: python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Materialize the sparse data\n",
    "data_dense = data_vectorized.todense()\n",
    "\n",
    "# Compute Sparsicity = Percentage of Non-Zero cells\n",
    "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build LDA Model\n",
    "lda_model = LatentDirichletAllocation(n_topics=20,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "print(lda_model)  # Model attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
    "             evaluate_every=-1, learning_decay=0.7,\n",
    "             learning_method='online', learning_offset=10.0,\n",
    "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
    "             n_components=10, n_jobs=-1, n_topics=20, perp_tol=0.1,\n",
    "             random_state=100, topic_word_prior=None,\n",
    "             total_samples=1000000.0, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Log Likelyhood: Higher the better\n",
    "print(\"Log Likelihood: \", lda_model.score(data_vectorized))\n",
    "\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity: \", lda_model.perplexity(data_vectorized))\n",
    "\n",
    "# See model parameters\n",
    "pprint(lda_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))#,categories=['comp.graphics'])#fetch_20newsgroups(subset='train')\n",
    "\n",
    "from pprint import pprint\n",
    "cat_list=list(newsgroups_train.target_names)\n",
    "pprint(list(newsgroups_train.target_names))\n",
    "print(newsgroups_train.filenames.shape)\n",
    "#for c in cat_list:\n",
    "sub = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'),categories=['comp.graphics'])#fetch_20newsgroups(subset='train')\n",
    "#print(c,sub.filenames.shape)\n",
    "documents = sub.data\n",
    "print(len(documents))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import brown \n",
    "\n",
    "for category in brown.categories(): \n",
    "    print(category)\n",
    "\t#words = brown.words(categories=category)\n",
    "\t#text = \" \".join(words) \n",
    "\t#filename = category + '.txt' \n",
    "\t#outfile = open(filename, 'w') \n",
    "\t#outfile.write(text) \n",
    "#outfile.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = [] # 500 samples\n",
    "\n",
    "for category in brown.categories():\n",
    "    print(category)\n",
    "    words = brown.words(categories=category)\n",
    "    text = \" \".join(words) \n",
    "    #print(text)\n",
    "    for fileid in brown.fileids(category):\n",
    "        print(fileid)\n",
    "        w=brown.words(fileids = fileid)\n",
    "        text = \" \".join(w) \n",
    "        print((text,category))\n",
    "        dataset.append((brown.words(fileids = fileid),category))\n",
    "    print(\"---------------------------\")\n",
    "        \n",
    "dataset[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
