{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot') \n",
    "import numpy as np\n",
    "import scipy.stats.stats as st\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#from stemming.porter2 import stem\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import digits\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import csv\n",
    "\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "import  gensim.models as md\n",
    "from gensim.models.phrases import Phrases, Phraser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Write Excell sheet\n",
    "'''\n",
    "def save_file_to_database(data_rows,path_database,file_databbase,header_list):#header_list=['index','text']\n",
    "    outfile = open(path_database+file_databbase,'w')\n",
    "    writer=csv.writer(outfile)\n",
    "    #header_list=['uuid','paragraph','doc_id']\n",
    "    i=0\n",
    "    for line in data_rows:\n",
    "        row=[i,line]#,'paragraph no.'+str(i)]\n",
    "        if i==0:\n",
    "            \n",
    "            writer.writerow(header_list)\n",
    "            writer.writerow(row)\n",
    "        else:\n",
    "            #print('ff')\n",
    "            writer.writerow(row)\n",
    "        i+= 1\n",
    "        #outfile.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_text_from_database(path_database,file_databbase):\n",
    "    queue_paragraph=[]\n",
    "    #f = open(sys.argv[1], 'rt')\n",
    "    outfile = open(path_database+file_databbase,'rt')\n",
    "    try:\n",
    "                \n",
    "        reader=csv.reader(outfile)\n",
    "        for row in reader:\n",
    "            queue_paragraph.append(row)\n",
    "            #print (row)\n",
    "    finally:\n",
    "        print (\"row\")\n",
    "        outfile.close()\n",
    "        \n",
    "    return queue_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_cvs_by_pands(path_database,file_databbase,index_col, header):\n",
    "    return pd.read_csv(path_database+file_databbase,index_col=index_col,header=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pragraph_to_setnences(str):\n",
    "    return sent_tokenize(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stopwords_list():\n",
    "    stopwordsFile = open('./input/stopwords/stopwords.txt')\n",
    "    stopwordsFile.seek(0)\n",
    "    stopwordsV1 = stopwordsFile.readlines()\n",
    "    stopwordsV2 = []\n",
    "    for sent in stopwordsV1:\n",
    "        sent.replace('\\n', '')\n",
    "        new_word = sent[0:len(sent) - 1]\n",
    "        stopwordsV2.append(new_word.lower())\n",
    "    return stopwordsV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_stop_words = ['the', 'that', 'to', 'as', 'there', 'has', 'and', 'or', 'is', 'not', 'a', 'of', 'but', 'in', 'by', 'on', 'are', 'it', 'if','what','where','how','when']\n",
    "new_stop_words2=['--','i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now','even','until','then','must']\n",
    "numbers=[1,2,3,4,5,6,7,8,9]\n",
    "stopwordsV2=stopwords_list()\n",
    "#stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "def remove_stopword_sentences(str):\n",
    "   \n",
    "            \n",
    "    list_word=[]\n",
    "    tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "    \n",
    "    words=tokenizer.tokenize(str)\n",
    "    for word in words:\n",
    "        new_word = word.encode('ascii', 'ignore').decode('utf-8')\n",
    "        if new_word != '':\n",
    "    \n",
    "            english_stops = set(stopwords.words('english'))\n",
    "           \n",
    "            list_word=[new_word for new_word in words if new_word.lower() not in english_stops\n",
    "                       and new_word.lower() not in new_stop_words \n",
    "                       and new_word.lower() not in new_stop_words2 \n",
    "                       and  not new_word.lower().isdigit() \n",
    "                       and new_word.lower() not in digits \n",
    "                       and new_word.lower() not in  numbers and word.lower() not in stopwordsV2\n",
    "                       and new_word.lower() not in string.punctuation]\n",
    "    \n",
    "  \n",
    "    \n",
    "    return list_word#(stem(setem_word for setem_word in  ([word for word in words if word not in english_stops and word not in new_stop_words])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Courps to CSV docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def courps_to_CSV_docs():\n",
    "    #Reading the news articles file\n",
    "    nyTimesFile = open('./input/new-york-times-articles/nytimes_news_articles.txt', encoding='latin-1')\n",
    "    nyTimesFile.seek(0)\n",
    "    nyTimesV1 = nyTimesFile.readlines()\n",
    "    nyTimesTemp = []\n",
    "    nyTimesURL = []\n",
    "\n",
    "    for i in range(0, len(nyTimesV1)-1):\n",
    "        if re.findall('URL', nyTimesV1[i]) == []:\n",
    "            sent = sent + nyTimesV1[i]\n",
    "            if (re.findall('URL', nyTimesV1[i+1]) != []) and (i+1 < len(nyTimesV1)):\n",
    "                nyTimesTemp.append(sent.strip())\n",
    "        else:\n",
    "            sent = ''\n",
    "            nyTimesURL.append(nyTimesV1[i])\n",
    "\n",
    "    for i in range(0, len(nyTimesTemp)):\n",
    "        nyTimesTemp[i] = nyTimesTemp[i]+'articleID'+str(i)\n",
    "    print(len(nyTimesTemp))\n",
    "    header_list=['index','text']\n",
    "    save_file_to_database(nyTimesTemp,path_database,pragraph_index,header_list)\n",
    "    '''for i in range(1):\n",
    "        print(i,\"============================================\")\n",
    "        print(\"============================================\")'''\n",
    "    #nytimes = preProcessor(nyTimesTemp)\n",
    "    print(\"============================================\")\n",
    "    #print(nytimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs=read_cvs_by_pands(path_database,pragraph_index,None,0)\n",
    "paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc to sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paragraphs_to_sentece(pragraphs):\n",
    "    sentenses_list=[]\n",
    "    \n",
    "    for index_p in  range(len(pragraphs)):\n",
    "        #print(index_p)\n",
    "        #print(\"pppppppppppppppppppp\")\n",
    "        #print(pragraphs[index_p])\n",
    "        \n",
    "        setnences=pragraph_to_setnences(pragraphs[index_p])\n",
    "        print(\"sssssssssssssssssssssssssss\")\n",
    "        #print(setnences)\n",
    "\n",
    "        for indexs in range(len(setnences)):\n",
    "            row=[]\n",
    "            #print(setnences)\n",
    "            row.append(index_p)\n",
    "            row.append(indexs)\n",
    "            row.append(setnences[indexs])\n",
    "            sentenses_list.append(row)\n",
    "    header_list=['index_P','index_sent','sentence']\n",
    "    df = pd.DataFrame(sentenses_list, columns=header_list)#, index=index)\n",
    "    #df\n",
    "\n",
    "    #print(sentenses_list)\n",
    "    df.to_csv(path_database+Sentences, encoding='utf-8', index=False)\n",
    "    #save_file_to_database(sentenses_list,path_database,\"Sentences.csv\",header_list)        \n",
    "    return df\n",
    "\n",
    "#paragraphs_to_sentece(paragraphs.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  paragraphs_to_sentece_Not_stop_word(pragraphs):\n",
    "    #words_list=[]\n",
    "    sentenses_list=[]\n",
    "    \n",
    "    for index_p in  range(len(pragraphs)):\n",
    "        setnences=pragraph_to_setnences(pragraphs[index_p])\n",
    "        \n",
    "        for indexs in range(len(setnences)):    \n",
    "            #print(\"Sentence No. \",indexs,\": \",setnences[indexs],\"\\n\")\n",
    "            words=remove_stopword_sentences(setnences[indexs])\n",
    "            wordsent=''\n",
    "            row=[]\n",
    "            \n",
    "            row.append(index_p)\n",
    "            row.append(indexs)\n",
    "            row.append(words)\n",
    "            sentenses_list.append(row)\n",
    "    header_list=['index_P','index_sent','words_not_stop']\n",
    "    df = pd.DataFrame(sentenses_list, columns=header_list)#, index=index)\n",
    "    #df\n",
    "\n",
    "    #print(sentenses_list)\n",
    "    df.to_csv(path_database+Sentences_not_stops, encoding='utf-8', index=False)\n",
    "    #save_file_to_database(sentenses_list,path_database,\"Sentences.csv\",header_list)        \n",
    "    return df\n",
    "\n",
    "#paragraphs_to_sentece_Not_stop_word(paragraphs.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence To LESK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "this function for compute lesk of word in sentence\n",
    "'''\n",
    "\n",
    "def lesk_word_sentence(sentence,word):\n",
    "    from nltk.wsd import lesk\n",
    "    lesk_synset=''\n",
    "    #lesks= []\n",
    "    #for word in words:\n",
    "    #disambiguated=lesk(context_sentence=sentence, ambiguous_word=word)\n",
    "    disambiguated=lesk(sentence,word, 'n')\n",
    "    #print(disambiguated)\n",
    "    #if disambiguated is not None:\n",
    "    lesk_synset=disambiguated\n",
    "    #else:\n",
    "    #lesk_synset=0\n",
    "    #print(\"Word is: \",word,\"\\n LESK: \",lesk(sentence,word, 'n'),\"\\n Sentence: \",sentence )\n",
    "        \n",
    "    return lesk_synset\n",
    "\n",
    "#lesk(\"Computer science is a discipline that spans theory and practice\",\"science\")\n",
    "\n",
    "#sent = 'people should be able to marry a person of their choice'.split()\n",
    "#lesk(sent, 'able')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lesk_word_sentence(\"I love you\",\"love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_lesks():\n",
    "    df_Sentences=read_cvs_by_pands(path_database,Sentences,None,0)\n",
    "    df_Sentences_not_stops=read_cvs_by_pands(path_database,Sentences_not_stops,None,0)\n",
    "    sentences=df_Sentences.sentence\n",
    "    words_not_stop=df_Sentences_not_stops.words_not_stop#[0]#['words_not_stop'][0]\n",
    "    lesk_df_list=[]\n",
    "    file=open('./NYT_output/file.txt', 'a+')\n",
    "    print(len(df_Sentences))\n",
    "    for i in range(317989):#len(df_Sentences)):\n",
    "        lesks=[]\n",
    "        lesks_name=[]\n",
    "        row=[]\n",
    "        if words_not_stop[i]!='[]':\n",
    "\n",
    "            words_not_stop[i]=words_not_stop[i].replace(\"'\",'').replace(\"[\",'').replace(\"]\",'')\n",
    "            sentences[i]=sentences[i].replace(\"'\",'').replace(\"[\",'').replace(\"]\",'')       \n",
    "            words_not_stop_list=words_not_stop[i].split(',')\n",
    "            #print(words_not_stop_list)\n",
    "            #print(sentences[i])\n",
    "            for word in words_not_stop_list:\n",
    "                #print(word)\n",
    "                l=lesk_word_sentence(sentences[i],word)\n",
    "                #print(\"l\",l)\n",
    "                if l is not None:\n",
    "                    lesks.append(l)\n",
    "                    lesks_name.append(l.name())\n",
    "        #print(\"888888888888888888888888888888888888888888\")\n",
    "        row.append(df_Sentences_not_stops.index_P[i])\n",
    "        row.append(df_Sentences_not_stops.index_sent[i])\n",
    "        row.append(lesks)\n",
    "        row.append(lesks_name)\n",
    "        lesk_df_list.append(row)\n",
    "        print(i)\n",
    "        file.write(str(i))\n",
    "\n",
    "    #print(lesk_df_list)\n",
    "    header_list=['index_P','index_sent','lesks','lesks_name']\n",
    "    df = pd.DataFrame(lesk_df_list, columns=header_list)\n",
    "    \n",
    "    df.to_csv(path_database+\"lesks.csv\", encoding='utf-8', index=False)  \n",
    "    #return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sent_lesks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/nzalake52/text-search-text-mining/notebook\n",
    "#We will be implementing a simple search engine using nltk in python\n",
    "#With the help of TF-IDF ranking and cosine similarity we can rank the documents and get the desired output.\n",
    "#Following is the code for the same, many machine learning algorithms can be applied and the use of this search engine \n",
    "#can be extended.\n",
    "#This is the simplest search engine implementation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Creating the function for preprocessing the text file, the functions does the following:\n",
    "#Removing the URLs in the file\n",
    "#Removing blank lines\n",
    "#Converting the letters that were not read properly due to encoding, to be viewed properly\n",
    "#Removing stopwords from the text\n",
    "#Removing the punctuations form the text\n",
    "\n",
    "def preProcessor(textFile):\n",
    "    print('Starting pre-processing of the corpus..')\n",
    "    print('Start: Word Tokenizing')\n",
    "\n",
    "    textFilev1 = []\n",
    "    textFilev1 = [word_tokenize(sent) for sent in textFile]\n",
    "    \n",
    "    \n",
    "    print('Stop: Word Tokenizing')\n",
    "    print('Start: ASCII encoding for special characters')\n",
    "\n",
    "    textFilev2 = []\n",
    "    for sent in textFilev1:\n",
    "        new_sent = []\n",
    "        for word in sent:\n",
    "            new_word = word.encode('ascii', 'ignore').decode('utf-8')\n",
    "            if new_word != '':\n",
    "                new_sent.append(new_word)\n",
    "        textFilev2.append(new_sent)\n",
    "\n",
    "    print('Stop: ASCII encoding for special characters')\n",
    "    print('Start: Stopwords Removal')\n",
    "\n",
    "    stopwordsFile = open('./input/stopwords/stopwords.txt')\n",
    "    stopwordsFile.seek(0)\n",
    "    stopwordsV1 = stopwordsFile.readlines()\n",
    "    stopwordsV2 = []\n",
    "    for sent in stopwordsV1:\n",
    "        sent.replace('\\n', '')\n",
    "        new_word = sent[0:len(sent) - 1]\n",
    "        stopwordsV2.append(new_word.lower())\n",
    "\n",
    "    textFilev1 = []\n",
    "    for sent in textFilev2:\n",
    "        new_sent = []\n",
    "        for word in sent:\n",
    "            if word.lower() not in stopwordsV2:\n",
    "                new_sent.append(word.lower())\n",
    "        textFilev1.append(new_sent)\n",
    "\n",
    "    print('Stop: Stopwords Removal')\n",
    "    print('Start: Punctuation Removal')\n",
    "\n",
    "    textFilev2 = []\n",
    "    for sent in textFilev1:\n",
    "        new_sent = []\n",
    "        for word in sent:\n",
    "            if word not in string.punctuation:\n",
    "                new_sent.append(word)\n",
    "        textFilev2.append(new_sent)\n",
    "\n",
    "    print('Stop: Punctuation Removal')\n",
    "    print('Start: Phrase Detection')\n",
    "\n",
    "    textFilev1 = []\n",
    "    common_terms = [\"of\", \"with\", \"without\", \"and\", \"or\", \"the\", \"a\", \"so\", \"and\"]\n",
    "    phraseTrainer = Phrases(textFilev2, delimiter=b' ', common_terms=common_terms)\n",
    "    phraser = Phraser(phraseTrainer)\n",
    "    for article in textFilev2:\n",
    "        textFilev1.append((phraser[article]))\n",
    "\n",
    "    print('Stop: Phrase Detection')\n",
    "\n",
    "    return textFilev1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function for creating intermediate index\n",
    "def file_indexing(file):\n",
    "    fileIndex = {}\n",
    "    for index, word in enumerate(file):\n",
    "        if word in fileIndex.keys():\n",
    "            fileIndex[word].append(index)\n",
    "        else:\n",
    "            fileIndex[word] = [index]\n",
    "    return fileIndex\n",
    "\n",
    "#building final index\n",
    "def fullIndex(intIndex):\n",
    "    totalindex = {}\n",
    "    for fileName in intIndex.keys():\n",
    "        for word in intIndex[fileName].keys():\n",
    "            if word in totalindex.keys():\n",
    "                if fileName in totalindex[word].keys():\n",
    "                    totalindex[word][fileName].extend(intIndex[fileName][word][:])\n",
    "                else:\n",
    "                    totalindex[word][fileName] = intIndex[fileName][word]\n",
    "            else:\n",
    "                totalindex[word] = {fileName : intIndex[fileName][word]}\n",
    "    return totalindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyTimesIndex = {}\n",
    "for sent in nytimes:\n",
    "    nyTimesIndex[' '.join(sent)] = file_indexing(sent)\n",
    "\n",
    "nyTimesIndexV1 = fullIndex(nyTimesIndex)\n",
    "\n",
    "#Functions to create one word or phrase query search\n",
    "def wordSearch(word):\n",
    "    if word in nyTimesIndexV1.keys():\n",
    "        return [file for file in nyTimesIndexV1[word].keys()]\n",
    "\n",
    "\n",
    "def phraseQuery(string):\n",
    "    lists, result = [], []\n",
    "    for word in string.split():\n",
    "        lists.append(wordSearch(word))\n",
    "    setList = set(lists[0]).intersection(*lists)\n",
    "    for fileName in setList:\n",
    "        result.append(fileName)\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "searchResult = phraseQuery('white  house')\n",
    "searchResult1 = []\n",
    "for file in wordSearch('white'):\n",
    "    searchResult1.append(file)\n",
    "for file in wordSearch('house'):\n",
    "    if file not in searchResult1:\n",
    "        searchResult1.append(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Making use of TF-IDF ranking to rank the documents given by the searches\n",
    "#Also using similarity metrics (Cosine similarity) to get the similarity scores between both the documents \n",
    "#from both search results\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer().fit(searchResult1)\n",
    "searchResult1TFIDF = tfidf.transform(searchResult1)\n",
    "searchResultTFIDF = tfidf.transform(searchResult)\n",
    "sims = cosine_similarity(searchResult1TFIDF, searchResultTFIDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now putting the cosine similarity results into a data frame\n",
    "#Sorting the dataframe by score and getting the most similar and appropriate 30 documents from the search results\n",
    "cosineSum = []\n",
    "for ind in range(len(sims)):\n",
    "    cosineSum.append(sum(sims[ind]))\n",
    "\n",
    "sumDF = pd.DataFrame({'score':cosineSum})\n",
    "sumDF['index'] = [i for i in range(len(cosineSum))]\n",
    "sumDF.sort_values(by='score', inplace=True, ascending=False)\n",
    "\n",
    "for ind in sumDF['index']:\n",
    "    print(nyTimesURL[int(searchResult1[ind][str(searchResult1[ind]).find('articleid')+9:])], '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
