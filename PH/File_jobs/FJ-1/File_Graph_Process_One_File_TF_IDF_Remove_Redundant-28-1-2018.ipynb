{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/python3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import csv\n",
    "from tensorflow.python.client import timeline\n",
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import Counter\n",
    "import threading\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import digits\n",
    "import os\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#gpuN=\"0\"#sys.argv[1]\n",
    "task_no=sys.argv[1]\n",
    "all_gpus =sys.argv[2]\n",
    "#cuda=\"0,1\"#sys.argv[3]\n",
    "processor=sys.argv[4]\n",
    "dataset=sys.argv[5]\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "dir_path =\"/home/helwan003u1\"\n",
    "path_data_source=dir_path+\"/data/data_source/\"#wiki/wikipedia2text/\"\n",
    "#path_data_source=\"/home/fsg/Desktop/\"\n",
    "\n",
    "path_data_base=dir_path+\"/data/database/\"+processor+dataset+\"/\"\n",
    "\n",
    "#path_data_base=dir_path+\"/data/database/gpu/\"\n",
    "#path_data_base=dir_path+\"/data/database/csv2/\"\n",
    "\n",
    "\n",
    "#path_data_base=\"/home/fsg/Desktop/csv-fatma/\"\n",
    "\n",
    "\n",
    "#files_path_data_source=\"files/splits/\"\n",
    "\n",
    "files_path_data_source=dataset+\"/\"#\"corpus/\"#\"demo/\"\n",
    "\n",
    "#sub_path_data_source=\"small/\"\n",
    "\n",
    "file_path=path_data_source+files_path_data_source\n",
    "\n",
    "file_names = [os.path.join(file_path, f) \n",
    "                      for f in os.listdir(file_path) \n",
    "                      if f.endswith(\".txt\")]\n",
    "\n",
    "\n",
    "path_tf=\"sub_tf/\"\n",
    "path_idf=\"sub_idf/\"\n",
    "path_tfidf=\"sub_tfidf/\"\n",
    "path_non_redundant=\"sub_word_tf/\"\n",
    "path_sim=\"semantics/sim/\"\n",
    "\n",
    "file_path_tf=path_data_base+path_tf\n",
    "file_path_idf=path_data_base+path_idf\n",
    "file_path_tfidf=path_data_base+path_tfidf\n",
    "file_path_non_redundant=path_data_base+path_non_redundant\n",
    "file_path_sim=path_data_base+path_sim\n",
    "\n",
    "file_names_tf = [os.path.join(file_path_tf, f) \n",
    "                      for f in os.listdir(file_path_tf) \n",
    "                      if f.endswith(\".csv\")]\n",
    "file_names_idf = [os.path.join(file_path_idf, f) \n",
    "                      for f in os.listdir(file_path_idf) \n",
    "                      if f.endswith(\".csv\")]\n",
    "\n",
    "\n",
    "file_names_tfidf = [os.path.join(file_path_tfidf, f) \n",
    "                      for f in os.listdir(file_path_tfidf) \n",
    "                      if f.endswith(\".csv\")]\n",
    "\n",
    "\n",
    "file_names_non_redundant = [os.path.join(file_path_non_redundant, f) \n",
    "                      for f in os.listdir(file_path_non_redundant) \n",
    "                      if f.endswith(\".csv\")]\n",
    "\n",
    "\n",
    "file_names_sim = [os.path.join(file_path_sim, f) \n",
    "                      for f in os.listdir(file_path_sim) \n",
    "                      if f.endswith(\".csv\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess, re, os, sys #https://github.com/yaroslavvb/stuff/blob/master/notebook_util.py\n",
    "def run_command(cmd):\n",
    "    \"\"\"Run command, return output as string.\"\"\"\n",
    "    \n",
    "    output = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True).communicate()[0]\n",
    "    return output.decode(\"ascii\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_available_gpus():\n",
    "    \"\"\"Returns list of available GPU ids.\"\"\"\n",
    "    \n",
    "    output = run_command(\"nvidia-smi -L\")\n",
    "    # lines of the form GPU 0: TITAN X\n",
    "    gpu_regex = re.compile(r\"GPU (?P<gpu_id>\\d+):\")\n",
    "    result = []\n",
    "    for line in output.strip().split(\"\\n\"):\n",
    "        m = gpu_regex.match(line)\n",
    "        assert m, \"Couldnt parse \"+line\n",
    "        result.append(int(m.group(\"gpu_id\")))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_txt(txt,file):\n",
    "    text_file = open(file, \"w\")\n",
    "    text_file.write(txt)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gpu_memory_map(gpu_memory_file,gpu_output_file):\n",
    "    \"\"\"Returns map of GPU id to memory allocated on that GPU.\"\"\"\n",
    "\n",
    "    output = run_command(\"nvidia-smi\")\n",
    "    save_txt(output,gpu_output_file)\n",
    "    #print(\"nvidia-smi\",output)\n",
    "    gpu_output = output[output.find(\"GPU Memory\"):]\n",
    "    #print(\"GPU Memory\",gpu_output)\n",
    "    save_txt(gpu_output,gpu_memory_file)\n",
    "   \n",
    "    # lines of the form\n",
    "    # |    0      8734    C   python                                       11705MiB |\n",
    "    memory_regex = re.compile(r\"[|]\\s+?(?P<gpu_id>\\d+)\\D+?(?P<pid>\\d+).+[ ](?P<gpu_memory>\\d+)MiB\")\n",
    "    #print(\"memory_regex\",memory_regex)\n",
    "    rows = gpu_output.split(\"\\n\")\n",
    "    #print(\"rows\",rows)\n",
    "    result = {gpu_id: 0 for gpu_id in list_available_gpus()}\n",
    "    #print(\"result\",result)\n",
    "    for row in gpu_output.split(\"\\n\"):\n",
    "        m = memory_regex.search(row)\n",
    "        if not m:\n",
    "            continue\n",
    "        gpu_id = int(m.group(\"gpu_id\"))\n",
    "        gpu_memory = int(m.group(\"gpu_memory\"))\n",
    "        result[gpu_id] += gpu_memory\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_cvs_by_pands(path_database,file_database,index_col, header):\n",
    "    import csv\n",
    "    import pandas as pd\n",
    "    df=pd.read_csv(path_database+file_database,index_col=index_col,header=header)\n",
    "    \n",
    "    return df#pd.read_csv(path_database+file_database,index_col=index_col,header=header)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def isfile_empty(file_path_name):\n",
    "    f=open(file_path_name, 'r',encoding='utf-8') \n",
    "    is_blank = len(f.read().strip()) == 0\n",
    "    return is_blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#isfile_empty(\"/home/fsg/Desktop/cssplit225.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_cvs_by_pands(path_database,file_database,header,data_rows):\n",
    "    import csv\n",
    "    import pandas as pd\n",
    "    csv_df=pd.DataFrame(data_rows,columns=header ) \n",
    "    csv_df.to_csv(path_database+file_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_sord_tf_file(path_data_source,sub_path_data_source_tf,i):\n",
    "    #df=read_cvs_by_pands(path_data_source,sub_path_data_source_tf+\"cs\"+str(i)+\".csv\",0,header=0)\n",
    "    #print(path_data_source+sub_path_data_source_tf+i)\n",
    "    df=read_cvs_by_pands(path_data_source,sub_path_data_source_tf+i,0,header=0)\n",
    "    #ee.T.sort_index(inplace=True)\n",
    "    df = df.T.sort_index()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_sord_idf_file(path_data_source,sub_path_data_source_idf,i):\n",
    "    #df=read_cvs_by_pands(path_data_source,sub_path_data_source_idf+\"cs\"+str(i)+\".csv\",0,header=0)\n",
    "    df=read_cvs_by_pands(path_data_source,sub_path_data_source_idf+i,0,header=0)\n",
    "    #ee.T.sort_index(inplace=True)\n",
    "    df = df.sort_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "Write Excell sheet\n",
    "'''\n",
    "def save_file_to_database(data_rows,path_database,file_databbase,header_list):\n",
    "    import csv\n",
    "    outfile = open(path_database+file_databbase,'w')\n",
    "    writer=csv.writer(outfile)\n",
    "    #header_list=['uuid','paragraph','doc_id']\n",
    "    i=0\n",
    "    for line in data_rows:\n",
    "        row=[i,line,'paragraph no.'+str(i)]\n",
    "        if i==0:\n",
    "            \n",
    "            writer.writerow(header_list)\n",
    "            writer.writerow(row)\n",
    "        else:\n",
    "            ##print('ff')\n",
    "            writer.writerow(row)\n",
    "        i+= 1\n",
    "        #outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Read Excell sheet\n",
    "'''\n",
    "def read_text_from_database(path_database,file_databbase):\n",
    "    import csv\n",
    "    queue_paragraph=[]\n",
    "    #f = open(sys.argv[1], 'rt')\n",
    "    outfile = open(path_database+file_databbase,'rt')\n",
    "    try:\n",
    "                \n",
    "        reader=csv.reader(outfile)\n",
    "        for row in reader:\n",
    "            queue_paragraph.append(row)\n",
    "            ##print (row)\n",
    "    finally:\n",
    "        ##print (\"row\")\n",
    "        outfile.close()\n",
    "        \n",
    "    return queue_paragraph\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_row_csv(path_database,idf,list_data):\n",
    "    import csv\n",
    "    with open(path_database+idf, 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(list_data)\n",
    "        f.close()\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_file(path_data_source,file_name):\n",
    "    import csv\n",
    "    outfile = open(path_data_source+file_name,'w')\n",
    "    writer=csv.writer(outfile)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_file_exist(file_path,file_name):\n",
    "    file_names = [os.path.join(file_path, f) \n",
    "                      for f in os.listdir(file_path) \n",
    "                      if f.endswith(\".csv\")]\n",
    "    #print(file_names)\n",
    "    if file_name in file_names:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save pragraphs to files\n",
    "def write_file(pragraph,num_pragraph,path):\n",
    "    file = open(path+str(num_pragraph)+\".txt\",\"w\") \n",
    " \n",
    "    file.write(pragraph) \n",
    "    \n",
    "    file.close() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create sub dataset\n",
    "def sub_dataset(path_data_source,data_source):\n",
    "    pragraphs=txt_pragraphs(read_file(path_data_source+data_source))\n",
    "    counter=0\n",
    "    for pragraph in pragraphs:\n",
    "        ##print('pragraph no ',counter)\n",
    "        write_file(pragraph,counter,sub_path_data_source)\n",
    "        counter +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(str):\n",
    "    file = open(str,'r',encoding='utf-8')\n",
    "    txt=file.read()\n",
    "    ##print(txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def txt_pragraphs(str):\n",
    "    pragraphs = str.split(\"\\n\\n\")\n",
    "    return pragraphs\n",
    "#pragraphs=txt_pragraphs(txt)\n",
    "#type(pragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pragraph_to_setnences(str):\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    return sent_tokenize(str)\n",
    "#setnences=pragraph_to_setnences(pragraphs[n_pragraph])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_numbers(s):\n",
    "    from string import digits\n",
    "\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    res = s.translate(remove_digits)\n",
    "    if res!='':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove_numbers(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_stop_words = ['the', 'that', 'to', 'as', 'there', 'has', 'and', 'or', 'is', 'not', 'a', 'of', 'but', 'in', 'by', 'on', 'are', 'it', 'if','what','where','how','when']\n",
    "new_stop_words2=['--','i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now','even','until','then','must']\n",
    "numbers=[1,2,3,4,5,6,7,8,9]\n",
    "#stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "def remove_stopword_sentences(sent):\n",
    "    import nltk\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    import time\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    from string import digits\n",
    "    from nltk.corpus.reader.wordnet import WordNetError\n",
    "    import sys\n",
    "    list_word=[]\n",
    "\n",
    "    try:\n",
    "        tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "    \n",
    "        words=tokenizer.tokenize(sent)\n",
    "    \n",
    "        english_stops = set(stopwords.words('english'))\n",
    "        #stems=[]\n",
    "        \n",
    "        list_word=[word for word in words if word.lower() not in english_stops and word.lower() not in new_stop_words and word.lower() not in new_stop_words2 and  not word.lower().isdigit() and word.lower() not in digits and word.lower() not in  numbers and remove_numbers(word)]\n",
    "    \n",
    "    #for word in list_word:\n",
    "        #stems.append(stem(word))\n",
    "        #stems.append(PorterStemmer().stem(word))\n",
    "        #stems.append(stemmer.stem(word))\n",
    "        #stems.append(stemmer.stem(\"computer\"))\n",
    "        #stems.append(word)\n",
    "    except WordNetError as e:\n",
    "        print(\"WordNetError on concept {}: {}\".format(\"remove_stopword_sentences: \",e))\n",
    "    except AttributeError as e:\n",
    "        print(\"Attribute error on concept {}: {}\".format(\"remove_stopword_sentences: \", e))\n",
    "    except:\n",
    "        print(\"Unexpected error on concept {}: {}\".format(\"remove_stopword_sentences: \", sys.exc_info()[0]))\n",
    "    \n",
    "    return list_word#stems#(stem(setem_word for setem_word in  ([word for word in words if word not in english_stops and word not in new_stop_words])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove_stopword_sentences(\"//125 dfd \\ dfjfd-eee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_list_sentece(pragraph):\n",
    "    words_list=[]\n",
    "    setnences=pragraph_to_setnences(pragraph)\n",
    "    for indexs in range(len(setnences)):    \n",
    "        ##print(\"Sentence No. \",indexs,\": \",setnences[indexs],\"\\n\")\n",
    "        words=remove_stopword_sentences(setnences[indexs])\n",
    "        wordsent=''\n",
    "        for index in range(len(words)):\n",
    "            wordsent+=' '+words[index]\n",
    "            ##print(\"wordsent:\",wordsent)\n",
    "            \n",
    "        words_list.append(wordsent)\n",
    "        #count = Counter(words)\n",
    "        ##print(\"wordsent:\",wordsent)\n",
    "        ##print(\" word:\",words)\n",
    "    ##print(words_list)\n",
    "    return words_list\n",
    "\n",
    "#corpus=word_list_sentece(pragraphs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "this function for compute lesk for each word(list of word) in sentence\n",
    "'''\n",
    "def lesk_words_sentence(words,sentence):\n",
    "    import nltk\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    from nltk.wsd import lesk\n",
    "    lesks= []\n",
    "    for word in words:\n",
    "        if lesk(sentence,word, 'n') is not None:\n",
    "            lesks.append(lesk(sentence,word, 'n'))\n",
    "            ##print(\"Word is: \",word,\"\\n LESK: \",lesk(sentence,word, 'n'),\"\\n Sentence: \",sentence )\n",
    "        \n",
    "    return lesks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "this function for compute lesk of word in sentence\n",
    "'''\n",
    "\n",
    "def lesk_word_sentence(sentence,word):\n",
    "    import nltk\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    from nltk.wsd import lesk\n",
    "    from nltk.corpus.reader.wordnet import WordNetError\n",
    "    import sys\n",
    "    disambiguated=''\n",
    "    ##print(type(disambiguated))\n",
    "    try:\n",
    "        \n",
    "    #lesks= []\n",
    "    #for word in words:\n",
    "    #disambiguated=lesk(context_sentence=sentence, ambiguous_word=word)\n",
    "    \n",
    "        disambiguated=lesk(sentence,word, 'n')\n",
    "        ##print(type(disambiguated))\n",
    "    ##print(disambiguated)\n",
    "    #if disambiguated is not None:\n",
    "        #lesk_synset=disambiguated\n",
    "    #else:\n",
    "    #lesk_synset=0\n",
    "    ##print(\"Word is: \",word,\"\\n LESK: \",lesk(sentence,word, 'n'),\"\\n Sentence: \",sentence )\n",
    "    except WordNetError as e:\n",
    "        print(\"WordNetError on concept {}: {}\".format(\"lesk_word_sentence: \",e))\n",
    "    except AttributeError as e:\n",
    "        print(\"Attribute error on concept {}: {}\".format(\"lesk_word_sentence: \", e))\n",
    "    except:\n",
    "        print(\"Unexpected error on concept {}: {}\".format(\"lesk_word_sentence: \", sys.exc_info()[0]))    \n",
    "    return disambiguated\n",
    "\n",
    "#lesk(\"Computer science is a discipline that spans theory and practice\",\"science\")\n",
    "\n",
    "#sent = 'people should be able to marry a person of their choice'.split()\n",
    "#lesk(sent, 'able')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def idf_df(df,D,base):\n",
    "    #[7/df['0']]\n",
    "    y = [log_idf(D,x,base) for x in df['0']]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_idf(D,d,base):\n",
    "    return math.log((D/d), base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idf_one(path_data_source,sub_path_data_source_tf,sub_path_data_source_idf,i,sub_path_data_source_tfidf):\n",
    "    df_tf=read_sord_tf_file(path_data_source,sub_path_data_source_tf,i)\n",
    "    df_idf=read_sord_idf_file(path_data_source,sub_path_data_source_idf,i)\n",
    "\n",
    "    if len(df_idf) != 0:\n",
    "        idf=idf_df(df_idf,len(df_idf),10)\n",
    "\n",
    "        full_tfidf=[]\n",
    "        for index in range(len(idf)):\n",
    "            ##print(index)\n",
    "            tfidf=df_tf[0][index]*idf[index]\n",
    "            full_tfidf.append(tfidf)\n",
    "\n",
    "\n",
    "    df_tf_idf=pd.DataFrame(full_tfidf)\n",
    "    df_tf_idf.index=df_tf.index\n",
    "    #df_tf_idf.to_csv(path_data_source+sub_path_data_source_tfidf+\"cs\"+str(i)+\".csv\")\n",
    "    df_tf_idf.to_csv(path_data_source+sub_path_data_source_tfidf+i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def full_name_file(full_name_path):#like cs.csv\n",
    "    d=full_name_path.split(\"/\")\n",
    "    ##print(d)\n",
    "    name=d[len(d)-1]#.split(\".\")\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name_file(full_name_path): #like cs\n",
    "    d=full_name_path.split(\"/\")\n",
    "    ##print(d)\n",
    "    name=d[len(d)-1].split(\".\")\n",
    "    return name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "merge two dictionary \n",
    "d={'a': 1, 'c': 3, 'k': 5}\n",
    "d1={'g': 1, 'c': 3, 'b': 5}\n",
    "like merge(d, d1,lambda x,y: x+1)\n",
    "{'a': 1, 'b': 5, 'c': 4, 'g': 1, 'k': 5}\n",
    "'''\n",
    "\n",
    "def merge(d1, d2, merge_fn=lambda x,y:y):\n",
    "    \"\"\"\n",
    "    Merges two dictionaries, non-destructively, combining \n",
    "    values on duplicate keys as defined by the optional merge\n",
    "    function.  The default behavior replaces the values in d1\n",
    "    with corresponding values in d2.  (There is no other generally\n",
    "    applicable merge strategy, but often you'll have homogeneous \n",
    "    types in your dicts, so specifying a merge technique can be \n",
    "    valuable.)\n",
    "\n",
    "    Examples:\n",
    "\n",
    "    >>> d1\n",
    "    {'a': 1, 'c': 3, 'b': 2}\n",
    "    >>> merge(d1, d1)\n",
    "    {'a': 1, 'c': 3, 'b': 2}\n",
    "    >>> merge(d1, d1, lambda x,y: x+y)\n",
    "    {'a': 2, 'c': 6, 'b': 4}\n",
    "\n",
    "    \"\"\"\n",
    "    result = dict(d1)\n",
    "    for k,v in d2.items():\n",
    "        if k in result:\n",
    "            result[k] = merge_fn(result[k], v)\n",
    "            ##print(k)\n",
    "        #else:\n",
    "            #result[k] = v\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def match_lists(list_one,list_two):\n",
    "    dic_one=list_to_dict_one(list_one)\n",
    "    dic_two=list_to_dict_one(list_two)\n",
    "    return merge(dic_one, dic_two,lambda x,y: x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert list to dic has value 1\n",
    "def list_to_dict_one(my_list):\n",
    "    my_dict = {k: 1 for k in my_list} \n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Write Excell sheet\n",
    "'''\n",
    "def save_list_to_csv(data_rows,path_data_base,path_file,file_name):\n",
    "    import csv\n",
    "    outfile = open(path_data_base+path_file+file_name,'w')\n",
    "    writer=csv.writer(outfile)\n",
    "    \n",
    "    writer.writerow(data_rows)\n",
    "     \n",
    "    outfile.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def match_file_files(one_file,list_files):\n",
    "    path_database=dir_path+\"/data/database/csv/\"\n",
    "    path_sub_idf=\"sub_idf/\"\n",
    "    \n",
    "    word_list_one_file=csv_to_list(one_file)\n",
    "    ##print(type(word_list_one_file))\n",
    "    dict_one_word_list=list_to_dict_one(word_list_one_file[0])\n",
    "    ##print(dict_one_word_list)\n",
    "   \n",
    "    for i in range(len(list_files)):  \n",
    "        \n",
    "        ##print(\"***************\",i,\"******************\")\n",
    "        filename=list_files[i]\n",
    "        if one_file != filename:\n",
    "            ##print(name_file(filename))\n",
    "            word_list=csv_to_list(filename)\n",
    "            dict_word_list=list_to_dict_one(word_list[0])\n",
    "            ##print(dict_word_list)\n",
    "            dict_one_word_list=merge(dict_one_word_list, dict_word_list,lambda dict_one_word_list,dict_word_list:dict_one_word_list+1)\n",
    "            ##print(dict_one_word_list)\n",
    "        #else:\n",
    "            ##print(\"equal\")\n",
    "    write_cvs_by_pands(path_database+path_sub_idf,name_file(one_file)+'.csv',dict_one_word_list)\n",
    "    ##print('\\n',merg_dict.keys())\n",
    "    return dict_one_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def file_to_LESK_TF(filename,path_database,path_tf): \n",
    "    path_database=path_database#dir_path+\"/data/database/csv/\"\n",
    "    #path_sub_tfidf=path_sub_tfidf#\"sub_word_tf/\"\n",
    "    #path_full_tfidf=path_full_tfidf#\"full_word_tf/\"\n",
    "    path_tf=path_tf#\"sub_tf/\"\n",
    "    #TF_File=\"TF-\"\n",
    "    #TF_Full=\"TF-Full.csv\"\n",
    "\n",
    "    #for i in range(len(file_list_task)):    \n",
    "    #with tf.Session(config=config) as sess:\n",
    "    #index_paragraph=0\n",
    "    col=1\n",
    "\n",
    "    #index_file=0\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    ##print(file_names)\n",
    "\n",
    "    #for filename in file_names:\n",
    "        ##print(\"index_file\",str(index_file))\n",
    "    word_file_fatma=[]\n",
    "    #filename=file_list_task[i]\n",
    "    with open(filename,encoding='utf-8') as inf:\n",
    "            ##print(\"tpe\",type(inf))\n",
    "        txt=inf.read()\n",
    "\n",
    "        paragraph_list=txt_pragraphs(txt)   \n",
    "\n",
    "\n",
    "        for paragraph in paragraph_list: #get pragraphs(documents) from DB\n",
    "                ##print(\"Pragraph type \",type(paragraph))\n",
    "\n",
    "\n",
    "            #if index_paragraph ==0:\n",
    "                #index_paragraph += 1\n",
    "            #else:\n",
    "\n",
    "            setnences=pragraph_to_setnences(paragraph)#partitions paragraph to sentence\n",
    "\n",
    "\n",
    "            for setnence in setnences:\n",
    "                        ##print(\"  \",setnence)                            \n",
    "\n",
    "                words=remove_stopword_sentences(setnence)#remove stop words and noise\n",
    "                #try:\n",
    "\n",
    "                for word in words:\n",
    "                    try:\n",
    "\n",
    "                            lesk=lesk_word_sentence(setnence,word)#get LESK of word in sentence\n",
    "\n",
    "\n",
    "                            #paragraph_word.append(word_sentence)\n",
    "\n",
    "                            if lesk is not None:\n",
    "                            ##print(\"type of lesk in words\",type(lesk),lesk)\n",
    "\n",
    "                                word_file_fatma.append(lesk.name())\n",
    "\n",
    "                    except WordNetError as e:\n",
    "                            print(\"WordNetError on concept {}:{}\".format(\"My model \"+word+\" \"+lesk,e))\n",
    "                    except AttributeError as e:\n",
    "                            print(\"Attribute error on concept {}:{}\".format(\"My model \"+word+\" \"+lesk,e))\n",
    "                    except:\n",
    "                            print(\"Unexpected error on concept {}:{}\".format(\"My model \"+word+\" \"+lesk,sys.exc_info()[0]))\n",
    "\n",
    "\n",
    "\n",
    "                '''////////////////END Sentence////////////////# '''\n",
    "\n",
    "\n",
    "            #write_cvs_by_pands(path_database,word_sentences_table,word_sentences_list,word_sentences_list_data)\n",
    "\n",
    "\n",
    "            '''////////////////END PARAGRAPH////////////////# '''\n",
    "\n",
    "    #write_cvs_by_pands(path_database,sentences_paragraph_table,sentences_paragraph_list,sentences_paragraph_list_data)\n",
    "\n",
    "    ##print(word_file_fatma)\n",
    "\n",
    "    word_file_Freq=Counter(word_file_fatma)\n",
    "    sum_count=sum(word_file_Freq.values())\n",
    "\n",
    "    ##print(type(word_file_Freq))\n",
    "    ##print(word_file_Freq)\n",
    "    #csv_df=pd.DataFrame([word_file_Freq],columns=word_file_Freq.keys() ) \n",
    "    freq=[]\n",
    "    for i in word_file_Freq.values():\n",
    "        c=i/sum_count\n",
    "        freq.append(c)\n",
    "    csv_df=pd.DataFrame([freq],columns=word_file_Freq.keys() ) \n",
    "\n",
    "    #Save TF file\n",
    "    #new_file_name=\"cs\"+name_file(filename)+\".csv\"\n",
    "    new_file_name=name_file(filename)+\".csv\"\n",
    "    csv_df.to_csv(path_database+path_tf+new_file_name)\n",
    "        # add to idf file \n",
    "\n",
    "    #full_list=[]\n",
    "    #full_list.insert(0,name_file(filename)) # to add name of file in the firest cell like cs1 or cs4\n",
    "    #full_list=full_list+list(word_file_Freq.keys())\n",
    "    # add to single\n",
    "\n",
    "    #add_row_csv(path_database+path_sub_tfidf,full_name_file(filename),list(word_file_Freq.keys()))\n",
    "    # add to total idf file \n",
    "    #add_row_csv(path_database+path_full_tfidf,TF_Full,list(word_file_Freq.keys()))\n",
    "    #index_file +=1\n",
    "    return new_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#with open(\"/home/fsg/Desktop/split0.txt\",encoding='utf-8') as inf:\n",
    "    #txt=inf.read()\n",
    "    #print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\"ff/sduy.2.2.01\".replace(\"/\", \"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non Redundant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1={'a': 1, 'b': 1, 'c': 1}\n",
    "d2={'d': 1, 'x': 1, 'b': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "d1={'a': 1, 'b': 1, 'c': 1}\n",
    "d2={'d': 1, 'x': 1, 'b': 1}\n",
    "result={'a': 1, 'c': 1}\n",
    "'''\n",
    "\n",
    "def dict_remove_redundant(dic1,dic2):\n",
    "    \n",
    "    dic1=merge(dic1, dic2,lambda dic1,dic2:dic1*0)\n",
    "    #print(\"\\n\")\n",
    "    #print(\"in remove\",len(dic1),dic1,\"\\n\")\n",
    "    dic1=dict((k,v) for k, v in dic1.items() if v)\n",
    "    #print(\"in remove 2\",len(dic1),dic1,\"\\n\")\n",
    "    return dic1\n",
    "\n",
    "#dict_remove_redundant(d1,d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dict_key_to_list(dic):#like dict_keys(['a', 'c'])\n",
    "    \n",
    "    return dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_last_file_list(file_path,extention):\n",
    "    \n",
    "    file_names = [os.path.join(file_path, f) \n",
    "                      for f in os.listdir(file_path) \n",
    "                      if f.endswith(extention) and not isfile_empty(file_path+f)]\n",
    "    return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def number_file(f):\n",
    "    import re\n",
    "    r = re.compile(\"([a-zA-Z]+)([0-9]+)\")\n",
    "    m = r.match(f.split('.')[0])\n",
    "    return int(m.group(2))\n",
    "#number_file('ff1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file_list_befor_task(file_path,extention):\n",
    "    \n",
    "    file_names = [os.path.join(file_path, f) \n",
    "                      for f in os.listdir(file_path) \n",
    "                      if f.endswith(extention) and not isfile_empty(file_path+f) and number_file(f)< int(task_no) ]\n",
    "    return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read_file_list_befor_task('/home/fsg/Desktop/files/',\".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_redundant(tf_file,path_data_base,path_tf,path_non_redundant):\n",
    "    print(\"remove_redundant\",tf_file)\n",
    "    list_tf=read_cvs_by_pands(path_data_base+path_tf,tf_file,0,0).keys()\n",
    "    dic_tf=list_to_dict_one(list_tf)\n",
    "    ##print(dic_tf)\n",
    "    \n",
    "    file_names_non_redundant=read_file_list_befor_task(path_data_base+path_non_redundant,\".csv\")\n",
    "    ##print(\"file_names_non_redundant\",file_names_non_redundant)\n",
    "    for file_nonredun in file_names_non_redundant:\n",
    "        ##print(\"file_nonredun\")\n",
    "        pure_file_name=full_name_file(file_nonredun)\n",
    "        ##print(pure_file_name)\n",
    "        list_non=read_cvs_by_pands(file_path_non_redundant,pure_file_name,None,0)\n",
    "        \n",
    "        dic_non=list_to_dict_one(list_non)\n",
    "        #print(\"list_non\",dic_non)\n",
    "        dic_tf=dict_remove_redundant(dic_tf,dic_non)#///////////////////\n",
    "        #print(\"dic_tf\",len(dic_tf),dic_tf)\n",
    "    list_term=dict_key_to_list(dic_tf)\n",
    "    #print(\"list_term\",len(list_term))\n",
    "    \n",
    "    save_list_to_csv(list_term,path_data_base,path_non_redundant,tf_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " #remove_redundant(\"cs1.csv\",path_data_base,path_tf,path_non_redundant)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dict_IDF(dic1,dic2):\n",
    "    \n",
    "    dic3=merge(dic1, dic2,lambda dic1,dic2:dic1+1)\n",
    "    dic4=merge(dic2, dic1,lambda dic2,dic1:dic2+1)\n",
    "    \n",
    "    return dic3,dic3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#d1={'a': 1, 'b': 1, 'c': 1}\n",
    "def dict_to_DF(dic):\n",
    "    df=pd.DataFrame([dic])\n",
    "    return df\n",
    "\n",
    "#dict_to_DF(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_df_to_csv(df,path_database,sub_path,new_file_name):\n",
    "     df.to_csv(path_database+sub_path+new_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#s='2,010.0'\n",
    "\n",
    "#s.split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def magic(numList):         # [1,2,3]\n",
    "    print(numList)\n",
    "    s = map(str, numList)   # ['1','2','3']\n",
    "    \n",
    "    s = ''.join(s)          # '123'\n",
    "    s=s.split('.')[0]\n",
    "    s = int(s)              # 123\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "magic([1.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def df_to_dict(df):\n",
    "        \n",
    "    dic={}\n",
    "    keys=df.keys()\n",
    "    \n",
    "    values= df.T.values.tolist()\n",
    "    #print(len(values))\n",
    "    for i in range(len(keys)):\n",
    "        #print(keys[i])\n",
    "        dic[keys[i]]=magic(values[i])\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df=read_cvs_by_pands(path_data_base+path_idf,\"cs0.csv\",0,0)\n",
    "#df_to_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Idf(path_data_base,path_tf,tf_file_name,path_idf):\n",
    "    #tf_file_name=\"cs2.csv\"\n",
    "    list_tf=read_cvs_by_pands(path_data_base+path_tf,tf_file_name,0,0).keys()\n",
    "\n",
    "    dic_tf=list_to_dict_one(list_tf)\n",
    "    #print(dic_tf)\n",
    "\n",
    "    file_names_IDF=read_last_file_list(path_data_base+path_idf,\".csv\")\n",
    "\n",
    "\n",
    "    for file_IDF in file_names_IDF:\n",
    "            old_dic_tf_updated=dic_tf.copy()\n",
    "            ##print(\"file_nonredun\")\n",
    "            pure_file_name=full_name_file(file_IDF)\n",
    "            print(pure_file_name)\n",
    "            #open this file name as list with value\n",
    "            df_idf=read_cvs_by_pands(path_data_base+path_idf,pure_file_name,0,0)#.keys()\n",
    "            #print(\"df_idf \\n\")\n",
    "            print(df_idf )\n",
    "            #convert list to dic dict_idf\n",
    "            dict_idf=df_to_dict(df_idf)\n",
    "            #merge dic_tf with dict_idf\n",
    "            dic_tf=merge(dic_tf, dict_idf,lambda dic_tf,dict_idf:dic_tf+1)\n",
    "            #if dic_tf changed \n",
    "            if old_dic_tf_updated != dic_tf:\n",
    "                #print(\"yeeeeeeeees\")\n",
    "                #merge dict_idf  with dic_tf \n",
    "                dict_idf=merge(dict_idf, dic_tf,lambda dict_idf,dic_tf:dict_idf+1)\n",
    "                #convert dict_idf to df_idf\n",
    "                df_idf_updated=dict_to_DF(dict_idf)\n",
    "                #save df_idf to csv\n",
    "                save_df_to_csv(df_idf_updated,path_data_base,path_idf,pure_file_name)\n",
    "\n",
    "    df_idf=dict_to_DF(dic_tf)\n",
    "    save_df_to_csv(df_idf,path_data_base,path_idf,tf_file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sub_list_file(file_list_task,all_gpus):\n",
    "    import math\n",
    "    sub_len=math.ceil(len(file_list_task)/all_gpus)\n",
    "    global_list_len=math.ceil(len(file_list_task)/sub_len)\n",
    "    \n",
    "    global_list=[]\n",
    "    index=0\n",
    "    for x in range(global_list_len):\n",
    "        sublist=[]\n",
    "        for i in range(sub_len):\n",
    "            if index < len(file_list_task):\n",
    "                sublist.append(file_list_task[index])\n",
    "                index +=1\n",
    "                \n",
    "        global_list.append(sublist)\n",
    "\n",
    "    return global_list\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic as wnic\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def similarity_by_infocontent(sense1, sense2, option):\n",
    "    #sense1=\"Synset('\"+sense1+\"')\"\n",
    "    #sense2=\"Synset('\"+sense2+\"')\"\n",
    "    #print(sense1,sense2)\n",
    "    sense1 = wn.synset(sense1)\n",
    "    sense2 = wn.synset(sense2)\n",
    "    #print(sense1,sense2)\n",
    "    \"\"\" Returns similarity scores by information content. \"\"\"\n",
    "    #if sense1.pos != sense2.pos: # infocontent sim can't do diff POS.\n",
    "        #return 0\n",
    "\n",
    "    info_contents = ['ic-bnc-add1.dat', 'ic-bnc-resnik-add1.dat', \n",
    "                     'ic-bnc-resnik.dat', 'ic-bnc.dat', \n",
    "\n",
    "                     'ic-brown-add1.dat', 'ic-brown-resnik-add1.dat', \n",
    "                     'ic-brown-resnik.dat', 'ic-brown.dat', \n",
    "\n",
    "                     'ic-semcor-add1.dat', 'ic-semcor.dat',\n",
    "\n",
    "                     'ic-semcorraw-add1.dat', 'ic-semcorraw-resnik-add1.dat', \n",
    "                     'ic-semcorraw-resnik.dat', 'ic-semcorraw.dat', \n",
    "\n",
    "                     'ic-shaks-add1.dat', 'ic-shaks-resnik.dat', \n",
    "                     'ic-shaks-resnink-add1.dat', 'ic-shaks.dat', \n",
    "\n",
    "                     'ic-treebank-add1.dat', 'ic-treebank-resnik-add1.dat', \n",
    "                     'ic-treebank-resnik.dat', 'ic-treebank.dat']\n",
    "\n",
    "    if option in ['res', 'resnik']:\n",
    "        #return wn.res_similarity(sense1, sense2, wnic.ic('ic-bnc-resnik-add1.dat'))\n",
    "        #print('simRe snik (c1,c2) = -log p(lso(c1,c2)) = IC(lso(c1,c2)')\n",
    "        return wn.res_similarity(sense1, sense2, wnic.ic('ic-treebank-resnik-add1.dat'))\n",
    "    #return min(wn.res_similarity(sense1, sense2, wnic.ic(ic)) \\\n",
    "    #             for ic in info_contents)\n",
    "\n",
    "    elif option in ['jcn', \"jiang-conrath\"]:\n",
    "        #return wn.jcn_similarity(sense1, sense2, wnic.ic('ic-bnc-add1.dat'))\n",
    "        #print('sim(jcn) (c1,c2 )= (IC(c1) + IC(c2 )) - 2IC(lso(c1,c2 ))')\n",
    "        return wn.jcn_similarity(sense1, sense2, wnic.ic('ic-treebank.dat'))\n",
    "\n",
    "    elif option in ['lin']:\n",
    "        #return wn.lin_similarity(sense1, sense2, wnic.ic('ic-bnc-add1.dat'))\n",
    "        #print('sim(lin) (c1,c2)=(2IC(lso(c1,c2 )))/(IC(c1)+IC(c2))')\n",
    "        return wn.lin_similarity(sense1, sense2, wnic.ic('ic-treebank.dat'))\n",
    "\n",
    "def sim(sense1, sense2, option=\"path\"):\n",
    "    \"\"\" Calculates similarity based on user's choice. \"\"\"\n",
    "    option = option.lower()\n",
    "    if option.lower() in [\"path\", \"path_similarity\", \n",
    "                        \"wup\", \"wupa\", \"wu-palmer\", \"wu-palmer\",\n",
    "                        'lch', \"leacock-chordorow\"]:\n",
    "        return similarity_by_path(sense1, sense2, option) \n",
    "    elif option.lower() in [\"res\", \"resnik\",\n",
    "                          \"jcn\",\"jiang-conrath\",\n",
    "                          \"lin\"]:\n",
    "        return similarity_by_infocontent(sense1, sense2, option)\n",
    "\n",
    "def max_similarity(context_sentence, ambiguous_word, option=\"path\", \n",
    "                   pos=None, best=True):\n",
    "    \"\"\"\n",
    "    Perform WSD by maximizing the sum of maximum similarity between possible \n",
    "    synsets of all words in the context sentence and the possible synsets of the \n",
    "    ambiguous words (see http://goo.gl/XMq2BI):\n",
    "    {argmax}_{synset(a)}(\\sum_{i}^{n}{{max}_{synset(i)}(sim(i,a))}\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for i in wn.synsets(ambiguous_word):\n",
    "        try:\n",
    "            if pos and pos != str(i.pos()):\n",
    "                continue\n",
    "        except:\n",
    "            if pos and pos != str(i.pos):\n",
    "                continue\n",
    "        result[i] = sum(max([sim(i,k,option) for k in wn.synsets(j)]+[0]) \\\n",
    "                        for j in word_tokenize(context_sentence))\n",
    "\n",
    "    if option in [\"res\",\"resnik\"]: # lower score = more similar\n",
    "        result = sorted([(v,k) for k,v in result.items()])\n",
    "    else: # higher score = more similar\n",
    "        result = sorted([(v,k) for k,v in result.items()],reverse=True)\n",
    "    #print (result)\n",
    "    if best: return result[0][1];\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#similarity_by_infocontent('read/write_head.n.01', 'read/write_head.n.01', 'res')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sim_terms_one_file(path_data_base,path_non_redundant,file_name):\n",
    "    #print(\"Start_sim\",path_data_base,path_non_redundant,file_name,file_path_sim)\n",
    "    #list_terms=read_cvs_by_pands(path_data_source,sub_path_data_source_tfidf+file_name,0,0)#.index\n",
    "    list_terms=list(read_cvs_by_pands(path_data_base,path_non_redundant+file_name,None,0))\n",
    "    #compare between the same file\n",
    "    #print(len(list_terms))\n",
    "    #index=1\n",
    "    #limt=5\n",
    "    for i in range(len(list_terms)):\n",
    "        \n",
    "        term=list_terms[i]\n",
    "        term_file=term.replace(\"/\", \"_\")\n",
    "        #print(term)\n",
    "        is_term_new=False\n",
    "        if not is_file_exist(file_path_sim,file_path_sim+term_file+\".csv\"): # if !Fale this new term\n",
    "                #print(\" not term\",term)\n",
    "                #create_file(file_path_sim,term+\".csv\")  \n",
    "                is_term_new=True\n",
    "\n",
    "        for term_next in list_terms[i:]:\n",
    "            #print(\"terrm\",term,\"term_next\",term_next)\n",
    "\n",
    "            is_term_next_new=False\n",
    "\n",
    "            term_next_file=term_next.replace(\"/\", \"_\")\n",
    "\n",
    "            if not is_file_exist(file_path_sim,file_path_sim+term_next_file+\".csv\"): #next term is new\n",
    "                    #print(\" not termin\",list_terms[index])\n",
    "                    #create_file(file_path_sim,term_next+\".csv\")\n",
    "                    is_term_next_new=True\n",
    "\n",
    "            #print(term,is_term_new,term_next,is_term_next_new)\n",
    "            sim=0\n",
    "            list_term=[]\n",
    "            list_term_next=[]\n",
    "            if is_term_new or is_term_next_new:\n",
    "                sim=similarity_by_infocontent(term, term_next, 'res')\n",
    "                #print(term,term_next,\"sim\",sim)\n",
    "                if sim <1:\n",
    "                    sim=0\n",
    "                if sim !=0:\n",
    "                    list_term=[term,sim]\n",
    "                    list_term_next=[term_next,sim]\n",
    "\n",
    "            #print(list_term)\n",
    "            #print(list_term_next)\n",
    "            if sim !=0:\n",
    "                if term != term_next:\n",
    "                    #print(\"term != term_next\")\n",
    "                    if is_term_new:  \n",
    "                        #print(\"             is_term_new\",term)\n",
    "                        #print(\"is_term_new\",file_path_sim+term_file+\".csv\")\n",
    "                        create_file(file_path_sim,term_file+\".csv\")\n",
    "                        add_row_csv(file_path_sim,term_file+\".csv\",list_term)#add the same sim\n",
    "                        add_row_csv(file_path_sim,term_file+\".csv\",list_term_next)#add the next sim\n",
    "                        is_term_new=False\n",
    "                    else:\n",
    "                        #print(\"             is_term_old\",term)\n",
    "                        add_row_csv(file_path_sim,term_file+\".csv\",list_term_next)#add the next sim\n",
    "\n",
    "\n",
    "                    if is_term_next_new:\n",
    "                        #print(\"             is_term_next_new\",term_next)\n",
    "\n",
    "                        sim_nex=similarity_by_infocontent(term_next, term_next, 'res')\n",
    "                        #print(\"is_term_next_new \",file_path_sim,\" term_next\",term_next_file+\".csv\")\n",
    "                        create_file(file_path_sim,term_next_file+\".csv\")\n",
    "                        list_next=[term_next,sim_nex]\n",
    "                        add_row_csv(file_path_sim,term_next_file+\".csv\",list_next)#add term to next\n",
    "                        add_row_csv(file_path_sim,term_next_file+\".csv\",list_term)#add term to next\n",
    "                        is_term_next_new=False\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        #print(\"              is_term_next_old\",term_next,sim)\n",
    "                        add_row_csv(file_path_sim,term_next_file+\".csv\",list_term)#add term to next\n",
    "                else:\n",
    "                    #print(\"term == term_next\",term,term_next,sim,list_term)\n",
    "                    if is_term_new:  \n",
    "                        #print(\"            is_term_new\",term,sim,list_term)\n",
    "                        #print(\"else\",file_path_sim+term_file+\".csv\")\n",
    "                        create_file(file_path_sim,term_file+\".csv\")\n",
    "                        add_row_csv(file_path_sim,term_file+\".csv\",list_term)#add the same sim\n",
    "                        is_term_new=False\n",
    "\n",
    "\n",
    "        #print(\"finesed\")\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#list_terms=list(read_cvs_by_pands(path_data_base,path_non_redundant+\"cscs5.csv\",None,0))\n",
    "#list_terms=list(read_cvs_by_pands(path_data_base,path_non_redundant+\"cscs5.csv\",None,0))\n",
    "#list_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sim_terms_one_file(path_data_base,path_non_redundant,\"cs0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sim_terms_previous_file(path_data_base,path_non_redundant,file_name):\n",
    "    #print(\"sim_terms_previous_file\",path_data_base,path_non_redundant,file_name,file_path_sim)\n",
    "    \n",
    "    previous_file_names_list=read_last_file_list(path_data_base+path_non_redundant,\".csv\")\n",
    "    \n",
    "    #print(\"previous_file_names_list\",previous_file_names_list)\n",
    "    #list_terms=read_cvs_by_pands(path_data_source,sub_path_data_source_tfidf+file_name,0,0)#.index\n",
    "    list_terms=list(read_cvs_by_pands(path_data_base,path_non_redundant+file_name,None,0))\n",
    "    for previous_file_name in previous_file_names_list:\n",
    "        #print(previous_file_name)\n",
    "        pure_file_name=full_name_file(previous_file_name)\n",
    "        if pure_file_name !=file_name:\n",
    "            #print(pure_file_name)\n",
    "            list_terms_others=list(read_cvs_by_pands(path_data_base,path_non_redundant+pure_file_name,None,0))\n",
    "\n",
    "            #compare between the same file\n",
    "            #print(len(list_terms))\n",
    "            #index=1\n",
    "            limt=5\n",
    "            for i in range(len(list_terms)):\n",
    "                term=list_terms[i]\n",
    "                term_file=term.replace(\"/\", \"_\")\n",
    "                #print(term)\n",
    "                is_term_new=False\n",
    "                if not is_file_exist(file_path_sim,file_path_sim+term_file+\".csv\"): # if !Fale this new term\n",
    "                        #print(\" not term\",term)\n",
    "                        #create_file(file_path_sim,term+\".csv\")  \n",
    "                        is_term_new=True\n",
    "\n",
    "                #for term_next in list_terms[i:limt]:\n",
    "                for x in range(len(list_terms_others)):\n",
    "                    term_next=list_terms_others[x]\n",
    "                    term_next_file=term_next.replace(\"/\", \"_\")\n",
    "                    #print(\"terrm\",term,\"term_next\",term_next)\n",
    "\n",
    "                    is_term_next_new=False\n",
    "\n",
    "\n",
    "\n",
    "                    if not is_file_exist(file_path_sim,file_path_sim+term_next_file+\".csv\"): #next term is new\n",
    "                            #print(\" not termin\",list_terms[index])\n",
    "                            #create_file(file_path_sim,term_next+\".csv\")\n",
    "                            is_term_next_new=True\n",
    "\n",
    "                    #print(term,is_term_new,term_next,is_term_next_new)\n",
    "                    sim=0\n",
    "                    list_term=[]\n",
    "                    list_term_next=[]\n",
    "                    if is_term_new or is_term_next_new:\n",
    "                        sim=similarity_by_infocontent(term, term_next, 'res')\n",
    "                        #print(term,term_next,\"sim\",sim)\n",
    "                        if sim <1:\n",
    "                            sim=0\n",
    "                        if sim !=0:\n",
    "                            list_term=[term,sim]\n",
    "                            list_term_next=[term_next,sim]\n",
    "\n",
    "                    #print(list_term)\n",
    "                    #print(list_term_next)\n",
    "                    if sim !=0:\n",
    "                        if term != term_next:\n",
    "                            #print(\"term != term_next\")\n",
    "                            if is_term_new:  \n",
    "                                #print(\"             is_term_new\",term)\n",
    "                                #print(\"is_term_new\",file_path_sim+term_file+\".csv\")\n",
    "                                create_file(file_path_sim,term_file+\".csv\")\n",
    "                                add_row_csv(file_path_sim,term_file+\".csv\",list_term)#add the same sim\n",
    "                                add_row_csv(file_path_sim,term_file+\".csv\",list_term_next)#add the next sim\n",
    "                                is_term_new=False\n",
    "                            else:\n",
    "                                #print(\"             is_term_old\",term)\n",
    "                                add_row_csv(file_path_sim,term_file+\".csv\",list_term_next)#add the next sim\n",
    "\n",
    "\n",
    "                            if is_term_next_new:\n",
    "                                #print(\"             is_term_next_new\",term_next)\n",
    "\n",
    "                                sim_nex=similarity_by_infocontent(term_next, term_next, 'res')\n",
    "                                #print(\"is_term_next_new\",file_path_sim+term_next_file+\".csv\")\n",
    "                                create_file(file_path_sim,term_next_file+\".csv\")\n",
    "                                list_next=[term_next,sim_nex]\n",
    "                                add_row_csv(file_path_sim,term_next_file+\".csv\",list_next)#add term to next\n",
    "                                add_row_csv(file_path_sim,term_next_file+\".csv\",list_term)#add term to next\n",
    "                                is_term_next_new=False\n",
    "\n",
    "\n",
    "                            else:\n",
    "                                #print(\"              is_term_next_old\",term_next,sim)\n",
    "                                add_row_csv(file_path_sim,term_next_file+\".csv\",list_term)#add term to next\n",
    "                        else:\n",
    "                            #print(\"term == term_next\",term,term_next,sim,list_term)\n",
    "                            if is_term_new:  \n",
    "                                #print(\"            is_term_new\",term,sim,list_term)\n",
    "                                #print(\"else\",file_path_sim+term_file+\".csv\")\n",
    "                                create_file(file_path_sim,term_file+\".csv\")\n",
    "                                add_row_csv(file_path_sim,term_file+\".csv\",list_term)#add the same sim\n",
    "                                is_term_new=False\n",
    "\n",
    "\n",
    "                #print(\"finesed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load list word of current sub_word\n",
    "#start sim between word and next word\n",
    "# load next file from previous list \n",
    "##start sim between words in curent file and word in next file \n",
    "#store each comparison in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def gpu_full_process(filename,path_data_base,path_tf,path_non_redundant,path_idf,file_path_sim):\n",
    "    print(\"In gpu file_path_sim\",file_path_sim)\n",
    "    #index_file=0\n",
    "    #file_list_task=read_last_file_list(path_data_source+files_path_data_source,\".txt\")\n",
    "    #for i in range(len(file_list_task)):\n",
    "    #filename=file_list_task[i]\n",
    "    #print(filename)\n",
    "    if not isfile_empty(filename):\n",
    "        tf_file_name=file_to_LESK_TF(filename,path_data_base,path_tf)\n",
    "        if not isfile_empty(path_data_base+path_tf+tf_file_name):\n",
    "            print(\"finesed tf\",tf_file_name)\n",
    "            remove_redundant(tf_file_name,path_data_base,path_tf,path_non_redundant)\n",
    "            print(\"finesed remove_redundant\",tf_file_name)\n",
    "            Idf(path_data_base,path_tf,tf_file_name,path_idf)\n",
    "            print(\"finesed idf\",tf_file_name)\n",
    "            '''if not isfile_empty(path_data_base+path_non_redundant+tf_file_name):\n",
    "                sim_terms_one_file(path_data_base,path_non_redundant,tf_file_name)\n",
    "                print(\"finesed sim\",tf_file_name)\n",
    "                sim_terms_previous_file(path_data_base,path_non_redundant,tf_file_name)\n",
    "                print(\"sim_terms_previous_file\")\n",
    "            else:\n",
    "                print(\"Empty redundant\",tf_file_name)'''\n",
    "\n",
    "        else:\n",
    "            print(\"Empty tf\",tf_file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#isfile_empty(\"/home/fsg/Desktop/files/dd.txt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "    #return [x.name for x in local_device_protos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def distrupited_task_gpu(no_task,total_no_gpu):\n",
    "    \n",
    "    return no_task%total_no_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d55c8dff2cb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m config = tf.ConfigProto(device_count={\"GPU\": int(all_gpus),\"CPU\":1},\n\u001b[0m\u001b[1;32m      2\u001b[0m                         \u001b[0mallow_soft_placement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         \u001b[0minter_op_parallelism_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                         \u001b[0mintra_op_parallelism_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                         \u001b[0muse_per_session_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "'''config = tf.ConfigProto(device_count={\"GPU\": int(all_gpus),\"CPU\":1},\n",
    "                        allow_soft_placement=True,\n",
    "                        inter_op_parallelism_threads=16,\n",
    "                        intra_op_parallelism_threads=16,\n",
    "                        use_per_session_threads=True,\n",
    "                        log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True'''\n",
    "\n",
    "config = tf.ConfigProto(device_count={processor.upper():int(all_gpus)},\n",
    "                        allow_soft_placement=True,\n",
    "                        inter_op_parallelism_threads=1,\n",
    "                        intra_op_parallelism_threads=1,\n",
    "                        use_per_session_threads=True,\n",
    "                        log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "#import threading\n",
    "import timeit\n",
    "from tensorflow.python.client import timeline\n",
    "#start_gpu_memory=gpu_memory_map(gpuN+\"gpu_befor_session_memory.txt\",gpuN+\"gpu_befor_session_out.txt\")\n",
    "\n",
    "#save_txt(str(start_gpu_memory),gpuN+\"gpu_befor_session_memory_map.txt\")\n",
    "\n",
    "#print(\"gpu_memory_map\",start_gpu_memory)\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    print(\"************Started Session Main Process*************\")\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #start_gpu_memory=gpu_memory_map(gpuN+\"gpu_after_start_session_memory.txt\",gpuN+\"gpu_after_start_session_out.txt\")\n",
    "    gpu_name='/'+processor+':0'#/gpu:'+str(gpu_no)    \n",
    "    #print(\"gpu_memory_map_after_session\",start_gpu_memory)       \n",
    "    with tf.device(gpu_name):#'/gpu:0'):\n",
    "        print(\"************Started Session CPU *************\")\n",
    "        #start = timeit.default_timer()\n",
    "        #file_list_task=read_last_file_list(path_data_source+files_path_data_source,\".txt\")\n",
    "        #print(\"file_list_task\",file_list_task)\n",
    "        #sub_file_list_task=sub_list_file(file_list_task,int(all_gpus))[int(gpuN)]\n",
    "        #print(\"sub_file_list_task\",sub_file_list_task)\n",
    "        #gpu_no=distrupited_task_gpu(int(task_no),int(all_gpus))\n",
    "        #gpu_name='/gpu:'+str(gpu_no)\n",
    "        \n",
    "        #with tf.device(gpu_name):\n",
    "        print(\"************Started Session GPU *************\")\n",
    "        #print(\"sublist\",sub_file_list_task,\"gpu_name\",gpu_name)\n",
    "        #print(\"task_no\",task_no,\"gpu_name\",gpu_name)\n",
    "        #print(\"In session\",file_path_sim)\n",
    "        #for task_no in range(3):\n",
    "        task_no=str(task_no)\n",
    "        filename=dataset+task_no+\".txt\"#\"split\"+task_no+\".txt\" #\"cs\"+task_no+\".txt\" \n",
    "        totals_filename=file_path+filename\n",
    "        gpu_full_process(totals_filename,path_data_base,path_tf,path_non_redundant,path_idf,file_path_sim)\n",
    "            #Your statements here\n",
    "\n",
    "        #stop = timeit.default_timer()\n",
    "\n",
    "        #print (\"tttttttttttttt\",stop - start )\n",
    "    \n",
    "   "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
