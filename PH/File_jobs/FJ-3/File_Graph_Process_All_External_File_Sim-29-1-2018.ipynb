{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/python3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import csv\n",
    "from tensorflow.python.client import timeline\n",
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import Counter\n",
    "import threading\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import digits\n",
    "import os\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "all_gpus =\"1\"#sys.argv[2]\n",
    "#cuda=\"0,1\"#sys.argv[3]\n",
    "processor=sys.argv[4]\n",
    "dataset=sys.argv[5]\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "\n",
    "#path_data_source=dir_path+\"/data/data_source/wiki/wikipedia2text/\"\n",
    "dir_path =\"/home/helwan003u1\"\n",
    "path_data_source=dir_path+\"/data/data_source/\"\n",
    "\n",
    "#path_data_source=\"/home/fsg/Desktop/\"\n",
    "path_data_base=dir_path+\"/data/database/\"+processor+dataset+\"/\"\n",
    "#path_data_base=dir_path+\"/data/database/csv/\"\n",
    "#path_data_base=dir_path+\"/data/database/csv2/\"\n",
    "files_path_data_source=dataset+\"/\"\n",
    "\n",
    "#path_data_base=\"/home/fsg/Desktop/csv-fatma/\"\n",
    "\n",
    "\n",
    "#files_path_data_source=\"files/\"\n",
    "\n",
    "#files_path_data_source=\"mincorpus/\"#\"corpus/\"#\"demo/\"\n",
    "\n",
    "#sub_path_data_source=\"small/\"\n",
    "\n",
    "file_path=path_data_source+files_path_data_source\n",
    "\n",
    "file_names = [os.path.join(file_path, f) \n",
    "                      for f in os.listdir(file_path) \n",
    "                      if f.endswith(\".txt\")]\n",
    "\n",
    "\n",
    "path_tf=\"sub_tf/\"\n",
    "path_idf=\"sub_idf/\"\n",
    "path_tfidf=\"sub_tfidf/\"\n",
    "path_non_redundant=\"sub_word_tf/\"\n",
    "path_sim=\"semantics/sim/\"\n",
    "\n",
    "file_path_tf=path_data_base+path_tf\n",
    "file_path_idf=path_data_base+path_idf\n",
    "file_path_tfidf=path_data_base+path_tfidf\n",
    "file_path_non_redundant=path_data_base+path_non_redundant\n",
    "file_path_sim=path_data_base+path_sim\n",
    "\n",
    "file_names_tf = [os.path.join(file_path_tf, f) \n",
    "                      for f in os.listdir(file_path_tf) \n",
    "                      if f.endswith(\".csv\")]\n",
    "file_names_idf = [os.path.join(file_path_idf, f) \n",
    "                      for f in os.listdir(file_path_idf) \n",
    "                      if f.endswith(\".csv\")]\n",
    "\n",
    "\n",
    "file_names_tfidf = [os.path.join(file_path_tfidf, f) \n",
    "                      for f in os.listdir(file_path_tfidf) \n",
    "                      if f.endswith(\".csv\")]\n",
    "\n",
    "\n",
    "file_names_non_redundant = [os.path.join(file_path_non_redundant, f) \n",
    "                      for f in os.listdir(file_path_non_redundant) \n",
    "                      if f.endswith(\".csv\")]\n",
    "\n",
    "\n",
    "file_names_sim = [os.path.join(file_path_sim, f) \n",
    "                      for f in os.listdir(file_path_sim) \n",
    "                      if f.endswith(\".csv\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess, re, os, sys #https://github.com/yaroslavvb/stuff/blob/master/notebook_util.py\n",
    "def run_command(cmd):\n",
    "    \"\"\"Run command, return output as string.\"\"\"\n",
    "    \n",
    "    output = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True).communicate()[0]\n",
    "    return output.decode(\"ascii\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_available_gpus():\n",
    "    \"\"\"Returns list of available GPU ids.\"\"\"\n",
    "    \n",
    "    output = run_command(\"nvidia-smi -L\")\n",
    "    # lines of the form GPU 0: TITAN X\n",
    "    gpu_regex = re.compile(r\"GPU (?P<gpu_id>\\d+):\")\n",
    "    result = []\n",
    "    for line in output.strip().split(\"\\n\"):\n",
    "        m = gpu_regex.match(line)\n",
    "        assert m, \"Couldnt parse \"+line\n",
    "        result.append(int(m.group(\"gpu_id\")))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_txt(txt,file):\n",
    "    text_file = open(file, \"w\")\n",
    "    text_file.write(txt)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gpu_memory_map(gpu_memory_file,gpu_output_file):\n",
    "    \"\"\"Returns map of GPU id to memory allocated on that GPU.\"\"\"\n",
    "\n",
    "    output = run_command(\"nvidia-smi\")\n",
    "    save_txt(output,gpu_output_file)\n",
    "    #print(\"nvidia-smi\",output)\n",
    "    gpu_output = output[output.find(\"GPU Memory\"):]\n",
    "    #print(\"GPU Memory\",gpu_output)\n",
    "    save_txt(gpu_output,gpu_memory_file)\n",
    "   \n",
    "    # lines of the form\n",
    "    # |    0      8734    C   python                                       11705MiB |\n",
    "    memory_regex = re.compile(r\"[|]\\s+?(?P<gpu_id>\\d+)\\D+?(?P<pid>\\d+).+[ ](?P<gpu_memory>\\d+)MiB\")\n",
    "    #print(\"memory_regex\",memory_regex)\n",
    "    rows = gpu_output.split(\"\\n\")\n",
    "    #print(\"rows\",rows)\n",
    "    result = {gpu_id: 0 for gpu_id in list_available_gpus()}\n",
    "    #print(\"result\",result)\n",
    "    for row in gpu_output.split(\"\\n\"):\n",
    "        m = memory_regex.search(row)\n",
    "        if not m:\n",
    "            continue\n",
    "        gpu_id = int(m.group(\"gpu_id\"))\n",
    "        gpu_memory = int(m.group(\"gpu_memory\"))\n",
    "        result[gpu_id] += gpu_memory\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_cvs_by_pands(path_database,file_database,index_col, header):\n",
    "    import csv\n",
    "    import pandas as pd\n",
    "    df=pd.read_csv(path_database+file_database,index_col=index_col,header=header)\n",
    "    \n",
    "    return df#pd.read_csv(path_database+file_database,index_col=index_col,header=header)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def isfile_empty(file_path_name):\n",
    "    f=open(file_path_name, 'r',encoding='utf-8') \n",
    "    is_blank = len(f.read().strip()) == 0\n",
    "    return is_blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#isfile_empty(\"/home/fsg/Desktop/cssplit225.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_cvs_by_pands(path_database,file_database,header,data_rows):\n",
    "    import csv\n",
    "    import pandas as pd\n",
    "    csv_df=pd.DataFrame(data_rows,columns=header ) \n",
    "    csv_df.to_csv(path_database+file_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_sord_tf_file(path_data_source,sub_path_data_source_tf,i):\n",
    "    #df=read_cvs_by_pands(path_data_source,sub_path_data_source_tf+\"cs\"+str(i)+\".csv\",0,header=0)\n",
    "    #print(path_data_source+sub_path_data_source_tf+i)\n",
    "    df=read_cvs_by_pands(path_data_source,sub_path_data_source_tf+i,0,header=0)\n",
    "    #ee.T.sort_index(inplace=True)\n",
    "    df = df.T.sort_index()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_sord_idf_file(path_data_source,sub_path_data_source_idf,i):\n",
    "    #df=read_cvs_by_pands(path_data_source,sub_path_data_source_idf+\"cs\"+str(i)+\".csv\",0,header=0)\n",
    "    df=read_cvs_by_pands(path_data_source,sub_path_data_source_idf+i,0,header=0)\n",
    "    #ee.T.sort_index(inplace=True)\n",
    "    df = df.sort_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "Write Excell sheet\n",
    "'''\n",
    "def save_file_to_database(data_rows,path_database,file_databbase,header_list):\n",
    "    import csv\n",
    "    outfile = open(path_database+file_databbase,'w')\n",
    "    writer=csv.writer(outfile)\n",
    "    #header_list=['uuid','paragraph','doc_id']\n",
    "    i=0\n",
    "    for line in data_rows:\n",
    "        row=[i,line,'paragraph no.'+str(i)]\n",
    "        if i==0:\n",
    "            \n",
    "            writer.writerow(header_list)\n",
    "            writer.writerow(row)\n",
    "        else:\n",
    "            ##print('ff')\n",
    "            writer.writerow(row)\n",
    "        i+= 1\n",
    "        #outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Read Excell sheet\n",
    "'''\n",
    "def read_text_from_database(path_database,file_databbase):\n",
    "    import csv\n",
    "    queue_paragraph=[]\n",
    "    #f = open(sys.argv[1], 'rt')\n",
    "    outfile = open(path_database+file_databbase,'rt')\n",
    "    try:\n",
    "                \n",
    "        reader=csv.reader(outfile)\n",
    "        for row in reader:\n",
    "            queue_paragraph.append(row)\n",
    "            ##print (row)\n",
    "    finally:\n",
    "        ##print (\"row\")\n",
    "        outfile.close()\n",
    "        \n",
    "    return queue_paragraph\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_row_csv(path_database,idf,list_data):\n",
    "    import csv\n",
    "    with open(path_database+idf, 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(list_data)\n",
    "        f.close()\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_file(path_data_source,file_name):\n",
    "    import csv\n",
    "    outfile = open(path_data_source+file_name,'w')\n",
    "    writer=csv.writer(outfile)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_file_exist(file_path,file_name):\n",
    "    file_names = [os.path.join(file_path, f) \n",
    "                      for f in os.listdir(file_path) \n",
    "                      if f.endswith(\".csv\")]\n",
    "    #print(file_names)\n",
    "    if file_name in file_names:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save pragraphs to files\n",
    "def write_file(pragraph,num_pragraph,path):\n",
    "    file = open(path+str(num_pragraph)+\".txt\",\"w\") \n",
    " \n",
    "    file.write(pragraph) \n",
    "    \n",
    "    file.close() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create sub dataset\n",
    "def sub_dataset(path_data_source,data_source):\n",
    "    pragraphs=txt_pragraphs(read_file(path_data_source+data_source))\n",
    "    counter=0\n",
    "    for pragraph in pragraphs:\n",
    "        ##print('pragraph no ',counter)\n",
    "        write_file(pragraph,counter,sub_path_data_source)\n",
    "        counter +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(str):\n",
    "    file = open(str,'r',encoding='utf-8')\n",
    "    txt=file.read()\n",
    "    ##print(txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def txt_pragraphs(str):\n",
    "    pragraphs = str.split(\"\\n\\n\")\n",
    "    return pragraphs\n",
    "#pragraphs=txt_pragraphs(txt)\n",
    "#type(pragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pragraph_to_setnences(str):\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    return sent_tokenize(str)\n",
    "#setnences=pragraph_to_setnences(pragraphs[n_pragraph])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_stop_words = ['the', 'that', 'to', 'as', 'there', 'has', 'and', 'or', 'is', 'not', 'a', 'of', 'but', 'in', 'by', 'on', 'are', 'it', 'if','what','where','how','when']\n",
    "new_stop_words2=['--','i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now','even','until','then','must']\n",
    "numbers=[1,2,3,4,5,6,7,8,9]\n",
    "#stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "def remove_stopword_sentences(sent):\n",
    "    import nltk\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    import time\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    from string import digits\n",
    "    from nltk.corpus.reader.wordnet import WordNetError\n",
    "    import sys\n",
    "    list_word=[]\n",
    "\n",
    "    try:\n",
    "        tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "    \n",
    "        words=tokenizer.tokenize(sent)\n",
    "    \n",
    "        english_stops = set(stopwords.words('english'))\n",
    "        #stems=[]\n",
    "        \n",
    "        list_word=[word for word in words if word.lower() not in english_stops and word.lower() not in new_stop_words and word.lower() not in new_stop_words2 and  not word.lower().isdigit() and word.lower() not in digits and word.lower() not in  numbers]\n",
    "    \n",
    "    #for word in list_word:\n",
    "        #stems.append(stem(word))\n",
    "        #stems.append(PorterStemmer().stem(word))\n",
    "        #stems.append(stemmer.stem(word))\n",
    "        #stems.append(stemmer.stem(\"computer\"))\n",
    "        #stems.append(word)\n",
    "    except WordNetError as e:\n",
    "        print(\"WordNetError on concept {}: {}\".format(\"remove_stopword_sentences: \",e))\n",
    "    except AttributeError as e:\n",
    "        print(\"Attribute error on concept {}: {}\".format(\"remove_stopword_sentences: \", e))\n",
    "    except:\n",
    "        print(\"Unexpected error on concept {}: {}\".format(\"remove_stopword_sentences: \", sys.exc_info()[0]))\n",
    "    \n",
    "    return list_word#stems#(stem(setem_word for setem_word in  ([word for word in words if word not in english_stops and word not in new_stop_words])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove_stopword_sentences(\"//jdnf dfd \\ dfjfd-eee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_list_sentece(pragraph):\n",
    "    words_list=[]\n",
    "    setnences=pragraph_to_setnences(pragraph)\n",
    "    for indexs in range(len(setnences)):    \n",
    "        ##print(\"Sentence No. \",indexs,\": \",setnences[indexs],\"\\n\")\n",
    "        words=remove_stopword_sentences(setnences[indexs])\n",
    "        wordsent=''\n",
    "        for index in range(len(words)):\n",
    "            wordsent+=' '+words[index]\n",
    "            ##print(\"wordsent:\",wordsent)\n",
    "            \n",
    "        words_list.append(wordsent)\n",
    "        #count = Counter(words)\n",
    "        ##print(\"wordsent:\",wordsent)\n",
    "        ##print(\" word:\",words)\n",
    "    ##print(words_list)\n",
    "    return words_list\n",
    "\n",
    "#corpus=word_list_sentece(pragraphs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "this function for compute lesk for each word(list of word) in sentence\n",
    "'''\n",
    "def lesk_words_sentence(words,sentence):\n",
    "    import nltk\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    from nltk.wsd import lesk\n",
    "    lesks= []\n",
    "    for word in words:\n",
    "        if lesk(sentence,word, 'n') is not None:\n",
    "            lesks.append(lesk(sentence,word, 'n'))\n",
    "            ##print(\"Word is: \",word,\"\\n LESK: \",lesk(sentence,word, 'n'),\"\\n Sentence: \",sentence )\n",
    "        \n",
    "    return lesks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "this function for compute lesk of word in sentence\n",
    "'''\n",
    "\n",
    "def lesk_word_sentence(sentence,word):\n",
    "    import nltk\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    from nltk.wsd import lesk\n",
    "    from nltk.corpus.reader.wordnet import WordNetError\n",
    "    import sys\n",
    "    disambiguated=''\n",
    "    ##print(type(disambiguated))\n",
    "    try:\n",
    "        \n",
    "    #lesks= []\n",
    "    #for word in words:\n",
    "    #disambiguated=lesk(context_sentence=sentence, ambiguous_word=word)\n",
    "    \n",
    "        disambiguated=lesk(sentence,word, 'n')\n",
    "        ##print(type(disambiguated))\n",
    "    ##print(disambiguated)\n",
    "    #if disambiguated is not None:\n",
    "        #lesk_synset=disambiguated\n",
    "    #else:\n",
    "    #lesk_synset=0\n",
    "    ##print(\"Word is: \",word,\"\\n LESK: \",lesk(sentence,word, 'n'),\"\\n Sentence: \",sentence )\n",
    "    except WordNetError as e:\n",
    "        print(\"WordNetError on concept {}: {}\".format(\"lesk_word_sentence: \",e))\n",
    "    except AttributeError as e:\n",
    "        print(\"Attribute error on concept {}: {}\".format(\"lesk_word_sentence: \", e))\n",
    "    except:\n",
    "        print(\"Unexpected error on concept {}: {}\".format(\"lesk_word_sentence: \", sys.exc_info()[0]))    \n",
    "    return disambiguated\n",
    "\n",
    "#lesk(\"Computer science is a discipline that spans theory and practice\",\"science\")\n",
    "\n",
    "#sent = 'people should be able to marry a person of their choice'.split()\n",
    "#lesk(sent, 'able')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def idf_df(df,D,base):\n",
    "    #[7/df['0']]\n",
    "    y = [log_idf(D,x,base) for x in df['0']]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_idf(D,d,base):\n",
    "    return math.log((D/d), base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idf_one(path_data_source,sub_path_data_source_tf,sub_path_data_source_idf,i,sub_path_data_source_tfidf):\n",
    "    df_tf=read_sord_tf_file(path_data_source,sub_path_data_source_tf,i)\n",
    "    df_idf=read_sord_idf_file(path_data_source,sub_path_data_source_idf,i)\n",
    "\n",
    "    if len(df_idf) != 0:\n",
    "        idf=idf_df(df_idf,len(df_idf),10)\n",
    "\n",
    "        full_tfidf=[]\n",
    "        for index in range(len(idf)):\n",
    "            ##print(index)\n",
    "            tfidf=df_tf[0][index]*idf[index]\n",
    "            full_tfidf.append(tfidf)\n",
    "\n",
    "\n",
    "    df_tf_idf=pd.DataFrame(full_tfidf)\n",
    "    df_tf_idf.index=df_tf.index\n",
    "    #df_tf_idf.to_csv(path_data_source+sub_path_data_source_tfidf+\"cs\"+str(i)+\".csv\")\n",
    "    df_tf_idf.to_csv(path_data_source+sub_path_data_source_tfidf+i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def full_name_file(full_name_path):#like cs.csv\n",
    "    d=full_name_path.split(\"/\")\n",
    "    ##print(d)\n",
    "    name=d[len(d)-1]#.split(\".\")\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name_file(full_name_path): #like cs\n",
    "    d=full_name_path.split(\"/\")\n",
    "    ##print(d)\n",
    "    name=d[len(d)-1].split(\".\")\n",
    "    return name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#name_file(\"/data/database/csv/split156.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "merge two dictionary \n",
    "d={'a': 1, 'c': 3, 'k': 5}\n",
    "d1={'g': 1, 'c': 3, 'b': 5}\n",
    "like merge(d, d1,lambda x,y: x+1)\n",
    "{'a': 1, 'b': 5, 'c': 4, 'g': 1, 'k': 5}\n",
    "'''\n",
    "\n",
    "def merge(d1, d2, merge_fn=lambda x,y:y):\n",
    "    \"\"\"\n",
    "    Merges two dictionaries, non-destructively, combining \n",
    "    values on duplicate keys as defined by the optional merge\n",
    "    function.  The default behavior replaces the values in d1\n",
    "    with corresponding values in d2.  (There is no other generally\n",
    "    applicable merge strategy, but often you'll have homogeneous \n",
    "    types in your dicts, so specifying a merge technique can be \n",
    "    valuable.)\n",
    "\n",
    "    Examples:\n",
    "\n",
    "    >>> d1\n",
    "    {'a': 1, 'c': 3, 'b': 2}\n",
    "    >>> merge(d1, d1)\n",
    "    {'a': 1, 'c': 3, 'b': 2}\n",
    "    >>> merge(d1, d1, lambda x,y: x+y)\n",
    "    {'a': 2, 'c': 6, 'b': 4}\n",
    "\n",
    "    \"\"\"\n",
    "    result = dict(d1)\n",
    "    for k,v in d2.items():\n",
    "        if k in result:\n",
    "            result[k] = merge_fn(result[k], v)\n",
    "            ##print(k)\n",
    "        #else:\n",
    "            #result[k] = v\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def match_lists(list_one,list_two):\n",
    "    dic_one=list_to_dict_one(list_one)\n",
    "    dic_two=list_to_dict_one(list_two)\n",
    "    return merge(dic_one, dic_two,lambda x,y: x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert list to dic has value 1\n",
    "def list_to_dict_one(my_list):\n",
    "    my_dict = {k: 1 for k in my_list} \n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Write Excell sheet\n",
    "'''\n",
    "def save_list_to_csv(data_rows,path_data_base,path_file,file_name):\n",
    "    import csv\n",
    "    outfile = open(path_data_base+path_file+file_name,'w')\n",
    "    writer=csv.writer(outfile)\n",
    "    \n",
    "    writer.writerow(data_rows)\n",
    "     \n",
    "    outfile.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def match_file_files(one_file,list_files):\n",
    "    path_database=dir_path+\"/data/database/csv/\"\n",
    "    path_sub_idf=\"sub_idf/\"\n",
    "    \n",
    "    word_list_one_file=csv_to_list(one_file)\n",
    "    ##print(type(word_list_one_file))\n",
    "    dict_one_word_list=list_to_dict_one(word_list_one_file[0])\n",
    "    ##print(dict_one_word_list)\n",
    "   \n",
    "    for i in range(len(list_files)):  \n",
    "        \n",
    "        ##print(\"***************\",i,\"******************\")\n",
    "        filename=list_files[i]\n",
    "        if one_file != filename:\n",
    "            ##print(name_file(filename))\n",
    "            word_list=csv_to_list(filename)\n",
    "            dict_word_list=list_to_dict_one(word_list[0])\n",
    "            ##print(dict_word_list)\n",
    "            dict_one_word_list=merge(dict_one_word_list, dict_word_list,lambda dict_one_word_list,dict_word_list:dict_one_word_list+1)\n",
    "            ##print(dict_one_word_list)\n",
    "        #else:\n",
    "            ##print(\"equal\")\n",
    "    write_cvs_by_pands(path_database+path_sub_idf,name_file(one_file)+'.csv',dict_one_word_list)\n",
    "    ##print('\\n',merg_dict.keys())\n",
    "    return dict_one_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def file_to_LESK_TF(filename,path_database,path_tf): \n",
    "    path_database=path_database#dir_path+\"/data/database/csv/\"\n",
    "    #path_sub_tfidf=path_sub_tfidf#\"sub_word_tf/\"\n",
    "    #path_full_tfidf=path_full_tfidf#\"full_word_tf/\"\n",
    "    path_tf=path_tf#\"sub_tf/\"\n",
    "    #TF_File=\"TF-\"\n",
    "    #TF_Full=\"TF-Full.csv\"\n",
    "\n",
    "    #for i in range(len(file_list_task)):    \n",
    "    #with tf.Session(config=config) as sess:\n",
    "    #index_paragraph=0\n",
    "    col=1\n",
    "\n",
    "    #index_file=0\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    ##print(file_names)\n",
    "\n",
    "    #for filename in file_names:\n",
    "        ##print(\"index_file\",str(index_file))\n",
    "    word_file_fatma=[]\n",
    "    #filename=file_list_task[i]\n",
    "    with open(filename,encoding='utf-8') as inf:\n",
    "            ##print(\"tpe\",type(inf))\n",
    "        txt=inf.read()\n",
    "\n",
    "        paragraph_list=txt_pragraphs(txt)   \n",
    "\n",
    "\n",
    "        for paragraph in paragraph_list: #get pragraphs(documents) from DB\n",
    "                ##print(\"Pragraph type \",type(paragraph))\n",
    "\n",
    "\n",
    "            #if index_paragraph ==0:\n",
    "                #index_paragraph += 1\n",
    "            #else:\n",
    "\n",
    "            setnences=pragraph_to_setnences(paragraph)#partitions paragraph to sentence\n",
    "\n",
    "\n",
    "            for setnence in setnences:\n",
    "                        ##print(\"  \",setnence)                            \n",
    "\n",
    "                words=remove_stopword_sentences(setnence)#remove stop words and noise\n",
    "                #try:\n",
    "\n",
    "                for word in words:\n",
    "                    try:\n",
    "\n",
    "                            lesk=lesk_word_sentence(setnence,word)#get LESK of word in sentence\n",
    "\n",
    "\n",
    "                            #paragraph_word.append(word_sentence)\n",
    "\n",
    "                            if lesk is not None:\n",
    "                            ##print(\"type of lesk in words\",type(lesk),lesk)\n",
    "\n",
    "                                word_file_fatma.append(lesk.name())\n",
    "\n",
    "                    except WordNetError as e:\n",
    "                            print(\"WordNetError on concept {}:{}\".format(\"My model \"+word+\" \"+lesk,e))\n",
    "                    except AttributeError as e:\n",
    "                            print(\"Attribute error on concept {}:{}\".format(\"My model \"+word+\" \"+lesk,e))\n",
    "                    except:\n",
    "                            print(\"Unexpected error on concept {}:{}\".format(\"My model \"+word+\" \"+lesk,sys.exc_info()[0]))\n",
    "\n",
    "\n",
    "\n",
    "                '''////////////////END Sentence////////////////# '''\n",
    "\n",
    "\n",
    "            #write_cvs_by_pands(path_database,word_sentences_table,word_sentences_list,word_sentences_list_data)\n",
    "\n",
    "\n",
    "            '''////////////////END PARAGRAPH////////////////# '''\n",
    "\n",
    "    #write_cvs_by_pands(path_database,sentences_paragraph_table,sentences_paragraph_list,sentences_paragraph_list_data)\n",
    "\n",
    "    ##print(word_file_fatma)\n",
    "\n",
    "    word_file_Freq=Counter(word_file_fatma)\n",
    "    sum_count=sum(word_file_Freq.values())\n",
    "\n",
    "    ##print(type(word_file_Freq))\n",
    "    ##print(word_file_Freq)\n",
    "    #csv_df=pd.DataFrame([word_file_Freq],columns=word_file_Freq.keys() ) \n",
    "    freq=[]\n",
    "    for i in word_file_Freq.values():\n",
    "        c=i/sum_count\n",
    "        freq.append(c)\n",
    "    csv_df=pd.DataFrame([freq],columns=word_file_Freq.keys() ) \n",
    "\n",
    "    #Save TF file\n",
    "    #new_file_name=\"cs\"+name_file(filename)+\".csv\"\n",
    "    new_file_name=name_file(filename)+\".csv\"\n",
    "    csv_df.to_csv(path_database+path_tf+new_file_name)\n",
    "        # add to idf file \n",
    "\n",
    "    #full_list=[]\n",
    "    #full_list.insert(0,name_file(filename)) # to add name of file in the firest cell like cs1 or cs4\n",
    "    #full_list=full_list+list(word_file_Freq.keys())\n",
    "    # add to single\n",
    "\n",
    "    #add_row_csv(path_database+path_sub_tfidf,full_name_file(filename),list(word_file_Freq.keys()))\n",
    "    # add to total idf file \n",
    "    #add_row_csv(path_database+path_full_tfidf,TF_Full,list(word_file_Freq.keys()))\n",
    "    #index_file +=1\n",
    "    return new_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#with open(\"/home/fsg/Desktop/split0.txt\",encoding='utf-8') as inf:\n",
    "    #txt=inf.read()\n",
    "    #print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\"ff/sduy.2.2.01\".replace(\"/\", \"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non Redundant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1={'a': 1, 'b': 1, 'c': 1}\n",
    "d2={'d': 1, 'x': 1, 'b': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "d1={'a': 1, 'b': 1, 'c': 1}\n",
    "d2={'d': 1, 'x': 1, 'b': 1}\n",
    "result={'a': 1, 'c': 1}\n",
    "'''\n",
    "\n",
    "def dict_remove_redundant(dic1,dic2):\n",
    "    \n",
    "    dic1=merge(dic1, dic2,lambda dic1,dic2:dic1*0)\n",
    "    #print(\"\\n\")\n",
    "    #print(\"in remove\",len(dic1),dic1,\"\\n\")\n",
    "    dic1=dict((k,v) for k, v in dic1.items() if v)\n",
    "    #print(\"in remove 2\",len(dic1),dic1,\"\\n\")\n",
    "    return dic1\n",
    "\n",
    "#dict_remove_redundant(d1,d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dict_key_to_list(dic):#like dict_keys(['a', 'c'])\n",
    "    \n",
    "    return dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_last_file_list(file_path,extention):\n",
    "    \n",
    "    file_names = [os.path.join(file_path, f) \n",
    "                      for f in os.listdir(file_path) \n",
    "                      if f.endswith(extention) and not isfile_empty(file_path+f)]\n",
    "    return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def name_file_no(full_name_path): #like cs\n",
    "    import re\n",
    "    d=full_name_path.split(\"/\")\n",
    "    ##print(d)\n",
    "    name=d[len(d)-1].split(\".\")\n",
    "    #print(name[0])\n",
    "    name_no=re.split('(\\d+)',name[0])\n",
    "    #print(name_no)\n",
    "    return name_no[1]#name[0][5:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#name_file_no(\"/data/database/csv/cscs1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_last_file_list_sim_previouse(file_path,extention,no_task):\n",
    "    import os\n",
    "    #print(\"ggggggggggggggggggggggggggg \",int(name_file_no(f)),int(no_task))\n",
    "    file_names = [os.path.join(file_path, f) \n",
    "                      for f in os.listdir(file_path) \n",
    "                      if f.endswith(extention) and not isfile_empty(file_path+f) and int(name_file_no(f))<int(no_task)]\n",
    "    return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read_last_file_list_sim_previouse(path_data_base+path_non_redundant,\".csv\",\"1\")\n",
    "name_file_no(\"cscs1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_redundant(tf_file,path_data_base,path_tf,path_non_redundant):\n",
    "    print(\"remove_redundant\",tf_file)\n",
    "    list_tf=read_cvs_by_pands(path_data_base+path_tf,tf_file,0,0).keys()\n",
    "    dic_tf=list_to_dict_one(list_tf)\n",
    "    ##print(dic_tf)\n",
    "    \n",
    "    file_names_non_redundant=read_last_file_list(path_data_base+path_non_redundant,\".csv\")\n",
    "    ##print(\"file_names_non_redundant\",file_names_non_redundant)\n",
    "    for file_nonredun in file_names_non_redundant:\n",
    "        ##print(\"file_nonredun\")\n",
    "        pure_file_name=full_name_file(file_nonredun)\n",
    "        ##print(pure_file_name)\n",
    "        list_non=read_cvs_by_pands(file_path_non_redundant,pure_file_name,None,0)\n",
    "        \n",
    "        dic_non=list_to_dict_one(list_non)\n",
    "        #print(\"list_non\",dic_non)\n",
    "        dic_tf=dict_remove_redundant(dic_tf,dic_non)#///////////////////\n",
    "        #print(\"dic_tf\",len(dic_tf),dic_tf)\n",
    "    list_term=dict_key_to_list(dic_tf)\n",
    "    #print(\"list_term\",len(list_term))\n",
    "    \n",
    "    save_list_to_csv(list_term,path_data_base,path_non_redundant,tf_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " #remove_redundant(\"cs1.csv\",path_data_base,path_tf,path_non_redundant)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dict_IDF(dic1,dic2):\n",
    "    \n",
    "    dic3=merge(dic1, dic2,lambda dic1,dic2:dic1+1)\n",
    "    dic4=merge(dic2, dic1,lambda dic2,dic1:dic2+1)\n",
    "    \n",
    "    return dic3,dic3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#d1={'a': 1, 'b': 1, 'c': 1}\n",
    "def dict_to_DF(dic):\n",
    "    df=pd.DataFrame([dic])\n",
    "    return df\n",
    "\n",
    "#dict_to_DF(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_df_to_csv(df,path_database,sub_path,new_file_name):\n",
    "     df.to_csv(path_database+sub_path+new_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def magic(numList):         # [1,2,3]\n",
    "    s = map(str, numList)   # ['1','2','3']\n",
    "    s = ''.join(s)          # '123'\n",
    "    s = int(s)              # 123\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def df_to_dict(df):\n",
    "        \n",
    "    dic={}\n",
    "    keys=df.keys()\n",
    "    \n",
    "    values= df.T.values.tolist()\n",
    "    #print(len(values))\n",
    "    for i in range(len(keys)):\n",
    "        #print(keys[i])\n",
    "        dic[keys[i]]=magic(values[i])\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df=read_cvs_by_pands(path_data_base+path_idf,\"cs0.csv\",0,0)\n",
    "#df_to_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Idf(path_data_base,path_tf,tf_file_name,path_idf):\n",
    "    #tf_file_name=\"cs2.csv\"\n",
    "    list_tf=read_cvs_by_pands(path_data_base+path_tf,tf_file_name,0,0).keys()\n",
    "\n",
    "    dic_tf=list_to_dict_one(list_tf)\n",
    "    #print(dic_tf)\n",
    "\n",
    "    file_names_IDF=read_last_file_list(path_data_base+path_idf,\".csv\")\n",
    "\n",
    "\n",
    "    for file_IDF in file_names_IDF:\n",
    "            old_dic_tf_updated=dic_tf.copy()\n",
    "            ##print(\"file_nonredun\")\n",
    "            pure_file_name=full_name_file(file_IDF)\n",
    "            #print(\"pure_file_name\")\n",
    "            #open this file name as list with value\n",
    "            df_idf=read_cvs_by_pands(path_data_base+path_idf,pure_file_name,0,0)#.keys()\n",
    "            #print(\"df_idf \\n\")\n",
    "            #print(df_idf )\n",
    "            #convert list to dic dict_idf\n",
    "            dict_idf=df_to_dict(df_idf)\n",
    "            #merge dic_tf with dict_idf\n",
    "            dic_tf=merge(dic_tf, dict_idf,lambda dic_tf,dict_idf:dic_tf+1)\n",
    "            #if dic_tf changed \n",
    "            if old_dic_tf_updated != dic_tf:\n",
    "                #print(\"yeeeeeeeees\")\n",
    "                #merge dict_idf  with dic_tf \n",
    "                dict_idf=merge(dict_idf, dic_tf,lambda dict_idf,dic_tf:dict_idf+1)\n",
    "                #convert dict_idf to df_idf\n",
    "                df_idf_updated=dict_to_DF(dict_idf)\n",
    "                #save df_idf to csv\n",
    "                save_df_to_csv(df_idf_updated,path_data_base,path_idf,pure_file_name)\n",
    "\n",
    "    df_idf=dict_to_DF(dic_tf)\n",
    "    save_df_to_csv(df_idf,path_data_base,path_idf,tf_file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sub_list_file(file_list_task,all_gpus):\n",
    "    import math\n",
    "    sub_len=math.ceil(len(file_list_task)/all_gpus)\n",
    "    global_list_len=math.ceil(len(file_list_task)/sub_len)\n",
    "    \n",
    "    global_list=[]\n",
    "    index=0\n",
    "    for x in range(global_list_len):\n",
    "        sublist=[]\n",
    "        for i in range(sub_len):\n",
    "            if index < len(file_list_task):\n",
    "                sublist.append(file_list_task[index])\n",
    "                index +=1\n",
    "                \n",
    "        global_list.append(sublist)\n",
    "\n",
    "    return global_list\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic as wnic\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def similarity_by_infocontent(sense1, sense2, option):\n",
    "    #sense1=\"Synset('\"+sense1+\"')\"\n",
    "    #sense2=\"Synset('\"+sense2+\"')\"\n",
    "    #print(sense1,sense2)\n",
    "    sense1 = wn.synset(sense1)\n",
    "    sense2 = wn.synset(sense2)\n",
    "    #print(sense1,sense2)\n",
    "    \"\"\" Returns similarity scores by information content. \"\"\"\n",
    "    #if sense1.pos != sense2.pos: # infocontent sim can't do diff POS.\n",
    "        #return 0\n",
    "\n",
    "    info_contents = ['ic-bnc-add1.dat', 'ic-bnc-resnik-add1.dat', \n",
    "                     'ic-bnc-resnik.dat', 'ic-bnc.dat', \n",
    "\n",
    "                     'ic-brown-add1.dat', 'ic-brown-resnik-add1.dat', \n",
    "                     'ic-brown-resnik.dat', 'ic-brown.dat', \n",
    "\n",
    "                     'ic-semcor-add1.dat', 'ic-semcor.dat',\n",
    "\n",
    "                     'ic-semcorraw-add1.dat', 'ic-semcorraw-resnik-add1.dat', \n",
    "                     'ic-semcorraw-resnik.dat', 'ic-semcorraw.dat', \n",
    "\n",
    "                     'ic-shaks-add1.dat', 'ic-shaks-resnik.dat', \n",
    "                     'ic-shaks-resnink-add1.dat', 'ic-shaks.dat', \n",
    "\n",
    "                     'ic-treebank-add1.dat', 'ic-treebank-resnik-add1.dat', \n",
    "                     'ic-treebank-resnik.dat', 'ic-treebank.dat']\n",
    "\n",
    "    if option in ['res', 'resnik']:\n",
    "        #return wn.res_similarity(sense1, sense2, wnic.ic('ic-bnc-resnik-add1.dat'))\n",
    "        #print('simRe snik (c1,c2) = -log p(lso(c1,c2)) = IC(lso(c1,c2)')\n",
    "        return wn.res_similarity(sense1, sense2, wnic.ic('ic-treebank-resnik-add1.dat'))\n",
    "    #return min(wn.res_similarity(sense1, sense2, wnic.ic(ic)) \\\n",
    "    #             for ic in info_contents)\n",
    "\n",
    "    elif option in ['jcn', \"jiang-conrath\"]:\n",
    "        #return wn.jcn_similarity(sense1, sense2, wnic.ic('ic-bnc-add1.dat'))\n",
    "        #print('sim(jcn) (c1,c2 )= (IC(c1) + IC(c2 )) - 2IC(lso(c1,c2 ))')\n",
    "        return wn.jcn_similarity(sense1, sense2, wnic.ic('ic-treebank.dat'))\n",
    "\n",
    "    elif option in ['lin']:\n",
    "        #return wn.lin_similarity(sense1, sense2, wnic.ic('ic-bnc-add1.dat'))\n",
    "        #print('sim(lin) (c1,c2)=(2IC(lso(c1,c2 )))/(IC(c1)+IC(c2))')\n",
    "        return wn.lin_similarity(sense1, sense2, wnic.ic('ic-treebank.dat'))\n",
    "\n",
    "def sim(sense1, sense2, option=\"path\"):\n",
    "    \"\"\" Calculates similarity based on user's choice. \"\"\"\n",
    "    option = option.lower()\n",
    "    if option.lower() in [\"path\", \"path_similarity\", \n",
    "                        \"wup\", \"wupa\", \"wu-palmer\", \"wu-palmer\",\n",
    "                        'lch', \"leacock-chordorow\"]:\n",
    "        return similarity_by_path(sense1, sense2, option) \n",
    "    elif option.lower() in [\"res\", \"resnik\",\n",
    "                          \"jcn\",\"jiang-conrath\",\n",
    "                          \"lin\"]:\n",
    "        return similarity_by_infocontent(sense1, sense2, option)\n",
    "\n",
    "def max_similarity(context_sentence, ambiguous_word, option=\"path\", \n",
    "                   pos=None, best=True):\n",
    "    \"\"\"\n",
    "    Perform WSD by maximizing the sum of maximum similarity between possible \n",
    "    synsets of all words in the context sentence and the possible synsets of the \n",
    "    ambiguous words (see http://goo.gl/XMq2BI):\n",
    "    {argmax}_{synset(a)}(\\sum_{i}^{n}{{max}_{synset(i)}(sim(i,a))}\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for i in wn.synsets(ambiguous_word):\n",
    "        try:\n",
    "            if pos and pos != str(i.pos()):\n",
    "                continue\n",
    "        except:\n",
    "            if pos and pos != str(i.pos):\n",
    "                continue\n",
    "        result[i] = sum(max([sim(i,k,option) for k in wn.synsets(j)]+[0]) \\\n",
    "                        for j in word_tokenize(context_sentence))\n",
    "\n",
    "    if option in [\"res\",\"resnik\"]: # lower score = more similar\n",
    "        result = sorted([(v,k) for k,v in result.items()])\n",
    "    else: # higher score = more similar\n",
    "        result = sorted([(v,k) for k,v in result.items()],reverse=True)\n",
    "    #print (result)\n",
    "    if best: return result[0][1];\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#similarity_by_infocontent('read/write_head.n.01', 'read/write_head.n.01', 'res')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sim_terms_one_file(path_data_base,path_non_redundant,file_name):\n",
    "    #print(\"Start_sim\",path_data_base,path_non_redundant,file_name,file_path_sim)\n",
    "    #list_terms=read_cvs_by_pands(path_data_source,sub_path_data_source_tfidf+file_name,0,0)#.index\n",
    "    list_terms=list(read_cvs_by_pands(path_data_base,path_non_redundant+file_name,None,0))\n",
    "    #compare between the same file\n",
    "    #print(len(list_terms))\n",
    "    #index=1\n",
    "    #limt=5\n",
    "    for i in range(len(list_terms)):\n",
    "        \n",
    "        term=list_terms[i]\n",
    "        term_file=term.replace(\"/\", \"_\")\n",
    "        #print(term)\n",
    "        is_term_new=False\n",
    "        if not is_file_exist(file_path_sim,file_path_sim+term_file+\".csv\"): # if !Fale this new term\n",
    "                #print(\" not term\",term)\n",
    "                #create_file(file_path_sim,term+\".csv\")  \n",
    "                is_term_new=True\n",
    "\n",
    "        for term_next in list_terms[i:]:\n",
    "            #print(\"terrm\",term,\"term_next\",term_next)\n",
    "\n",
    "            is_term_next_new=False\n",
    "\n",
    "            term_next_file=term_next.replace(\"/\", \"_\")\n",
    "\n",
    "            if not is_file_exist(file_path_sim,file_path_sim+term_next_file+\".csv\"): #next term is new\n",
    "                    #print(\" not termin\",list_terms[index])\n",
    "                    #create_file(file_path_sim,term_next+\".csv\")\n",
    "                    is_term_next_new=True\n",
    "\n",
    "            #print(term,is_term_new,term_next,is_term_next_new)\n",
    "            sim=0\n",
    "            list_term=[]\n",
    "            list_term_next=[]\n",
    "            if is_term_new or is_term_next_new:\n",
    "                sim=similarity_by_infocontent(term, term_next, 'res')\n",
    "                #print(term,term_next,\"sim\",sim)\n",
    "                if sim <1:\n",
    "                    sim=0\n",
    "                if sim !=0:\n",
    "                    list_term=[term,sim]\n",
    "                    list_term_next=[term_next,sim]\n",
    "\n",
    "            #print(list_term)\n",
    "            #print(list_term_next)\n",
    "            if sim !=0:\n",
    "                if term != term_next:\n",
    "                    #print(\"term != term_next\")\n",
    "                    if is_term_new:  \n",
    "                        #print(\"             is_term_new\",term)\n",
    "                        #print(\"is_term_new\",file_path_sim+term_file+\".csv\")\n",
    "                        create_file(file_path_sim,term_file+\".csv\")\n",
    "                        add_row_csv(file_path_sim,term_file+\".csv\",list_term)#add the same sim\n",
    "                        add_row_csv(file_path_sim,term_file+\".csv\",list_term_next)#add the next sim\n",
    "                        is_term_new=False\n",
    "                    else:\n",
    "                        #print(\"             is_term_old\",term)\n",
    "                        add_row_csv(file_path_sim,term_file+\".csv\",list_term_next)#add the next sim\n",
    "\n",
    "\n",
    "                    if is_term_next_new:\n",
    "                        #print(\"             is_term_next_new\",term_next)\n",
    "\n",
    "                        sim_nex=similarity_by_infocontent(term_next, term_next, 'res')\n",
    "                        #print(\"is_term_next_new \",file_path_sim,\" term_next\",term_next_file+\".csv\")\n",
    "                        create_file(file_path_sim,term_next_file+\".csv\")\n",
    "                        list_next=[term_next,sim_nex]\n",
    "                        add_row_csv(file_path_sim,term_next_file+\".csv\",list_next)#add term to next\n",
    "                        add_row_csv(file_path_sim,term_next_file+\".csv\",list_term)#add term to next\n",
    "                        is_term_next_new=False\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        #print(\"              is_term_next_old\",term_next,sim)\n",
    "                        add_row_csv(file_path_sim,term_next_file+\".csv\",list_term)#add term to next\n",
    "                else:\n",
    "                    #print(\"term == term_next\",term,term_next,sim,list_term)\n",
    "                    if is_term_new:  \n",
    "                        #print(\"            is_term_new\",term,sim,list_term)\n",
    "                        #print(\"else\",file_path_sim+term_file+\".csv\")\n",
    "                        create_file(file_path_sim,term_file+\".csv\")\n",
    "                        add_row_csv(file_path_sim,term_file+\".csv\",list_term)#add the same sim\n",
    "                        is_term_new=False\n",
    "\n",
    "\n",
    "        #print(\"finesed\")\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#list_terms=list(read_cvs_by_pands(path_data_base,path_non_redundant+\"cscs5.csv\",None,0))\n",
    "#list_terms=list(read_cvs_by_pands(path_data_base,path_non_redundant+\"cscs5.csv\",None,0))\n",
    "#list_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.name_file_no>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sim_terms_one_file(path_data_base,path_non_redundant,\"cs0.csv\")\n",
    "\n",
    "\n",
    "#name_file_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sim_terms_previous_file(path_data_base,path_non_redundant,file_name,task_no):\n",
    "    print(\"sim_terms_previous_file\",file_name)\n",
    "    \n",
    "    #previous_file_names_list=read_last_file_list(path_data_base+path_non_redundant,\".csv\")\n",
    "    previous_file_names_list=read_last_file_list_sim_previouse(path_data_base+path_non_redundant,\".csv\",task_no)\n",
    "    print(\"previous_file_names_list\",previous_file_names_list)\n",
    "    #list_terms=read_cvs_by_pands(path_data_source,sub_path_data_source_tfidf+file_name,0,0)#.index\n",
    "    list_terms=list(read_cvs_by_pands(path_data_base,path_non_redundant+file_name,None,0))\n",
    "    print(\"list_terms\",list_terms)\n",
    "    for previous_file_name in previous_file_names_list:\n",
    "        print(\"previous_file_name\",previous_file_name)\n",
    "        pure_file_name=full_name_file(previous_file_name)\n",
    "        if pure_file_name !=file_name:\n",
    "            print(\"pure_file_name\",pure_file_name)\n",
    "            list_terms_others=list(read_cvs_by_pands(path_data_base,path_non_redundant+pure_file_name,None,0))\n",
    "\n",
    "            #compare between the same file\n",
    "            #print(len(list_terms))\n",
    "            #index=1\n",
    "            limt=5\n",
    "            for i in range(len(list_terms)):\n",
    "                term=list_terms[i]\n",
    "                term_file=term.replace(\"/\", \"_\")\n",
    "                print(term)\n",
    "                is_term_new=False\n",
    "                if not is_file_exist(file_path_sim,file_path_sim+term_file+\".csv\"): # if !Fale this new term\n",
    "                        #print(\" not term\",term)\n",
    "                        #create_file(file_path_sim,term+\".csv\")  \n",
    "                        is_term_new=True\n",
    "\n",
    "                #for term_next in list_terms[i:limt]:\n",
    "                for x in range(len(list_terms_others)):\n",
    "                    term_next=list_terms_others[x]\n",
    "                    term_next_file=term_next.replace(\"/\", \"_\")\n",
    "                    print(\"terrm\",term,\"term_next\",term_next)\n",
    "\n",
    "                    is_term_next_new=False\n",
    "\n",
    "\n",
    "\n",
    "                    if not is_file_exist(file_path_sim,file_path_sim+term_next_file+\".csv\"): #next term is new\n",
    "                            #print(\" not termin\",list_terms[index])\n",
    "                            #create_file(file_path_sim,term_next+\".csv\")\n",
    "                            is_term_next_new=True\n",
    "\n",
    "                    #print(term,is_term_new,term_next,is_term_next_new)\n",
    "                    sim=0\n",
    "                    list_term=[]\n",
    "                    list_term_next=[]\n",
    "                    if is_term_new or is_term_next_new:\n",
    "                        sim=similarity_by_infocontent(term, term_next, 'res')\n",
    "                        #print(term,term_next,\"sim\",sim)\n",
    "                        if sim <1:\n",
    "                            sim=0\n",
    "                        if sim !=0:\n",
    "                            list_term=[term,sim]\n",
    "                            list_term_next=[term_next,sim]\n",
    "\n",
    "                    #print(list_term)\n",
    "                    #print(list_term_next)\n",
    "                    if sim !=0:\n",
    "                        if term != term_next:\n",
    "                            #print(\"term != term_next\")\n",
    "                            if is_term_new:  \n",
    "                                #print(\"             is_term_new\",term)\n",
    "                                #print(\"is_term_new\",file_path_sim+term_file+\".csv\")\n",
    "                                create_file(file_path_sim,term_file+\".csv\")\n",
    "                                add_row_csv(file_path_sim,term_file+\".csv\",list_term)#add the same sim\n",
    "                                add_row_csv(file_path_sim,term_file+\".csv\",list_term_next)#add the next sim\n",
    "                                is_term_new=False\n",
    "                            else:\n",
    "                                #print(\"             is_term_old\",term)\n",
    "                                add_row_csv(file_path_sim,term_file+\".csv\",list_term_next)#add the next sim\n",
    "\n",
    "\n",
    "                            if is_term_next_new:\n",
    "                                #print(\"             is_term_next_new\",term_next)\n",
    "\n",
    "                                sim_nex=similarity_by_infocontent(term_next, term_next, 'res')\n",
    "                                #print(\"is_term_next_new\",file_path_sim+term_next_file+\".csv\")\n",
    "                                create_file(file_path_sim,term_next_file+\".csv\")\n",
    "                                list_next=[term_next,sim_nex]\n",
    "                                add_row_csv(file_path_sim,term_next_file+\".csv\",list_next)#add term to next\n",
    "                                add_row_csv(file_path_sim,term_next_file+\".csv\",list_term)#add term to next\n",
    "                                is_term_next_new=False\n",
    "\n",
    "\n",
    "                            else:\n",
    "                                #print(\"              is_term_next_old\",term_next,sim)\n",
    "                                add_row_csv(file_path_sim,term_next_file+\".csv\",list_term)#add term to next\n",
    "                        else:\n",
    "                            #print(\"term == term_next\",term,term_next,sim,list_term)\n",
    "                            if is_term_new:  \n",
    "                                #print(\"            is_term_new\",term,sim,list_term)\n",
    "                                #print(\"else\",file_path_sim+term_file+\".csv\")\n",
    "                                create_file(file_path_sim,term_file+\".csv\")\n",
    "                                add_row_csv(file_path_sim,term_file+\".csv\",list_term)#add the same sim\n",
    "                                is_term_new=False\n",
    "\n",
    "\n",
    "                #print(\"finesed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load list word of current sub_word\n",
    "#start sim between word and next word\n",
    "# load next file from previous list \n",
    "##start sim between words in curent file and word in next file \n",
    "#store each comparison in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def gpu_full_process(filename,path_data_base,path_tf,path_non_redundant,path_idf,file_path_sim,task_no):\n",
    "    #print(\"In gpu file_path_sim\",file_path_sim)\n",
    "    #index_file=0\n",
    "    #file_list_task=read_last_file_list(path_data_source+files_path_data_source,\".txt\")\n",
    "    #for i in range(len(file_list_task)):\n",
    "    #filename=file_list_task[i]\n",
    "    #print(filename)\n",
    "    \n",
    "    if not isfile_empty(path_data_base+path_non_redundant+filename):\n",
    "        #sim_terms_one_file(path_data_base,path_non_redundant,filename)\n",
    "        #print(\"finesed sim\",filename)\n",
    "        sim_terms_previous_file(path_data_base,path_non_redundant,filename,task_no)\n",
    "        print(\"finesed sim_terms_previous_file\")\n",
    "    else:\n",
    "        print(\"Empty redundant\",filename)\n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#isfile_empty(\"/home/fsg/Desktop/files/dd.txt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "    #return [x.name for x in local_device_protos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def distrupited_task_gpu(no_task,total_no_gpu):\n",
    "    \n",
    "    return no_task%total_no_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************Started Session Main Process*************\n",
      "************Started Session CPU *************\n",
      "/home/fsg/Desktop/csv-fatma/sub_word_tf/cscs1.csv\n",
      "takno 1\n",
      "************Started Session GPU *************\n",
      "task_no 1 gpu_name /gpu:1\n",
      "cscs1.csv\n",
      "sim_terms_previous_file cscs1.csv\n",
      "previous_file_names_list []\n",
      "list_terms ['graphics.n.02', 'mathematics.n.01', 'scientist.n.01', 'practice.n.04', 'logic.n.05', 'span.n.04', 'terminus.n.03', 'outline.n.02']\n",
      "finesed sim_terms_previous_file\n",
      "/home/fsg/Desktop/csv-fatma/sub_word_tf/cscs4.csv\n",
      "takno 4\n",
      "************Started Session GPU *************\n",
      "task_no 4 gpu_name /gpu:0\n",
      "cscs4.csv\n",
      "sim_terms_previous_file cscs4.csv\n",
      "previous_file_names_list ['/home/fsg/Desktop/csv-fatma/sub_word_tf/cscs1.csv', '/home/fsg/Desktop/csv-fatma/sub_word_tf/cscs2.csv', '/home/fsg/Desktop/csv-fatma/sub_word_tf/cscs3.csv']\n",
      "list_terms ['methodology.n.02', 'cognition.n.01', 'resolution.n.04', 'computer_science.n.01', 'summons.n.03', 'hardware.n.03', 'foundation_garment.n.01', 'principle.n.04', 'stove.n.01', 'profession.n.01']\n",
      "previous_file_name /home/fsg/Desktop/csv-fatma/sub_word_tf/cscs1.csv\n",
      "pure_file_name cscs1.csv\n",
      "methodology.n.02\n",
      "terrm methodology.n.02 term_next graphics.n.02\n",
      "terrm methodology.n.02 term_next mathematics.n.01\n",
      "terrm methodology.n.02 term_next scientist.n.01\n",
      "terrm methodology.n.02 term_next practice.n.04\n",
      "terrm methodology.n.02 term_next logic.n.05\n",
      "terrm methodology.n.02 term_next span.n.04\n",
      "terrm methodology.n.02 term_next terminus.n.03\n",
      "terrm methodology.n.02 term_next outline.n.02\n",
      "cognition.n.01\n",
      "terrm cognition.n.01 term_next graphics.n.02\n",
      "terrm cognition.n.01 term_next mathematics.n.01\n",
      "terrm cognition.n.01 term_next scientist.n.01\n",
      "terrm cognition.n.01 term_next practice.n.04\n",
      "terrm cognition.n.01 term_next logic.n.05\n",
      "terrm cognition.n.01 term_next span.n.04\n",
      "terrm cognition.n.01 term_next terminus.n.03\n",
      "terrm cognition.n.01 term_next outline.n.02\n",
      "resolution.n.04\n",
      "terrm resolution.n.04 term_next graphics.n.02\n",
      "terrm resolution.n.04 term_next mathematics.n.01\n",
      "terrm resolution.n.04 term_next scientist.n.01\n",
      "terrm resolution.n.04 term_next practice.n.04\n",
      "terrm resolution.n.04 term_next logic.n.05\n",
      "terrm resolution.n.04 term_next span.n.04\n",
      "terrm resolution.n.04 term_next terminus.n.03\n",
      "terrm resolution.n.04 term_next outline.n.02\n",
      "computer_science.n.01\n",
      "terrm computer_science.n.01 term_next graphics.n.02\n",
      "terrm computer_science.n.01 term_next mathematics.n.01\n",
      "terrm computer_science.n.01 term_next scientist.n.01\n",
      "terrm computer_science.n.01 term_next practice.n.04\n",
      "terrm computer_science.n.01 term_next logic.n.05\n",
      "terrm computer_science.n.01 term_next span.n.04\n",
      "terrm computer_science.n.01 term_next terminus.n.03\n",
      "terrm computer_science.n.01 term_next outline.n.02\n",
      "summons.n.03\n",
      "terrm summons.n.03 term_next graphics.n.02\n",
      "terrm summons.n.03 term_next mathematics.n.01\n",
      "terrm summons.n.03 term_next scientist.n.01\n",
      "terrm summons.n.03 term_next practice.n.04\n",
      "terrm summons.n.03 term_next logic.n.05\n",
      "terrm summons.n.03 term_next span.n.04\n",
      "terrm summons.n.03 term_next terminus.n.03\n",
      "terrm summons.n.03 term_next outline.n.02\n",
      "hardware.n.03\n",
      "terrm hardware.n.03 term_next graphics.n.02\n",
      "terrm hardware.n.03 term_next mathematics.n.01\n",
      "terrm hardware.n.03 term_next scientist.n.01\n",
      "terrm hardware.n.03 term_next practice.n.04\n",
      "terrm hardware.n.03 term_next logic.n.05\n",
      "terrm hardware.n.03 term_next span.n.04\n",
      "terrm hardware.n.03 term_next terminus.n.03\n",
      "terrm hardware.n.03 term_next outline.n.02\n",
      "foundation_garment.n.01\n",
      "terrm foundation_garment.n.01 term_next graphics.n.02\n",
      "terrm foundation_garment.n.01 term_next mathematics.n.01\n",
      "terrm foundation_garment.n.01 term_next scientist.n.01\n",
      "terrm foundation_garment.n.01 term_next practice.n.04\n",
      "terrm foundation_garment.n.01 term_next logic.n.05\n",
      "terrm foundation_garment.n.01 term_next span.n.04\n",
      "terrm foundation_garment.n.01 term_next terminus.n.03\n",
      "terrm foundation_garment.n.01 term_next outline.n.02\n",
      "principle.n.04\n",
      "terrm principle.n.04 term_next graphics.n.02\n",
      "terrm principle.n.04 term_next mathematics.n.01\n",
      "terrm principle.n.04 term_next scientist.n.01\n",
      "terrm principle.n.04 term_next practice.n.04\n",
      "terrm principle.n.04 term_next logic.n.05\n",
      "terrm principle.n.04 term_next span.n.04\n",
      "terrm principle.n.04 term_next terminus.n.03\n",
      "terrm principle.n.04 term_next outline.n.02\n",
      "stove.n.01\n",
      "terrm stove.n.01 term_next graphics.n.02\n",
      "terrm stove.n.01 term_next mathematics.n.01\n",
      "terrm stove.n.01 term_next scientist.n.01\n",
      "terrm stove.n.01 term_next practice.n.04\n",
      "terrm stove.n.01 term_next logic.n.05\n",
      "terrm stove.n.01 term_next span.n.04\n",
      "terrm stove.n.01 term_next terminus.n.03\n",
      "terrm stove.n.01 term_next outline.n.02\n",
      "profession.n.01\n",
      "terrm profession.n.01 term_next graphics.n.02\n",
      "terrm profession.n.01 term_next mathematics.n.01\n",
      "terrm profession.n.01 term_next scientist.n.01\n",
      "terrm profession.n.01 term_next practice.n.04\n",
      "terrm profession.n.01 term_next logic.n.05\n",
      "terrm profession.n.01 term_next span.n.04\n",
      "terrm profession.n.01 term_next terminus.n.03\n",
      "terrm profession.n.01 term_next outline.n.02\n",
      "previous_file_name /home/fsg/Desktop/csv-fatma/sub_word_tf/cscs2.csv\n",
      "pure_file_name cscs2.csv\n",
      "methodology.n.02\n",
      "terrm methodology.n.02 term_next meet.n.01\n",
      "terrm methodology.n.02 term_next one.n.02\n",
      "terrm methodology.n.02 term_next fundamental.n.02\n",
      "terrm methodology.n.02 term_next concept.n.01\n",
      "terrm methodology.n.02 term_next lotion.n.02\n",
      "terrm methodology.n.02 term_next interaction.n.02\n",
      "terrm methodology.n.02 term_next frankincense.n.01\n",
      "terrm methodology.n.02 term_next focus.n.07\n",
      "terrm methodology.n.02 term_next particular.n.03\n",
      "terrm methodology.n.02 term_next well.n.05\n",
      "terrm methodology.n.02 term_next understanding.n.01\n",
      "terrm methodology.n.02 term_next intelligence.n.02\n",
      "terrm methodology.n.02 term_next stipulation.n.03\n",
      "cognition.n.01\n",
      "terrm cognition.n.01 term_next meet.n.01\n",
      "terrm cognition.n.01 term_next one.n.02\n",
      "terrm cognition.n.01 term_next fundamental.n.02\n",
      "terrm cognition.n.01 term_next concept.n.01\n",
      "terrm cognition.n.01 term_next lotion.n.02\n",
      "terrm cognition.n.01 term_next interaction.n.02\n",
      "terrm cognition.n.01 term_next frankincense.n.01\n",
      "terrm cognition.n.01 term_next focus.n.07\n",
      "terrm cognition.n.01 term_next particular.n.03\n",
      "terrm cognition.n.01 term_next well.n.05\n",
      "terrm cognition.n.01 term_next understanding.n.01\n",
      "terrm cognition.n.01 term_next intelligence.n.02\n",
      "terrm cognition.n.01 term_next stipulation.n.03\n",
      "resolution.n.04\n",
      "terrm resolution.n.04 term_next meet.n.01\n",
      "terrm resolution.n.04 term_next one.n.02\n",
      "terrm resolution.n.04 term_next fundamental.n.02\n",
      "terrm resolution.n.04 term_next concept.n.01\n",
      "terrm resolution.n.04 term_next lotion.n.02\n",
      "terrm resolution.n.04 term_next interaction.n.02\n",
      "terrm resolution.n.04 term_next frankincense.n.01\n",
      "terrm resolution.n.04 term_next focus.n.07\n",
      "terrm resolution.n.04 term_next particular.n.03\n",
      "terrm resolution.n.04 term_next well.n.05\n",
      "terrm resolution.n.04 term_next understanding.n.01\n",
      "terrm resolution.n.04 term_next intelligence.n.02\n",
      "terrm resolution.n.04 term_next stipulation.n.03\n",
      "computer_science.n.01\n",
      "terrm computer_science.n.01 term_next meet.n.01\n",
      "terrm computer_science.n.01 term_next one.n.02\n",
      "terrm computer_science.n.01 term_next fundamental.n.02\n",
      "terrm computer_science.n.01 term_next concept.n.01\n",
      "terrm computer_science.n.01 term_next lotion.n.02\n",
      "terrm computer_science.n.01 term_next interaction.n.02\n",
      "terrm computer_science.n.01 term_next frankincense.n.01\n",
      "terrm computer_science.n.01 term_next focus.n.07\n",
      "terrm computer_science.n.01 term_next particular.n.03\n",
      "terrm computer_science.n.01 term_next well.n.05\n",
      "terrm computer_science.n.01 term_next understanding.n.01\n",
      "terrm computer_science.n.01 term_next intelligence.n.02\n",
      "terrm computer_science.n.01 term_next stipulation.n.03\n",
      "summons.n.03\n",
      "terrm summons.n.03 term_next meet.n.01\n",
      "terrm summons.n.03 term_next one.n.02\n",
      "terrm summons.n.03 term_next fundamental.n.02\n",
      "terrm summons.n.03 term_next concept.n.01\n",
      "terrm summons.n.03 term_next lotion.n.02\n",
      "terrm summons.n.03 term_next interaction.n.02\n",
      "terrm summons.n.03 term_next frankincense.n.01\n",
      "terrm summons.n.03 term_next focus.n.07\n",
      "terrm summons.n.03 term_next particular.n.03\n",
      "terrm summons.n.03 term_next well.n.05\n",
      "terrm summons.n.03 term_next understanding.n.01\n",
      "terrm summons.n.03 term_next intelligence.n.02\n",
      "terrm summons.n.03 term_next stipulation.n.03\n",
      "hardware.n.03\n",
      "terrm hardware.n.03 term_next meet.n.01\n",
      "terrm hardware.n.03 term_next one.n.02\n",
      "terrm hardware.n.03 term_next fundamental.n.02\n",
      "terrm hardware.n.03 term_next concept.n.01\n",
      "terrm hardware.n.03 term_next lotion.n.02\n",
      "terrm hardware.n.03 term_next interaction.n.02\n",
      "terrm hardware.n.03 term_next frankincense.n.01\n",
      "terrm hardware.n.03 term_next focus.n.07\n",
      "terrm hardware.n.03 term_next particular.n.03\n",
      "terrm hardware.n.03 term_next well.n.05\n",
      "terrm hardware.n.03 term_next understanding.n.01\n",
      "terrm hardware.n.03 term_next intelligence.n.02\n",
      "terrm hardware.n.03 term_next stipulation.n.03\n",
      "foundation_garment.n.01\n",
      "terrm foundation_garment.n.01 term_next meet.n.01\n",
      "terrm foundation_garment.n.01 term_next one.n.02\n",
      "terrm foundation_garment.n.01 term_next fundamental.n.02\n",
      "terrm foundation_garment.n.01 term_next concept.n.01\n",
      "terrm foundation_garment.n.01 term_next lotion.n.02\n",
      "terrm foundation_garment.n.01 term_next interaction.n.02\n",
      "terrm foundation_garment.n.01 term_next frankincense.n.01\n",
      "terrm foundation_garment.n.01 term_next focus.n.07\n",
      "terrm foundation_garment.n.01 term_next particular.n.03\n",
      "terrm foundation_garment.n.01 term_next well.n.05\n",
      "terrm foundation_garment.n.01 term_next understanding.n.01\n",
      "terrm foundation_garment.n.01 term_next intelligence.n.02\n",
      "terrm foundation_garment.n.01 term_next stipulation.n.03\n",
      "principle.n.04\n",
      "terrm principle.n.04 term_next meet.n.01\n",
      "terrm principle.n.04 term_next one.n.02\n",
      "terrm principle.n.04 term_next fundamental.n.02\n",
      "terrm principle.n.04 term_next concept.n.01\n",
      "terrm principle.n.04 term_next lotion.n.02\n",
      "terrm principle.n.04 term_next interaction.n.02\n",
      "terrm principle.n.04 term_next frankincense.n.01\n",
      "terrm principle.n.04 term_next focus.n.07\n",
      "terrm principle.n.04 term_next particular.n.03\n",
      "terrm principle.n.04 term_next well.n.05\n",
      "terrm principle.n.04 term_next understanding.n.01\n",
      "terrm principle.n.04 term_next intelligence.n.02\n",
      "terrm principle.n.04 term_next stipulation.n.03\n",
      "stove.n.01\n",
      "terrm stove.n.01 term_next meet.n.01\n",
      "terrm stove.n.01 term_next one.n.02\n",
      "terrm stove.n.01 term_next fundamental.n.02\n",
      "terrm stove.n.01 term_next concept.n.01\n",
      "terrm stove.n.01 term_next lotion.n.02\n",
      "terrm stove.n.01 term_next interaction.n.02\n",
      "terrm stove.n.01 term_next frankincense.n.01\n",
      "terrm stove.n.01 term_next focus.n.07\n",
      "terrm stove.n.01 term_next particular.n.03\n",
      "terrm stove.n.01 term_next well.n.05\n",
      "terrm stove.n.01 term_next understanding.n.01\n",
      "terrm stove.n.01 term_next intelligence.n.02\n",
      "terrm stove.n.01 term_next stipulation.n.03\n",
      "profession.n.01\n",
      "terrm profession.n.01 term_next meet.n.01\n",
      "terrm profession.n.01 term_next one.n.02\n",
      "terrm profession.n.01 term_next fundamental.n.02\n",
      "terrm profession.n.01 term_next concept.n.01\n",
      "terrm profession.n.01 term_next lotion.n.02\n",
      "terrm profession.n.01 term_next interaction.n.02\n",
      "terrm profession.n.01 term_next frankincense.n.01\n",
      "terrm profession.n.01 term_next focus.n.07\n",
      "terrm profession.n.01 term_next particular.n.03\n",
      "terrm profession.n.01 term_next well.n.05\n",
      "terrm profession.n.01 term_next understanding.n.01\n",
      "terrm profession.n.01 term_next intelligence.n.02\n",
      "terrm profession.n.01 term_next stipulation.n.03\n",
      "previous_file_name /home/fsg/Desktop/csv-fatma/sub_word_tf/cscs3.csv\n",
      "pure_file_name cscs3.csv\n",
      "methodology.n.02\n",
      "terrm methodology.n.02 term_next design.n.06\n",
      "terrm methodology.n.02 term_next discipline.n.02\n",
      "terrm methodology.n.02 term_next psychoanalysis.n.01\n",
      "terrm methodology.n.02 term_next computer_architecture.n.02\n",
      "terrm methodology.n.02 term_next technique.n.01\n",
      "terrm methodology.n.02 term_next software.n.01\n",
      "terrm methodology.n.02 term_next trouble.n.01\n",
      "terrm methodology.n.02 term_next algorithm.n.01\n",
      "terrm methodology.n.02 term_next communication.n.03\n",
      "terrm methodology.n.02 term_next system.n.08\n",
      "terrm methodology.n.02 term_next engineering.n.03\n",
      "cognition.n.01\n",
      "terrm cognition.n.01 term_next design.n.06\n",
      "terrm cognition.n.01 term_next discipline.n.02\n",
      "terrm cognition.n.01 term_next psychoanalysis.n.01\n",
      "terrm cognition.n.01 term_next computer_architecture.n.02\n",
      "terrm cognition.n.01 term_next technique.n.01\n",
      "terrm cognition.n.01 term_next software.n.01\n",
      "terrm cognition.n.01 term_next trouble.n.01\n",
      "terrm cognition.n.01 term_next algorithm.n.01\n",
      "terrm cognition.n.01 term_next communication.n.03\n",
      "terrm cognition.n.01 term_next system.n.08\n",
      "terrm cognition.n.01 term_next engineering.n.03\n",
      "resolution.n.04\n",
      "terrm resolution.n.04 term_next design.n.06\n",
      "terrm resolution.n.04 term_next discipline.n.02\n",
      "terrm resolution.n.04 term_next psychoanalysis.n.01\n",
      "terrm resolution.n.04 term_next computer_architecture.n.02\n",
      "terrm resolution.n.04 term_next technique.n.01\n",
      "terrm resolution.n.04 term_next software.n.01\n",
      "terrm resolution.n.04 term_next trouble.n.01\n",
      "terrm resolution.n.04 term_next algorithm.n.01\n",
      "terrm resolution.n.04 term_next communication.n.03\n",
      "terrm resolution.n.04 term_next system.n.08\n",
      "terrm resolution.n.04 term_next engineering.n.03\n",
      "computer_science.n.01\n",
      "terrm computer_science.n.01 term_next design.n.06\n",
      "terrm computer_science.n.01 term_next discipline.n.02\n",
      "terrm computer_science.n.01 term_next psychoanalysis.n.01\n",
      "terrm computer_science.n.01 term_next computer_architecture.n.02\n",
      "terrm computer_science.n.01 term_next technique.n.01\n",
      "terrm computer_science.n.01 term_next software.n.01\n",
      "terrm computer_science.n.01 term_next trouble.n.01\n",
      "terrm computer_science.n.01 term_next algorithm.n.01\n",
      "terrm computer_science.n.01 term_next communication.n.03\n",
      "terrm computer_science.n.01 term_next system.n.08\n",
      "terrm computer_science.n.01 term_next engineering.n.03\n",
      "summons.n.03\n",
      "terrm summons.n.03 term_next design.n.06\n",
      "terrm summons.n.03 term_next discipline.n.02\n",
      "terrm summons.n.03 term_next psychoanalysis.n.01\n",
      "terrm summons.n.03 term_next computer_architecture.n.02\n",
      "terrm summons.n.03 term_next technique.n.01\n",
      "terrm summons.n.03 term_next software.n.01\n",
      "terrm summons.n.03 term_next trouble.n.01\n",
      "terrm summons.n.03 term_next algorithm.n.01\n",
      "terrm summons.n.03 term_next communication.n.03\n",
      "terrm summons.n.03 term_next system.n.08\n",
      "terrm summons.n.03 term_next engineering.n.03\n",
      "hardware.n.03\n",
      "terrm hardware.n.03 term_next design.n.06\n",
      "terrm hardware.n.03 term_next discipline.n.02\n",
      "terrm hardware.n.03 term_next psychoanalysis.n.01\n",
      "terrm hardware.n.03 term_next computer_architecture.n.02\n",
      "terrm hardware.n.03 term_next technique.n.01\n",
      "terrm hardware.n.03 term_next software.n.01\n",
      "terrm hardware.n.03 term_next trouble.n.01\n",
      "terrm hardware.n.03 term_next algorithm.n.01\n",
      "terrm hardware.n.03 term_next communication.n.03\n",
      "terrm hardware.n.03 term_next system.n.08\n",
      "terrm hardware.n.03 term_next engineering.n.03\n",
      "foundation_garment.n.01\n",
      "terrm foundation_garment.n.01 term_next design.n.06\n",
      "terrm foundation_garment.n.01 term_next discipline.n.02\n",
      "terrm foundation_garment.n.01 term_next psychoanalysis.n.01\n",
      "terrm foundation_garment.n.01 term_next computer_architecture.n.02\n",
      "terrm foundation_garment.n.01 term_next technique.n.01\n",
      "terrm foundation_garment.n.01 term_next software.n.01\n",
      "terrm foundation_garment.n.01 term_next trouble.n.01\n",
      "terrm foundation_garment.n.01 term_next algorithm.n.01\n",
      "terrm foundation_garment.n.01 term_next communication.n.03\n",
      "terrm foundation_garment.n.01 term_next system.n.08\n",
      "terrm foundation_garment.n.01 term_next engineering.n.03\n",
      "principle.n.04\n",
      "terrm principle.n.04 term_next design.n.06\n",
      "terrm principle.n.04 term_next discipline.n.02\n",
      "terrm principle.n.04 term_next psychoanalysis.n.01\n",
      "terrm principle.n.04 term_next computer_architecture.n.02\n",
      "terrm principle.n.04 term_next technique.n.01\n",
      "terrm principle.n.04 term_next software.n.01\n",
      "terrm principle.n.04 term_next trouble.n.01\n",
      "terrm principle.n.04 term_next algorithm.n.01\n",
      "terrm principle.n.04 term_next communication.n.03\n",
      "terrm principle.n.04 term_next system.n.08\n",
      "terrm principle.n.04 term_next engineering.n.03\n",
      "stove.n.01\n",
      "terrm stove.n.01 term_next design.n.06\n",
      "terrm stove.n.01 term_next discipline.n.02\n",
      "terrm stove.n.01 term_next psychoanalysis.n.01\n",
      "terrm stove.n.01 term_next computer_architecture.n.02\n",
      "terrm stove.n.01 term_next technique.n.01\n",
      "terrm stove.n.01 term_next software.n.01\n",
      "terrm stove.n.01 term_next trouble.n.01\n",
      "terrm stove.n.01 term_next algorithm.n.01\n",
      "terrm stove.n.01 term_next communication.n.03\n",
      "terrm stove.n.01 term_next system.n.08\n",
      "terrm stove.n.01 term_next engineering.n.03\n",
      "profession.n.01\n",
      "terrm profession.n.01 term_next design.n.06\n",
      "terrm profession.n.01 term_next discipline.n.02\n",
      "terrm profession.n.01 term_next psychoanalysis.n.01\n",
      "terrm profession.n.01 term_next computer_architecture.n.02\n",
      "terrm profession.n.01 term_next technique.n.01\n",
      "terrm profession.n.01 term_next software.n.01\n",
      "terrm profession.n.01 term_next trouble.n.01\n",
      "terrm profession.n.01 term_next algorithm.n.01\n",
      "terrm profession.n.01 term_next communication.n.03\n",
      "terrm profession.n.01 term_next system.n.08\n",
      "terrm profession.n.01 term_next engineering.n.03\n",
      "finesed sim_terms_previous_file\n",
      "/home/fsg/Desktop/csv-fatma/sub_word_tf/cscs6.csv\n",
      "takno 6\n",
      "************Started Session GPU *************\n",
      "task_no 6 gpu_name /gpu:0\n",
      "cscs6.csv\n",
      "sim_terms_previous_file cscs6.csv\n",
      "previous_file_names_list ['/home/fsg/Desktop/csv-fatma/sub_word_tf/cscs1.csv', '/home/fsg/Desktop/csv-fatma/sub_word_tf/cscs4.csv', '/home/fsg/Desktop/csv-fatma/sub_word_tf/cscs2.csv', '/home/fsg/Desktop/csv-fatma/sub_word_tf/cscs3.csv']\n",
      "list_terms ['programming.n.02', 'method.n.01', 'structure.n.04', 'sphere.n.01', 'major.n.04', 'machine.n.05', 'storehouse.n.01', 'valet.n.01', 'terminology.n.01', 'theory.n.03', 'complex.n.03', 'information.n.05', 'job.n.02', 'data.n.01', 'growth.n.01', 'use.n.03']\n",
      "previous_file_name /home/fsg/Desktop/csv-fatma/sub_word_tf/cscs1.csv\n",
      "pure_file_name cscs1.csv\n",
      "programming.n.02\n",
      "terrm programming.n.02 term_next graphics.n.02\n",
      "terrm programming.n.02 term_next mathematics.n.01\n",
      "terrm programming.n.02 term_next scientist.n.01\n",
      "terrm programming.n.02 term_next practice.n.04\n",
      "terrm programming.n.02 term_next logic.n.05\n",
      "terrm programming.n.02 term_next span.n.04\n",
      "terrm programming.n.02 term_next terminus.n.03\n",
      "terrm programming.n.02 term_next outline.n.02\n",
      "method.n.01\n",
      "terrm method.n.01 term_next graphics.n.02\n",
      "terrm method.n.01 term_next mathematics.n.01\n",
      "terrm method.n.01 term_next scientist.n.01\n",
      "terrm method.n.01 term_next practice.n.04\n",
      "terrm method.n.01 term_next logic.n.05\n",
      "terrm method.n.01 term_next span.n.04\n",
      "terrm method.n.01 term_next terminus.n.03\n",
      "terrm method.n.01 term_next outline.n.02\n",
      "structure.n.04\n",
      "terrm structure.n.04 term_next graphics.n.02\n",
      "terrm structure.n.04 term_next mathematics.n.01\n",
      "terrm structure.n.04 term_next scientist.n.01\n",
      "terrm structure.n.04 term_next practice.n.04\n",
      "terrm structure.n.04 term_next logic.n.05\n",
      "terrm structure.n.04 term_next span.n.04\n",
      "terrm structure.n.04 term_next terminus.n.03\n",
      "terrm structure.n.04 term_next outline.n.02\n",
      "sphere.n.01\n",
      "terrm sphere.n.01 term_next graphics.n.02\n",
      "terrm sphere.n.01 term_next mathematics.n.01\n",
      "terrm sphere.n.01 term_next scientist.n.01\n",
      "terrm sphere.n.01 term_next practice.n.04\n",
      "terrm sphere.n.01 term_next logic.n.05\n",
      "terrm sphere.n.01 term_next span.n.04\n",
      "terrm sphere.n.01 term_next terminus.n.03\n",
      "terrm sphere.n.01 term_next outline.n.02\n",
      "major.n.04\n",
      "terrm major.n.04 term_next graphics.n.02\n",
      "terrm major.n.04 term_next mathematics.n.01\n",
      "terrm major.n.04 term_next scientist.n.01\n",
      "terrm major.n.04 term_next practice.n.04\n",
      "terrm major.n.04 term_next logic.n.05\n",
      "terrm major.n.04 term_next span.n.04\n",
      "terrm major.n.04 term_next terminus.n.03\n",
      "terrm major.n.04 term_next outline.n.02\n",
      "machine.n.05\n",
      "terrm machine.n.05 term_next graphics.n.02\n",
      "terrm machine.n.05 term_next mathematics.n.01\n",
      "terrm machine.n.05 term_next scientist.n.01\n",
      "terrm machine.n.05 term_next practice.n.04\n",
      "terrm machine.n.05 term_next logic.n.05\n",
      "terrm machine.n.05 term_next span.n.04\n",
      "terrm machine.n.05 term_next terminus.n.03\n",
      "terrm machine.n.05 term_next outline.n.02\n",
      "storehouse.n.01\n",
      "terrm storehouse.n.01 term_next graphics.n.02\n",
      "terrm storehouse.n.01 term_next mathematics.n.01\n",
      "terrm storehouse.n.01 term_next scientist.n.01\n",
      "terrm storehouse.n.01 term_next practice.n.04\n",
      "terrm storehouse.n.01 term_next logic.n.05\n",
      "terrm storehouse.n.01 term_next span.n.04\n",
      "terrm storehouse.n.01 term_next terminus.n.03\n",
      "terrm storehouse.n.01 term_next outline.n.02\n",
      "valet.n.01\n",
      "terrm valet.n.01 term_next graphics.n.02\n",
      "terrm valet.n.01 term_next mathematics.n.01\n",
      "terrm valet.n.01 term_next scientist.n.01\n",
      "terrm valet.n.01 term_next practice.n.04\n",
      "terrm valet.n.01 term_next logic.n.05\n",
      "terrm valet.n.01 term_next span.n.04\n",
      "terrm valet.n.01 term_next terminus.n.03\n",
      "terrm valet.n.01 term_next outline.n.02\n",
      "terminology.n.01\n",
      "terrm terminology.n.01 term_next graphics.n.02\n",
      "terrm terminology.n.01 term_next mathematics.n.01\n",
      "terrm terminology.n.01 term_next scientist.n.01\n",
      "terrm terminology.n.01 term_next practice.n.04\n",
      "terrm terminology.n.01 term_next logic.n.05\n",
      "terrm terminology.n.01 term_next span.n.04\n",
      "terrm terminology.n.01 term_next terminus.n.03\n",
      "terrm terminology.n.01 term_next outline.n.02\n",
      "theory.n.03\n",
      "terrm theory.n.03 term_next graphics.n.02\n",
      "terrm theory.n.03 term_next mathematics.n.01\n",
      "terrm theory.n.03 term_next scientist.n.01\n",
      "terrm theory.n.03 term_next practice.n.04\n",
      "terrm theory.n.03 term_next logic.n.05\n",
      "terrm theory.n.03 term_next span.n.04\n",
      "terrm theory.n.03 term_next terminus.n.03\n",
      "terrm theory.n.03 term_next outline.n.02\n",
      "complex.n.03\n",
      "terrm complex.n.03 term_next graphics.n.02\n",
      "terrm complex.n.03 term_next mathematics.n.01\n",
      "terrm complex.n.03 term_next scientist.n.01\n",
      "terrm complex.n.03 term_next practice.n.04\n",
      "terrm complex.n.03 term_next logic.n.05\n",
      "terrm complex.n.03 term_next span.n.04\n",
      "terrm complex.n.03 term_next terminus.n.03\n",
      "terrm complex.n.03 term_next outline.n.02\n",
      "information.n.05\n",
      "terrm information.n.05 term_next graphics.n.02\n",
      "terrm information.n.05 term_next mathematics.n.01\n",
      "terrm information.n.05 term_next scientist.n.01\n",
      "terrm information.n.05 term_next practice.n.04\n",
      "terrm information.n.05 term_next logic.n.05\n",
      "terrm information.n.05 term_next span.n.04\n",
      "terrm information.n.05 term_next terminus.n.03\n",
      "terrm information.n.05 term_next outline.n.02\n",
      "job.n.02\n",
      "terrm job.n.02 term_next graphics.n.02\n",
      "terrm job.n.02 term_next mathematics.n.01\n",
      "terrm job.n.02 term_next scientist.n.01\n",
      "terrm job.n.02 term_next practice.n.04\n",
      "terrm job.n.02 term_next logic.n.05\n",
      "terrm job.n.02 term_next span.n.04\n",
      "terrm job.n.02 term_next terminus.n.03\n",
      "terrm job.n.02 term_next outline.n.02\n",
      "data.n.01\n",
      "terrm data.n.01 term_next graphics.n.02\n",
      "terrm data.n.01 term_next mathematics.n.01\n",
      "terrm data.n.01 term_next scientist.n.01\n",
      "terrm data.n.01 term_next practice.n.04\n",
      "terrm data.n.01 term_next logic.n.05\n",
      "terrm data.n.01 term_next span.n.04\n",
      "terrm data.n.01 term_next terminus.n.03\n",
      "terrm data.n.01 term_next outline.n.02\n",
      "growth.n.01\n",
      "terrm growth.n.01 term_next graphics.n.02\n",
      "terrm growth.n.01 term_next mathematics.n.01\n",
      "terrm growth.n.01 term_next scientist.n.01\n",
      "terrm growth.n.01 term_next practice.n.04\n",
      "terrm growth.n.01 term_next logic.n.05\n",
      "terrm growth.n.01 term_next span.n.04\n",
      "terrm growth.n.01 term_next terminus.n.03\n",
      "terrm growth.n.01 term_next outline.n.02\n",
      "use.n.03\n",
      "terrm use.n.03 term_next graphics.n.02\n",
      "terrm use.n.03 term_next mathematics.n.01\n",
      "terrm use.n.03 term_next scientist.n.01\n",
      "terrm use.n.03 term_next practice.n.04\n",
      "terrm use.n.03 term_next logic.n.05\n",
      "terrm use.n.03 term_next span.n.04\n",
      "terrm use.n.03 term_next terminus.n.03\n",
      "terrm use.n.03 term_next outline.n.02\n",
      "previous_file_name /home/fsg/Desktop/csv-fatma/sub_word_tf/cscs4.csv\n",
      "pure_file_name cscs4.csv\n",
      "programming.n.02\n",
      "terrm programming.n.02 term_next methodology.n.02\n",
      "terrm programming.n.02 term_next cognition.n.01\n",
      "terrm programming.n.02 term_next resolution.n.04\n",
      "terrm programming.n.02 term_next computer_science.n.01\n",
      "terrm programming.n.02 term_next summons.n.03\n",
      "terrm programming.n.02 term_next hardware.n.03\n",
      "terrm programming.n.02 term_next foundation_garment.n.01\n",
      "terrm programming.n.02 term_next principle.n.04\n",
      "terrm programming.n.02 term_next stove.n.01\n",
      "terrm programming.n.02 term_next profession.n.01\n",
      "method.n.01\n",
      "terrm method.n.01 term_next methodology.n.02\n",
      "terrm method.n.01 term_next cognition.n.01\n",
      "terrm method.n.01 term_next resolution.n.04\n",
      "terrm method.n.01 term_next computer_science.n.01\n",
      "terrm method.n.01 term_next summons.n.03\n",
      "terrm method.n.01 term_next hardware.n.03\n",
      "terrm method.n.01 term_next foundation_garment.n.01\n",
      "terrm method.n.01 term_next principle.n.04\n",
      "terrm method.n.01 term_next stove.n.01\n",
      "terrm method.n.01 term_next profession.n.01\n",
      "structure.n.04\n",
      "terrm structure.n.04 term_next methodology.n.02\n",
      "terrm structure.n.04 term_next cognition.n.01\n",
      "terrm structure.n.04 term_next resolution.n.04\n",
      "terrm structure.n.04 term_next computer_science.n.01\n",
      "terrm structure.n.04 term_next summons.n.03\n",
      "terrm structure.n.04 term_next hardware.n.03\n",
      "terrm structure.n.04 term_next foundation_garment.n.01\n",
      "terrm structure.n.04 term_next principle.n.04\n",
      "terrm structure.n.04 term_next stove.n.01\n",
      "terrm structure.n.04 term_next profession.n.01\n",
      "sphere.n.01\n",
      "terrm sphere.n.01 term_next methodology.n.02\n",
      "terrm sphere.n.01 term_next cognition.n.01\n",
      "terrm sphere.n.01 term_next resolution.n.04\n",
      "terrm sphere.n.01 term_next computer_science.n.01\n",
      "terrm sphere.n.01 term_next summons.n.03\n",
      "terrm sphere.n.01 term_next hardware.n.03\n",
      "terrm sphere.n.01 term_next foundation_garment.n.01\n",
      "terrm sphere.n.01 term_next principle.n.04\n",
      "terrm sphere.n.01 term_next stove.n.01\n",
      "terrm sphere.n.01 term_next profession.n.01\n",
      "major.n.04\n",
      "terrm major.n.04 term_next methodology.n.02\n",
      "terrm major.n.04 term_next cognition.n.01\n",
      "terrm major.n.04 term_next resolution.n.04\n",
      "terrm major.n.04 term_next computer_science.n.01\n",
      "terrm major.n.04 term_next summons.n.03\n",
      "terrm major.n.04 term_next hardware.n.03\n",
      "terrm major.n.04 term_next foundation_garment.n.01\n",
      "terrm major.n.04 term_next principle.n.04\n",
      "terrm major.n.04 term_next stove.n.01\n",
      "terrm major.n.04 term_next profession.n.01\n",
      "machine.n.05\n",
      "terrm machine.n.05 term_next methodology.n.02\n",
      "terrm machine.n.05 term_next cognition.n.01\n",
      "terrm machine.n.05 term_next resolution.n.04\n",
      "terrm machine.n.05 term_next computer_science.n.01\n",
      "terrm machine.n.05 term_next summons.n.03\n",
      "terrm machine.n.05 term_next hardware.n.03\n",
      "terrm machine.n.05 term_next foundation_garment.n.01\n",
      "terrm machine.n.05 term_next principle.n.04\n",
      "terrm machine.n.05 term_next stove.n.01\n",
      "terrm machine.n.05 term_next profession.n.01\n",
      "storehouse.n.01\n",
      "terrm storehouse.n.01 term_next methodology.n.02\n",
      "terrm storehouse.n.01 term_next cognition.n.01\n",
      "terrm storehouse.n.01 term_next resolution.n.04\n",
      "terrm storehouse.n.01 term_next computer_science.n.01\n",
      "terrm storehouse.n.01 term_next summons.n.03\n",
      "terrm storehouse.n.01 term_next hardware.n.03\n",
      "terrm storehouse.n.01 term_next foundation_garment.n.01\n",
      "terrm storehouse.n.01 term_next principle.n.04\n",
      "terrm storehouse.n.01 term_next stove.n.01\n",
      "terrm storehouse.n.01 term_next profession.n.01\n",
      "valet.n.01\n",
      "terrm valet.n.01 term_next methodology.n.02\n",
      "terrm valet.n.01 term_next cognition.n.01\n",
      "terrm valet.n.01 term_next resolution.n.04\n",
      "terrm valet.n.01 term_next computer_science.n.01\n",
      "terrm valet.n.01 term_next summons.n.03\n",
      "terrm valet.n.01 term_next hardware.n.03\n",
      "terrm valet.n.01 term_next foundation_garment.n.01\n",
      "terrm valet.n.01 term_next principle.n.04\n",
      "terrm valet.n.01 term_next stove.n.01\n",
      "terrm valet.n.01 term_next profession.n.01\n",
      "terminology.n.01\n",
      "terrm terminology.n.01 term_next methodology.n.02\n",
      "terrm terminology.n.01 term_next cognition.n.01\n",
      "terrm terminology.n.01 term_next resolution.n.04\n",
      "terrm terminology.n.01 term_next computer_science.n.01\n",
      "terrm terminology.n.01 term_next summons.n.03\n",
      "terrm terminology.n.01 term_next hardware.n.03\n",
      "terrm terminology.n.01 term_next foundation_garment.n.01\n",
      "terrm terminology.n.01 term_next principle.n.04\n",
      "terrm terminology.n.01 term_next stove.n.01\n",
      "terrm terminology.n.01 term_next profession.n.01\n",
      "theory.n.03\n",
      "terrm theory.n.03 term_next methodology.n.02\n",
      "terrm theory.n.03 term_next cognition.n.01\n",
      "terrm theory.n.03 term_next resolution.n.04\n",
      "terrm theory.n.03 term_next computer_science.n.01\n",
      "terrm theory.n.03 term_next summons.n.03\n",
      "terrm theory.n.03 term_next hardware.n.03\n",
      "terrm theory.n.03 term_next foundation_garment.n.01\n",
      "terrm theory.n.03 term_next principle.n.04\n",
      "terrm theory.n.03 term_next stove.n.01\n",
      "terrm theory.n.03 term_next profession.n.01\n",
      "complex.n.03\n",
      "terrm complex.n.03 term_next methodology.n.02\n",
      "terrm complex.n.03 term_next cognition.n.01\n",
      "terrm complex.n.03 term_next resolution.n.04\n",
      "terrm complex.n.03 term_next computer_science.n.01\n",
      "terrm complex.n.03 term_next summons.n.03\n",
      "terrm complex.n.03 term_next hardware.n.03\n",
      "terrm complex.n.03 term_next foundation_garment.n.01\n",
      "terrm complex.n.03 term_next principle.n.04\n",
      "terrm complex.n.03 term_next stove.n.01\n",
      "terrm complex.n.03 term_next profession.n.01\n",
      "information.n.05\n",
      "terrm information.n.05 term_next methodology.n.02\n",
      "terrm information.n.05 term_next cognition.n.01\n",
      "terrm information.n.05 term_next resolution.n.04\n",
      "terrm information.n.05 term_next computer_science.n.01\n",
      "terrm information.n.05 term_next summons.n.03\n",
      "terrm information.n.05 term_next hardware.n.03\n",
      "terrm information.n.05 term_next foundation_garment.n.01\n",
      "terrm information.n.05 term_next principle.n.04\n",
      "terrm information.n.05 term_next stove.n.01\n",
      "terrm information.n.05 term_next profession.n.01\n",
      "job.n.02\n",
      "terrm job.n.02 term_next methodology.n.02\n",
      "terrm job.n.02 term_next cognition.n.01\n",
      "terrm job.n.02 term_next resolution.n.04\n",
      "terrm job.n.02 term_next computer_science.n.01\n",
      "terrm job.n.02 term_next summons.n.03\n",
      "terrm job.n.02 term_next hardware.n.03\n",
      "terrm job.n.02 term_next foundation_garment.n.01\n",
      "terrm job.n.02 term_next principle.n.04\n",
      "terrm job.n.02 term_next stove.n.01\n",
      "terrm job.n.02 term_next profession.n.01\n",
      "data.n.01\n",
      "terrm data.n.01 term_next methodology.n.02\n",
      "terrm data.n.01 term_next cognition.n.01\n",
      "terrm data.n.01 term_next resolution.n.04\n",
      "terrm data.n.01 term_next computer_science.n.01\n",
      "terrm data.n.01 term_next summons.n.03\n",
      "terrm data.n.01 term_next hardware.n.03\n",
      "terrm data.n.01 term_next foundation_garment.n.01\n",
      "terrm data.n.01 term_next principle.n.04\n",
      "terrm data.n.01 term_next stove.n.01\n",
      "terrm data.n.01 term_next profession.n.01\n",
      "growth.n.01\n",
      "terrm growth.n.01 term_next methodology.n.02\n",
      "terrm growth.n.01 term_next cognition.n.01\n",
      "terrm growth.n.01 term_next resolution.n.04\n",
      "terrm growth.n.01 term_next computer_science.n.01\n",
      "terrm growth.n.01 term_next summons.n.03\n",
      "terrm growth.n.01 term_next hardware.n.03\n",
      "terrm growth.n.01 term_next foundation_garment.n.01\n",
      "terrm growth.n.01 term_next principle.n.04\n",
      "terrm growth.n.01 term_next stove.n.01\n",
      "terrm growth.n.01 term_next profession.n.01\n",
      "use.n.03\n",
      "terrm use.n.03 term_next methodology.n.02\n",
      "terrm use.n.03 term_next cognition.n.01\n",
      "terrm use.n.03 term_next resolution.n.04\n",
      "terrm use.n.03 term_next computer_science.n.01\n",
      "terrm use.n.03 term_next summons.n.03\n",
      "terrm use.n.03 term_next hardware.n.03\n",
      "terrm use.n.03 term_next foundation_garment.n.01\n",
      "terrm use.n.03 term_next principle.n.04\n",
      "terrm use.n.03 term_next stove.n.01\n",
      "terrm use.n.03 term_next profession.n.01\n",
      "previous_file_name /home/fsg/Desktop/csv-fatma/sub_word_tf/cscs2.csv\n",
      "pure_file_name cscs2.csv\n",
      "programming.n.02\n",
      "terrm programming.n.02 term_next meet.n.01\n",
      "terrm programming.n.02 term_next one.n.02\n",
      "terrm programming.n.02 term_next fundamental.n.02\n",
      "terrm programming.n.02 term_next concept.n.01\n",
      "terrm programming.n.02 term_next lotion.n.02\n",
      "terrm programming.n.02 term_next interaction.n.02\n",
      "terrm programming.n.02 term_next frankincense.n.01\n",
      "terrm programming.n.02 term_next focus.n.07\n",
      "terrm programming.n.02 term_next particular.n.03\n",
      "terrm programming.n.02 term_next well.n.05\n",
      "terrm programming.n.02 term_next understanding.n.01\n",
      "terrm programming.n.02 term_next intelligence.n.02\n",
      "terrm programming.n.02 term_next stipulation.n.03\n",
      "method.n.01\n",
      "terrm method.n.01 term_next meet.n.01\n",
      "terrm method.n.01 term_next one.n.02\n",
      "terrm method.n.01 term_next fundamental.n.02\n",
      "terrm method.n.01 term_next concept.n.01\n",
      "terrm method.n.01 term_next lotion.n.02\n",
      "terrm method.n.01 term_next interaction.n.02\n",
      "terrm method.n.01 term_next frankincense.n.01\n",
      "terrm method.n.01 term_next focus.n.07\n",
      "terrm method.n.01 term_next particular.n.03\n",
      "terrm method.n.01 term_next well.n.05\n",
      "terrm method.n.01 term_next understanding.n.01\n",
      "terrm method.n.01 term_next intelligence.n.02\n",
      "terrm method.n.01 term_next stipulation.n.03\n",
      "structure.n.04\n",
      "terrm structure.n.04 term_next meet.n.01\n",
      "terrm structure.n.04 term_next one.n.02\n",
      "terrm structure.n.04 term_next fundamental.n.02\n",
      "terrm structure.n.04 term_next concept.n.01\n",
      "terrm structure.n.04 term_next lotion.n.02\n",
      "terrm structure.n.04 term_next interaction.n.02\n",
      "terrm structure.n.04 term_next frankincense.n.01\n",
      "terrm structure.n.04 term_next focus.n.07\n",
      "terrm structure.n.04 term_next particular.n.03\n",
      "terrm structure.n.04 term_next well.n.05\n",
      "terrm structure.n.04 term_next understanding.n.01\n",
      "terrm structure.n.04 term_next intelligence.n.02\n",
      "terrm structure.n.04 term_next stipulation.n.03\n",
      "sphere.n.01\n",
      "terrm sphere.n.01 term_next meet.n.01\n",
      "terrm sphere.n.01 term_next one.n.02\n",
      "terrm sphere.n.01 term_next fundamental.n.02\n",
      "terrm sphere.n.01 term_next concept.n.01\n",
      "terrm sphere.n.01 term_next lotion.n.02\n",
      "terrm sphere.n.01 term_next interaction.n.02\n",
      "terrm sphere.n.01 term_next frankincense.n.01\n",
      "terrm sphere.n.01 term_next focus.n.07\n",
      "terrm sphere.n.01 term_next particular.n.03\n",
      "terrm sphere.n.01 term_next well.n.05\n",
      "terrm sphere.n.01 term_next understanding.n.01\n",
      "terrm sphere.n.01 term_next intelligence.n.02\n",
      "terrm sphere.n.01 term_next stipulation.n.03\n",
      "major.n.04\n",
      "terrm major.n.04 term_next meet.n.01\n",
      "terrm major.n.04 term_next one.n.02\n",
      "terrm major.n.04 term_next fundamental.n.02\n",
      "terrm major.n.04 term_next concept.n.01\n",
      "terrm major.n.04 term_next lotion.n.02\n",
      "terrm major.n.04 term_next interaction.n.02\n",
      "terrm major.n.04 term_next frankincense.n.01\n",
      "terrm major.n.04 term_next focus.n.07\n",
      "terrm major.n.04 term_next particular.n.03\n",
      "terrm major.n.04 term_next well.n.05\n",
      "terrm major.n.04 term_next understanding.n.01\n",
      "terrm major.n.04 term_next intelligence.n.02\n",
      "terrm major.n.04 term_next stipulation.n.03\n",
      "machine.n.05\n",
      "terrm machine.n.05 term_next meet.n.01\n",
      "terrm machine.n.05 term_next one.n.02\n",
      "terrm machine.n.05 term_next fundamental.n.02\n",
      "terrm machine.n.05 term_next concept.n.01\n",
      "terrm machine.n.05 term_next lotion.n.02\n",
      "terrm machine.n.05 term_next interaction.n.02\n",
      "terrm machine.n.05 term_next frankincense.n.01\n",
      "terrm machine.n.05 term_next focus.n.07\n",
      "terrm machine.n.05 term_next particular.n.03\n",
      "terrm machine.n.05 term_next well.n.05\n",
      "terrm machine.n.05 term_next understanding.n.01\n",
      "terrm machine.n.05 term_next intelligence.n.02\n",
      "terrm machine.n.05 term_next stipulation.n.03\n",
      "storehouse.n.01\n",
      "terrm storehouse.n.01 term_next meet.n.01\n",
      "terrm storehouse.n.01 term_next one.n.02\n",
      "terrm storehouse.n.01 term_next fundamental.n.02\n",
      "terrm storehouse.n.01 term_next concept.n.01\n",
      "terrm storehouse.n.01 term_next lotion.n.02\n",
      "terrm storehouse.n.01 term_next interaction.n.02\n",
      "terrm storehouse.n.01 term_next frankincense.n.01\n",
      "terrm storehouse.n.01 term_next focus.n.07\n",
      "terrm storehouse.n.01 term_next particular.n.03\n",
      "terrm storehouse.n.01 term_next well.n.05\n",
      "terrm storehouse.n.01 term_next understanding.n.01\n",
      "terrm storehouse.n.01 term_next intelligence.n.02\n",
      "terrm storehouse.n.01 term_next stipulation.n.03\n",
      "valet.n.01\n",
      "terrm valet.n.01 term_next meet.n.01\n",
      "terrm valet.n.01 term_next one.n.02\n",
      "terrm valet.n.01 term_next fundamental.n.02\n",
      "terrm valet.n.01 term_next concept.n.01\n",
      "terrm valet.n.01 term_next lotion.n.02\n",
      "terrm valet.n.01 term_next interaction.n.02\n",
      "terrm valet.n.01 term_next frankincense.n.01\n",
      "terrm valet.n.01 term_next focus.n.07\n",
      "terrm valet.n.01 term_next particular.n.03\n",
      "terrm valet.n.01 term_next well.n.05\n",
      "terrm valet.n.01 term_next understanding.n.01\n",
      "terrm valet.n.01 term_next intelligence.n.02\n",
      "terrm valet.n.01 term_next stipulation.n.03\n",
      "terminology.n.01\n",
      "terrm terminology.n.01 term_next meet.n.01\n",
      "terrm terminology.n.01 term_next one.n.02\n",
      "terrm terminology.n.01 term_next fundamental.n.02\n",
      "terrm terminology.n.01 term_next concept.n.01\n",
      "terrm terminology.n.01 term_next lotion.n.02\n",
      "terrm terminology.n.01 term_next interaction.n.02\n",
      "terrm terminology.n.01 term_next frankincense.n.01\n",
      "terrm terminology.n.01 term_next focus.n.07\n",
      "terrm terminology.n.01 term_next particular.n.03\n",
      "terrm terminology.n.01 term_next well.n.05\n",
      "terrm terminology.n.01 term_next understanding.n.01\n",
      "terrm terminology.n.01 term_next intelligence.n.02\n",
      "terrm terminology.n.01 term_next stipulation.n.03\n",
      "theory.n.03\n",
      "terrm theory.n.03 term_next meet.n.01\n",
      "terrm theory.n.03 term_next one.n.02\n",
      "terrm theory.n.03 term_next fundamental.n.02\n",
      "terrm theory.n.03 term_next concept.n.01\n",
      "terrm theory.n.03 term_next lotion.n.02\n",
      "terrm theory.n.03 term_next interaction.n.02\n",
      "terrm theory.n.03 term_next frankincense.n.01\n",
      "terrm theory.n.03 term_next focus.n.07\n",
      "terrm theory.n.03 term_next particular.n.03\n",
      "terrm theory.n.03 term_next well.n.05\n",
      "terrm theory.n.03 term_next understanding.n.01\n",
      "terrm theory.n.03 term_next intelligence.n.02\n",
      "terrm theory.n.03 term_next stipulation.n.03\n",
      "complex.n.03\n",
      "terrm complex.n.03 term_next meet.n.01\n",
      "terrm complex.n.03 term_next one.n.02\n",
      "terrm complex.n.03 term_next fundamental.n.02\n",
      "terrm complex.n.03 term_next concept.n.01\n",
      "terrm complex.n.03 term_next lotion.n.02\n",
      "terrm complex.n.03 term_next interaction.n.02\n",
      "terrm complex.n.03 term_next frankincense.n.01\n",
      "terrm complex.n.03 term_next focus.n.07\n",
      "terrm complex.n.03 term_next particular.n.03\n",
      "terrm complex.n.03 term_next well.n.05\n",
      "terrm complex.n.03 term_next understanding.n.01\n",
      "terrm complex.n.03 term_next intelligence.n.02\n",
      "terrm complex.n.03 term_next stipulation.n.03\n",
      "information.n.05\n",
      "terrm information.n.05 term_next meet.n.01\n",
      "terrm information.n.05 term_next one.n.02\n",
      "terrm information.n.05 term_next fundamental.n.02\n",
      "terrm information.n.05 term_next concept.n.01\n",
      "terrm information.n.05 term_next lotion.n.02\n",
      "terrm information.n.05 term_next interaction.n.02\n",
      "terrm information.n.05 term_next frankincense.n.01\n",
      "terrm information.n.05 term_next focus.n.07\n",
      "terrm information.n.05 term_next particular.n.03\n",
      "terrm information.n.05 term_next well.n.05\n",
      "terrm information.n.05 term_next understanding.n.01\n",
      "terrm information.n.05 term_next intelligence.n.02\n",
      "terrm information.n.05 term_next stipulation.n.03\n",
      "job.n.02\n",
      "terrm job.n.02 term_next meet.n.01\n",
      "terrm job.n.02 term_next one.n.02\n",
      "terrm job.n.02 term_next fundamental.n.02\n",
      "terrm job.n.02 term_next concept.n.01\n",
      "terrm job.n.02 term_next lotion.n.02\n",
      "terrm job.n.02 term_next interaction.n.02\n",
      "terrm job.n.02 term_next frankincense.n.01\n",
      "terrm job.n.02 term_next focus.n.07\n",
      "terrm job.n.02 term_next particular.n.03\n",
      "terrm job.n.02 term_next well.n.05\n",
      "terrm job.n.02 term_next understanding.n.01\n",
      "terrm job.n.02 term_next intelligence.n.02\n",
      "terrm job.n.02 term_next stipulation.n.03\n",
      "data.n.01\n",
      "terrm data.n.01 term_next meet.n.01\n",
      "terrm data.n.01 term_next one.n.02\n",
      "terrm data.n.01 term_next fundamental.n.02\n",
      "terrm data.n.01 term_next concept.n.01\n",
      "terrm data.n.01 term_next lotion.n.02\n",
      "terrm data.n.01 term_next interaction.n.02\n",
      "terrm data.n.01 term_next frankincense.n.01\n",
      "terrm data.n.01 term_next focus.n.07\n",
      "terrm data.n.01 term_next particular.n.03\n",
      "terrm data.n.01 term_next well.n.05\n",
      "terrm data.n.01 term_next understanding.n.01\n",
      "terrm data.n.01 term_next intelligence.n.02\n",
      "terrm data.n.01 term_next stipulation.n.03\n",
      "growth.n.01\n",
      "terrm growth.n.01 term_next meet.n.01\n",
      "terrm growth.n.01 term_next one.n.02\n",
      "terrm growth.n.01 term_next fundamental.n.02\n",
      "terrm growth.n.01 term_next concept.n.01\n",
      "terrm growth.n.01 term_next lotion.n.02\n",
      "terrm growth.n.01 term_next interaction.n.02\n",
      "terrm growth.n.01 term_next frankincense.n.01\n",
      "terrm growth.n.01 term_next focus.n.07\n",
      "terrm growth.n.01 term_next particular.n.03\n",
      "terrm growth.n.01 term_next well.n.05\n",
      "terrm growth.n.01 term_next understanding.n.01\n",
      "terrm growth.n.01 term_next intelligence.n.02\n",
      "terrm growth.n.01 term_next stipulation.n.03\n",
      "use.n.03\n",
      "terrm use.n.03 term_next meet.n.01\n",
      "terrm use.n.03 term_next one.n.02\n",
      "terrm use.n.03 term_next fundamental.n.02\n",
      "terrm use.n.03 term_next concept.n.01\n",
      "terrm use.n.03 term_next lotion.n.02\n",
      "terrm use.n.03 term_next interaction.n.02\n",
      "terrm use.n.03 term_next frankincense.n.01\n",
      "terrm use.n.03 term_next focus.n.07\n",
      "terrm use.n.03 term_next particular.n.03\n",
      "terrm use.n.03 term_next well.n.05\n",
      "terrm use.n.03 term_next understanding.n.01\n",
      "terrm use.n.03 term_next intelligence.n.02\n",
      "terrm use.n.03 term_next stipulation.n.03\n",
      "previous_file_name /home/fsg/Desktop/csv-fatma/sub_word_tf/cscs3.csv\n",
      "pure_file_name cscs3.csv\n",
      "programming.n.02\n",
      "terrm programming.n.02 term_next design.n.06\n",
      "terrm programming.n.02 term_next discipline.n.02\n",
      "terrm programming.n.02 term_next psychoanalysis.n.01\n",
      "terrm programming.n.02 term_next computer_architecture.n.02\n",
      "terrm programming.n.02 term_next technique.n.01\n",
      "terrm programming.n.02 term_next software.n.01\n",
      "terrm programming.n.02 term_next trouble.n.01\n",
      "terrm programming.n.02 term_next algorithm.n.01\n",
      "terrm programming.n.02 term_next communication.n.03\n",
      "terrm programming.n.02 term_next system.n.08\n",
      "terrm programming.n.02 term_next engineering.n.03\n",
      "method.n.01\n",
      "terrm method.n.01 term_next design.n.06\n",
      "terrm method.n.01 term_next discipline.n.02\n",
      "terrm method.n.01 term_next psychoanalysis.n.01\n",
      "terrm method.n.01 term_next computer_architecture.n.02\n",
      "terrm method.n.01 term_next technique.n.01\n",
      "terrm method.n.01 term_next software.n.01\n",
      "terrm method.n.01 term_next trouble.n.01\n",
      "terrm method.n.01 term_next algorithm.n.01\n",
      "terrm method.n.01 term_next communication.n.03\n",
      "terrm method.n.01 term_next system.n.08\n",
      "terrm method.n.01 term_next engineering.n.03\n",
      "structure.n.04\n",
      "terrm structure.n.04 term_next design.n.06\n",
      "terrm structure.n.04 term_next discipline.n.02\n",
      "terrm structure.n.04 term_next psychoanalysis.n.01\n",
      "terrm structure.n.04 term_next computer_architecture.n.02\n",
      "terrm structure.n.04 term_next technique.n.01\n",
      "terrm structure.n.04 term_next software.n.01\n",
      "terrm structure.n.04 term_next trouble.n.01\n",
      "terrm structure.n.04 term_next algorithm.n.01\n",
      "terrm structure.n.04 term_next communication.n.03\n",
      "terrm structure.n.04 term_next system.n.08\n",
      "terrm structure.n.04 term_next engineering.n.03\n",
      "sphere.n.01\n",
      "terrm sphere.n.01 term_next design.n.06\n",
      "terrm sphere.n.01 term_next discipline.n.02\n",
      "terrm sphere.n.01 term_next psychoanalysis.n.01\n",
      "terrm sphere.n.01 term_next computer_architecture.n.02\n",
      "terrm sphere.n.01 term_next technique.n.01\n",
      "terrm sphere.n.01 term_next software.n.01\n",
      "terrm sphere.n.01 term_next trouble.n.01\n",
      "terrm sphere.n.01 term_next algorithm.n.01\n",
      "terrm sphere.n.01 term_next communication.n.03\n",
      "terrm sphere.n.01 term_next system.n.08\n",
      "terrm sphere.n.01 term_next engineering.n.03\n",
      "major.n.04\n",
      "terrm major.n.04 term_next design.n.06\n",
      "terrm major.n.04 term_next discipline.n.02\n",
      "terrm major.n.04 term_next psychoanalysis.n.01\n",
      "terrm major.n.04 term_next computer_architecture.n.02\n",
      "terrm major.n.04 term_next technique.n.01\n",
      "terrm major.n.04 term_next software.n.01\n",
      "terrm major.n.04 term_next trouble.n.01\n",
      "terrm major.n.04 term_next algorithm.n.01\n",
      "terrm major.n.04 term_next communication.n.03\n",
      "terrm major.n.04 term_next system.n.08\n",
      "terrm major.n.04 term_next engineering.n.03\n",
      "machine.n.05\n",
      "terrm machine.n.05 term_next design.n.06\n",
      "terrm machine.n.05 term_next discipline.n.02\n",
      "terrm machine.n.05 term_next psychoanalysis.n.01\n",
      "terrm machine.n.05 term_next computer_architecture.n.02\n",
      "terrm machine.n.05 term_next technique.n.01\n",
      "terrm machine.n.05 term_next software.n.01\n",
      "terrm machine.n.05 term_next trouble.n.01\n",
      "terrm machine.n.05 term_next algorithm.n.01\n",
      "terrm machine.n.05 term_next communication.n.03\n",
      "terrm machine.n.05 term_next system.n.08\n",
      "terrm machine.n.05 term_next engineering.n.03\n",
      "storehouse.n.01\n",
      "terrm storehouse.n.01 term_next design.n.06\n",
      "terrm storehouse.n.01 term_next discipline.n.02\n",
      "terrm storehouse.n.01 term_next psychoanalysis.n.01\n",
      "terrm storehouse.n.01 term_next computer_architecture.n.02\n",
      "terrm storehouse.n.01 term_next technique.n.01\n",
      "terrm storehouse.n.01 term_next software.n.01\n",
      "terrm storehouse.n.01 term_next trouble.n.01\n",
      "terrm storehouse.n.01 term_next algorithm.n.01\n",
      "terrm storehouse.n.01 term_next communication.n.03\n",
      "terrm storehouse.n.01 term_next system.n.08\n",
      "terrm storehouse.n.01 term_next engineering.n.03\n",
      "valet.n.01\n",
      "terrm valet.n.01 term_next design.n.06\n",
      "terrm valet.n.01 term_next discipline.n.02\n",
      "terrm valet.n.01 term_next psychoanalysis.n.01\n",
      "terrm valet.n.01 term_next computer_architecture.n.02\n",
      "terrm valet.n.01 term_next technique.n.01\n",
      "terrm valet.n.01 term_next software.n.01\n",
      "terrm valet.n.01 term_next trouble.n.01\n",
      "terrm valet.n.01 term_next algorithm.n.01\n",
      "terrm valet.n.01 term_next communication.n.03\n",
      "terrm valet.n.01 term_next system.n.08\n",
      "terrm valet.n.01 term_next engineering.n.03\n",
      "terminology.n.01\n",
      "terrm terminology.n.01 term_next design.n.06\n",
      "terrm terminology.n.01 term_next discipline.n.02\n",
      "terrm terminology.n.01 term_next psychoanalysis.n.01\n",
      "terrm terminology.n.01 term_next computer_architecture.n.02\n",
      "terrm terminology.n.01 term_next technique.n.01\n",
      "terrm terminology.n.01 term_next software.n.01\n",
      "terrm terminology.n.01 term_next trouble.n.01\n",
      "terrm terminology.n.01 term_next algorithm.n.01\n",
      "terrm terminology.n.01 term_next communication.n.03\n",
      "terrm terminology.n.01 term_next system.n.08\n",
      "terrm terminology.n.01 term_next engineering.n.03\n",
      "theory.n.03\n",
      "terrm theory.n.03 term_next design.n.06\n",
      "terrm theory.n.03 term_next discipline.n.02\n",
      "terrm theory.n.03 term_next psychoanalysis.n.01\n",
      "terrm theory.n.03 term_next computer_architecture.n.02\n",
      "terrm theory.n.03 term_next technique.n.01\n",
      "terrm theory.n.03 term_next software.n.01\n",
      "terrm theory.n.03 term_next trouble.n.01\n",
      "terrm theory.n.03 term_next algorithm.n.01\n",
      "terrm theory.n.03 term_next communication.n.03\n",
      "terrm theory.n.03 term_next system.n.08\n",
      "terrm theory.n.03 term_next engineering.n.03\n",
      "complex.n.03\n",
      "terrm complex.n.03 term_next design.n.06\n",
      "terrm complex.n.03 term_next discipline.n.02\n",
      "terrm complex.n.03 term_next psychoanalysis.n.01\n",
      "terrm complex.n.03 term_next computer_architecture.n.02\n",
      "terrm complex.n.03 term_next technique.n.01\n",
      "terrm complex.n.03 term_next software.n.01\n",
      "terrm complex.n.03 term_next trouble.n.01\n",
      "terrm complex.n.03 term_next algorithm.n.01\n",
      "terrm complex.n.03 term_next communication.n.03\n",
      "terrm complex.n.03 term_next system.n.08\n",
      "terrm complex.n.03 term_next engineering.n.03\n",
      "information.n.05\n",
      "terrm information.n.05 term_next design.n.06\n",
      "terrm information.n.05 term_next discipline.n.02\n",
      "terrm information.n.05 term_next psychoanalysis.n.01\n",
      "terrm information.n.05 term_next computer_architecture.n.02\n",
      "terrm information.n.05 term_next technique.n.01\n",
      "terrm information.n.05 term_next software.n.01\n",
      "terrm information.n.05 term_next trouble.n.01\n",
      "terrm information.n.05 term_next algorithm.n.01\n",
      "terrm information.n.05 term_next communication.n.03\n",
      "terrm information.n.05 term_next system.n.08\n",
      "terrm information.n.05 term_next engineering.n.03\n",
      "job.n.02\n",
      "terrm job.n.02 term_next design.n.06\n",
      "terrm job.n.02 term_next discipline.n.02\n",
      "terrm job.n.02 term_next psychoanalysis.n.01\n",
      "terrm job.n.02 term_next computer_architecture.n.02\n",
      "terrm job.n.02 term_next technique.n.01\n",
      "terrm job.n.02 term_next software.n.01\n",
      "terrm job.n.02 term_next trouble.n.01\n",
      "terrm job.n.02 term_next algorithm.n.01\n",
      "terrm job.n.02 term_next communication.n.03\n",
      "terrm job.n.02 term_next system.n.08\n",
      "terrm job.n.02 term_next engineering.n.03\n",
      "data.n.01\n",
      "terrm data.n.01 term_next design.n.06\n",
      "terrm data.n.01 term_next discipline.n.02\n",
      "terrm data.n.01 term_next psychoanalysis.n.01\n",
      "terrm data.n.01 term_next computer_architecture.n.02\n",
      "terrm data.n.01 term_next technique.n.01\n",
      "terrm data.n.01 term_next software.n.01\n",
      "terrm data.n.01 term_next trouble.n.01\n",
      "terrm data.n.01 term_next algorithm.n.01\n",
      "terrm data.n.01 term_next communication.n.03\n",
      "terrm data.n.01 term_next system.n.08\n",
      "terrm data.n.01 term_next engineering.n.03\n",
      "growth.n.01\n",
      "terrm growth.n.01 term_next design.n.06\n",
      "terrm growth.n.01 term_next discipline.n.02\n",
      "terrm growth.n.01 term_next psychoanalysis.n.01\n",
      "terrm growth.n.01 term_next computer_architecture.n.02\n",
      "terrm growth.n.01 term_next technique.n.01\n",
      "terrm growth.n.01 term_next software.n.01\n",
      "terrm growth.n.01 term_next trouble.n.01\n",
      "terrm growth.n.01 term_next algorithm.n.01\n",
      "terrm growth.n.01 term_next communication.n.03\n",
      "terrm growth.n.01 term_next system.n.08\n",
      "terrm growth.n.01 term_next engineering.n.03\n",
      "use.n.03\n",
      "terrm use.n.03 term_next design.n.06\n",
      "terrm use.n.03 term_next discipline.n.02\n",
      "terrm use.n.03 term_next psychoanalysis.n.01\n",
      "terrm use.n.03 term_next computer_architecture.n.02\n",
      "terrm use.n.03 term_next technique.n.01\n",
      "terrm use.n.03 term_next software.n.01\n",
      "terrm use.n.03 term_next trouble.n.01\n",
      "terrm use.n.03 term_next algorithm.n.01\n",
      "terrm use.n.03 term_next communication.n.03\n",
      "terrm use.n.03 term_next system.n.08\n",
      "terrm use.n.03 term_next engineering.n.03\n",
      "finesed sim_terms_previous_file\n",
      "/home/fsg/Desktop/csv-fatma/sub_word_tf/cscs7.csv\n",
      "takno 7\n",
      "************Started Session GPU *************\n",
      "task_no 7 gpu_name /gpu:1\n",
      "cscs7.csv\n",
      "sim_terms_previous_file cscs7.csv\n",
      "previous_file_names_list ['/home/fsg/Desktop/csv-fatma/sub_word_tf/cscs1.csv', '/home/fsg/Desktop/csv-fatma/sub_word_tf/cscs4.csv', '/home/fsg/Desktop/csv-fatma/sub_word_tf/cscs6.csv', '/home/fsg/Desktop/csv-fatma/sub_word_tf/cscs2.csv', '/home/fsg/Desktop/csv-fatma/sub_word_tf/cscs3.csv']\n",
      "list_terms ['survey.n.01', 'science.n.01', 'computer.n.01', 'sense.n.05']\n",
      "previous_file_name /home/fsg/Desktop/csv-fatma/sub_word_tf/cscs1.csv\n",
      "pure_file_name cscs1.csv\n",
      "survey.n.01\n",
      "terrm survey.n.01 term_next graphics.n.02\n",
      "terrm survey.n.01 term_next mathematics.n.01\n",
      "terrm survey.n.01 term_next scientist.n.01\n",
      "terrm survey.n.01 term_next practice.n.04\n",
      "terrm survey.n.01 term_next logic.n.05\n",
      "terrm survey.n.01 term_next span.n.04\n",
      "terrm survey.n.01 term_next terminus.n.03\n",
      "terrm survey.n.01 term_next outline.n.02\n",
      "science.n.01\n",
      "terrm science.n.01 term_next graphics.n.02\n",
      "terrm science.n.01 term_next mathematics.n.01\n",
      "terrm science.n.01 term_next scientist.n.01\n",
      "terrm science.n.01 term_next practice.n.04\n",
      "terrm science.n.01 term_next logic.n.05\n",
      "terrm science.n.01 term_next span.n.04\n",
      "terrm science.n.01 term_next terminus.n.03\n",
      "terrm science.n.01 term_next outline.n.02\n",
      "computer.n.01\n",
      "terrm computer.n.01 term_next graphics.n.02\n",
      "terrm computer.n.01 term_next mathematics.n.01\n",
      "terrm computer.n.01 term_next scientist.n.01\n",
      "terrm computer.n.01 term_next practice.n.04\n",
      "terrm computer.n.01 term_next logic.n.05\n",
      "terrm computer.n.01 term_next span.n.04\n",
      "terrm computer.n.01 term_next terminus.n.03\n",
      "terrm computer.n.01 term_next outline.n.02\n",
      "sense.n.05\n",
      "terrm sense.n.05 term_next graphics.n.02\n",
      "terrm sense.n.05 term_next mathematics.n.01\n",
      "terrm sense.n.05 term_next scientist.n.01\n",
      "terrm sense.n.05 term_next practice.n.04\n",
      "terrm sense.n.05 term_next logic.n.05\n",
      "terrm sense.n.05 term_next span.n.04\n",
      "terrm sense.n.05 term_next terminus.n.03\n",
      "terrm sense.n.05 term_next outline.n.02\n",
      "previous_file_name /home/fsg/Desktop/csv-fatma/sub_word_tf/cscs4.csv\n",
      "pure_file_name cscs4.csv\n",
      "survey.n.01\n",
      "terrm survey.n.01 term_next methodology.n.02\n",
      "terrm survey.n.01 term_next cognition.n.01\n",
      "terrm survey.n.01 term_next resolution.n.04\n",
      "terrm survey.n.01 term_next computer_science.n.01\n",
      "terrm survey.n.01 term_next summons.n.03\n",
      "terrm survey.n.01 term_next hardware.n.03\n",
      "terrm survey.n.01 term_next foundation_garment.n.01\n",
      "terrm survey.n.01 term_next principle.n.04\n",
      "terrm survey.n.01 term_next stove.n.01\n",
      "terrm survey.n.01 term_next profession.n.01\n",
      "science.n.01\n",
      "terrm science.n.01 term_next methodology.n.02\n",
      "terrm science.n.01 term_next cognition.n.01\n",
      "terrm science.n.01 term_next resolution.n.04\n",
      "terrm science.n.01 term_next computer_science.n.01\n",
      "terrm science.n.01 term_next summons.n.03\n",
      "terrm science.n.01 term_next hardware.n.03\n",
      "terrm science.n.01 term_next foundation_garment.n.01\n",
      "terrm science.n.01 term_next principle.n.04\n",
      "terrm science.n.01 term_next stove.n.01\n",
      "terrm science.n.01 term_next profession.n.01\n",
      "computer.n.01\n",
      "terrm computer.n.01 term_next methodology.n.02\n",
      "terrm computer.n.01 term_next cognition.n.01\n",
      "terrm computer.n.01 term_next resolution.n.04\n",
      "terrm computer.n.01 term_next computer_science.n.01\n",
      "terrm computer.n.01 term_next summons.n.03\n",
      "terrm computer.n.01 term_next hardware.n.03\n",
      "terrm computer.n.01 term_next foundation_garment.n.01\n",
      "terrm computer.n.01 term_next principle.n.04\n",
      "terrm computer.n.01 term_next stove.n.01\n",
      "terrm computer.n.01 term_next profession.n.01\n",
      "sense.n.05\n",
      "terrm sense.n.05 term_next methodology.n.02\n",
      "terrm sense.n.05 term_next cognition.n.01\n",
      "terrm sense.n.05 term_next resolution.n.04\n",
      "terrm sense.n.05 term_next computer_science.n.01\n",
      "terrm sense.n.05 term_next summons.n.03\n",
      "terrm sense.n.05 term_next hardware.n.03\n",
      "terrm sense.n.05 term_next foundation_garment.n.01\n",
      "terrm sense.n.05 term_next principle.n.04\n",
      "terrm sense.n.05 term_next stove.n.01\n",
      "terrm sense.n.05 term_next profession.n.01\n",
      "previous_file_name /home/fsg/Desktop/csv-fatma/sub_word_tf/cscs6.csv\n",
      "pure_file_name cscs6.csv\n",
      "survey.n.01\n",
      "terrm survey.n.01 term_next programming.n.02\n",
      "terrm survey.n.01 term_next method.n.01\n",
      "terrm survey.n.01 term_next structure.n.04\n",
      "terrm survey.n.01 term_next sphere.n.01\n",
      "terrm survey.n.01 term_next major.n.04\n",
      "terrm survey.n.01 term_next machine.n.05\n",
      "terrm survey.n.01 term_next storehouse.n.01\n",
      "terrm survey.n.01 term_next valet.n.01\n",
      "terrm survey.n.01 term_next terminology.n.01\n",
      "terrm survey.n.01 term_next theory.n.03\n",
      "terrm survey.n.01 term_next complex.n.03\n",
      "terrm survey.n.01 term_next information.n.05\n",
      "terrm survey.n.01 term_next job.n.02\n",
      "terrm survey.n.01 term_next data.n.01\n",
      "terrm survey.n.01 term_next growth.n.01\n",
      "terrm survey.n.01 term_next use.n.03\n",
      "science.n.01\n",
      "terrm science.n.01 term_next programming.n.02\n",
      "terrm science.n.01 term_next method.n.01\n",
      "terrm science.n.01 term_next structure.n.04\n",
      "terrm science.n.01 term_next sphere.n.01\n",
      "terrm science.n.01 term_next major.n.04\n",
      "terrm science.n.01 term_next machine.n.05\n",
      "terrm science.n.01 term_next storehouse.n.01\n",
      "terrm science.n.01 term_next valet.n.01\n",
      "terrm science.n.01 term_next terminology.n.01\n",
      "terrm science.n.01 term_next theory.n.03\n",
      "terrm science.n.01 term_next complex.n.03\n",
      "terrm science.n.01 term_next information.n.05\n",
      "terrm science.n.01 term_next job.n.02\n",
      "terrm science.n.01 term_next data.n.01\n",
      "terrm science.n.01 term_next growth.n.01\n",
      "terrm science.n.01 term_next use.n.03\n",
      "computer.n.01\n",
      "terrm computer.n.01 term_next programming.n.02\n",
      "terrm computer.n.01 term_next method.n.01\n",
      "terrm computer.n.01 term_next structure.n.04\n",
      "terrm computer.n.01 term_next sphere.n.01\n",
      "terrm computer.n.01 term_next major.n.04\n",
      "terrm computer.n.01 term_next machine.n.05\n",
      "terrm computer.n.01 term_next storehouse.n.01\n",
      "terrm computer.n.01 term_next valet.n.01\n",
      "terrm computer.n.01 term_next terminology.n.01\n",
      "terrm computer.n.01 term_next theory.n.03\n",
      "terrm computer.n.01 term_next complex.n.03\n",
      "terrm computer.n.01 term_next information.n.05\n",
      "terrm computer.n.01 term_next job.n.02\n",
      "terrm computer.n.01 term_next data.n.01\n",
      "terrm computer.n.01 term_next growth.n.01\n",
      "terrm computer.n.01 term_next use.n.03\n",
      "sense.n.05\n",
      "terrm sense.n.05 term_next programming.n.02\n",
      "terrm sense.n.05 term_next method.n.01\n",
      "terrm sense.n.05 term_next structure.n.04\n",
      "terrm sense.n.05 term_next sphere.n.01\n",
      "terrm sense.n.05 term_next major.n.04\n",
      "terrm sense.n.05 term_next machine.n.05\n",
      "terrm sense.n.05 term_next storehouse.n.01\n",
      "terrm sense.n.05 term_next valet.n.01\n",
      "terrm sense.n.05 term_next terminology.n.01\n",
      "terrm sense.n.05 term_next theory.n.03\n",
      "terrm sense.n.05 term_next complex.n.03\n",
      "terrm sense.n.05 term_next information.n.05\n",
      "terrm sense.n.05 term_next job.n.02\n",
      "terrm sense.n.05 term_next data.n.01\n",
      "terrm sense.n.05 term_next growth.n.01\n",
      "terrm sense.n.05 term_next use.n.03\n",
      "previous_file_name /home/fsg/Desktop/csv-fatma/sub_word_tf/cscs2.csv\n",
      "pure_file_name cscs2.csv\n",
      "survey.n.01\n",
      "terrm survey.n.01 term_next meet.n.01\n",
      "terrm survey.n.01 term_next one.n.02\n",
      "terrm survey.n.01 term_next fundamental.n.02\n",
      "terrm survey.n.01 term_next concept.n.01\n",
      "terrm survey.n.01 term_next lotion.n.02\n",
      "terrm survey.n.01 term_next interaction.n.02\n",
      "terrm survey.n.01 term_next frankincense.n.01\n",
      "terrm survey.n.01 term_next focus.n.07\n",
      "terrm survey.n.01 term_next particular.n.03\n",
      "terrm survey.n.01 term_next well.n.05\n",
      "terrm survey.n.01 term_next understanding.n.01\n",
      "terrm survey.n.01 term_next intelligence.n.02\n",
      "terrm survey.n.01 term_next stipulation.n.03\n",
      "science.n.01\n",
      "terrm science.n.01 term_next meet.n.01\n",
      "terrm science.n.01 term_next one.n.02\n",
      "terrm science.n.01 term_next fundamental.n.02\n",
      "terrm science.n.01 term_next concept.n.01\n",
      "terrm science.n.01 term_next lotion.n.02\n",
      "terrm science.n.01 term_next interaction.n.02\n",
      "terrm science.n.01 term_next frankincense.n.01\n",
      "terrm science.n.01 term_next focus.n.07\n",
      "terrm science.n.01 term_next particular.n.03\n",
      "terrm science.n.01 term_next well.n.05\n",
      "terrm science.n.01 term_next understanding.n.01\n",
      "terrm science.n.01 term_next intelligence.n.02\n",
      "terrm science.n.01 term_next stipulation.n.03\n",
      "computer.n.01\n",
      "terrm computer.n.01 term_next meet.n.01\n",
      "terrm computer.n.01 term_next one.n.02\n",
      "terrm computer.n.01 term_next fundamental.n.02\n",
      "terrm computer.n.01 term_next concept.n.01\n",
      "terrm computer.n.01 term_next lotion.n.02\n",
      "terrm computer.n.01 term_next interaction.n.02\n",
      "terrm computer.n.01 term_next frankincense.n.01\n",
      "terrm computer.n.01 term_next focus.n.07\n",
      "terrm computer.n.01 term_next particular.n.03\n",
      "terrm computer.n.01 term_next well.n.05\n",
      "terrm computer.n.01 term_next understanding.n.01\n",
      "terrm computer.n.01 term_next intelligence.n.02\n",
      "terrm computer.n.01 term_next stipulation.n.03\n",
      "sense.n.05\n",
      "terrm sense.n.05 term_next meet.n.01\n",
      "terrm sense.n.05 term_next one.n.02\n",
      "terrm sense.n.05 term_next fundamental.n.02\n",
      "terrm sense.n.05 term_next concept.n.01\n",
      "terrm sense.n.05 term_next lotion.n.02\n",
      "terrm sense.n.05 term_next interaction.n.02\n",
      "terrm sense.n.05 term_next frankincense.n.01\n",
      "terrm sense.n.05 term_next focus.n.07\n",
      "terrm sense.n.05 term_next particular.n.03\n",
      "terrm sense.n.05 term_next well.n.05\n",
      "terrm sense.n.05 term_next understanding.n.01\n",
      "terrm sense.n.05 term_next intelligence.n.02\n",
      "terrm sense.n.05 term_next stipulation.n.03\n",
      "previous_file_name /home/fsg/Desktop/csv-fatma/sub_word_tf/cscs3.csv\n",
      "pure_file_name cscs3.csv\n",
      "survey.n.01\n",
      "terrm survey.n.01 term_next design.n.06\n",
      "terrm survey.n.01 term_next discipline.n.02\n",
      "terrm survey.n.01 term_next psychoanalysis.n.01\n",
      "terrm survey.n.01 term_next computer_architecture.n.02\n",
      "terrm survey.n.01 term_next technique.n.01\n",
      "terrm survey.n.01 term_next software.n.01\n",
      "terrm survey.n.01 term_next trouble.n.01\n",
      "terrm survey.n.01 term_next algorithm.n.01\n",
      "terrm survey.n.01 term_next communication.n.03\n",
      "terrm survey.n.01 term_next system.n.08\n",
      "terrm survey.n.01 term_next engineering.n.03\n",
      "science.n.01\n",
      "terrm science.n.01 term_next design.n.06\n",
      "terrm science.n.01 term_next discipline.n.02\n",
      "terrm science.n.01 term_next psychoanalysis.n.01\n",
      "terrm science.n.01 term_next computer_architecture.n.02\n",
      "terrm science.n.01 term_next technique.n.01\n",
      "terrm science.n.01 term_next software.n.01\n",
      "terrm science.n.01 term_next trouble.n.01\n",
      "terrm science.n.01 term_next algorithm.n.01\n",
      "terrm science.n.01 term_next communication.n.03\n",
      "terrm science.n.01 term_next system.n.08\n",
      "terrm science.n.01 term_next engineering.n.03\n",
      "computer.n.01\n",
      "terrm computer.n.01 term_next design.n.06\n",
      "terrm computer.n.01 term_next discipline.n.02\n",
      "terrm computer.n.01 term_next psychoanalysis.n.01\n",
      "terrm computer.n.01 term_next computer_architecture.n.02\n",
      "terrm computer.n.01 term_next technique.n.01\n",
      "terrm computer.n.01 term_next software.n.01\n",
      "terrm computer.n.01 term_next trouble.n.01\n",
      "terrm computer.n.01 term_next algorithm.n.01\n",
      "terrm computer.n.01 term_next communication.n.03\n",
      "terrm computer.n.01 term_next system.n.08\n",
      "terrm computer.n.01 term_next engineering.n.03\n",
      "sense.n.05\n",
      "terrm sense.n.05 term_next design.n.06\n",
      "terrm sense.n.05 term_next discipline.n.02\n",
      "terrm sense.n.05 term_next psychoanalysis.n.01\n",
      "terrm sense.n.05 term_next computer_architecture.n.02\n",
      "terrm sense.n.05 term_next technique.n.01\n",
      "terrm sense.n.05 term_next software.n.01\n",
      "terrm sense.n.05 term_next trouble.n.01\n",
      "terrm sense.n.05 term_next algorithm.n.01\n",
      "terrm sense.n.05 term_next communication.n.03\n",
      "terrm sense.n.05 term_next system.n.08\n",
      "terrm sense.n.05 term_next engineering.n.03\n",
      "finesed sim_terms_previous_file\n",
      "/home/fsg/Desktop/csv-fatma/sub_word_tf/cscs2.csv\n",
      "takno 2\n",
      "************Started Session GPU *************\n",
      "task_no 2 gpu_name /gpu:0\n",
      "cscs2.csv\n",
      "sim_terms_previous_file cscs2.csv\n",
      "previous_file_names_list ['/home/fsg/Desktop/csv-fatma/sub_word_tf/cscs1.csv']\n",
      "list_terms ['meet.n.01', 'one.n.02', 'fundamental.n.02', 'concept.n.01', 'lotion.n.02', 'interaction.n.02', 'frankincense.n.01', 'focus.n.07', 'particular.n.03', 'well.n.05', 'understanding.n.01', 'intelligence.n.02', 'stipulation.n.03']\n",
      "previous_file_name /home/fsg/Desktop/csv-fatma/sub_word_tf/cscs1.csv\n",
      "pure_file_name cscs1.csv\n",
      "meet.n.01\n",
      "terrm meet.n.01 term_next graphics.n.02\n",
      "terrm meet.n.01 term_next mathematics.n.01\n",
      "terrm meet.n.01 term_next scientist.n.01\n",
      "terrm meet.n.01 term_next practice.n.04\n",
      "terrm meet.n.01 term_next logic.n.05\n",
      "terrm meet.n.01 term_next span.n.04\n",
      "terrm meet.n.01 term_next terminus.n.03\n",
      "terrm meet.n.01 term_next outline.n.02\n",
      "one.n.02\n",
      "terrm one.n.02 term_next graphics.n.02\n",
      "terrm one.n.02 term_next mathematics.n.01\n",
      "terrm one.n.02 term_next scientist.n.01\n",
      "terrm one.n.02 term_next practice.n.04\n",
      "terrm one.n.02 term_next logic.n.05\n",
      "terrm one.n.02 term_next span.n.04\n",
      "terrm one.n.02 term_next terminus.n.03\n",
      "terrm one.n.02 term_next outline.n.02\n",
      "fundamental.n.02\n",
      "terrm fundamental.n.02 term_next graphics.n.02\n",
      "terrm fundamental.n.02 term_next mathematics.n.01\n",
      "terrm fundamental.n.02 term_next scientist.n.01\n",
      "terrm fundamental.n.02 term_next practice.n.04\n",
      "terrm fundamental.n.02 term_next logic.n.05\n",
      "terrm fundamental.n.02 term_next span.n.04\n",
      "terrm fundamental.n.02 term_next terminus.n.03\n",
      "terrm fundamental.n.02 term_next outline.n.02\n",
      "concept.n.01\n",
      "terrm concept.n.01 term_next graphics.n.02\n",
      "terrm concept.n.01 term_next mathematics.n.01\n",
      "terrm concept.n.01 term_next scientist.n.01\n",
      "terrm concept.n.01 term_next practice.n.04\n",
      "terrm concept.n.01 term_next logic.n.05\n",
      "terrm concept.n.01 term_next span.n.04\n",
      "terrm concept.n.01 term_next terminus.n.03\n",
      "terrm concept.n.01 term_next outline.n.02\n",
      "lotion.n.02\n",
      "terrm lotion.n.02 term_next graphics.n.02\n",
      "terrm lotion.n.02 term_next mathematics.n.01\n",
      "terrm lotion.n.02 term_next scientist.n.01\n",
      "terrm lotion.n.02 term_next practice.n.04\n",
      "terrm lotion.n.02 term_next logic.n.05\n",
      "terrm lotion.n.02 term_next span.n.04\n",
      "terrm lotion.n.02 term_next terminus.n.03\n",
      "terrm lotion.n.02 term_next outline.n.02\n",
      "interaction.n.02\n",
      "terrm interaction.n.02 term_next graphics.n.02\n",
      "terrm interaction.n.02 term_next mathematics.n.01\n",
      "terrm interaction.n.02 term_next scientist.n.01\n",
      "terrm interaction.n.02 term_next practice.n.04\n",
      "terrm interaction.n.02 term_next logic.n.05\n",
      "terrm interaction.n.02 term_next span.n.04\n",
      "terrm interaction.n.02 term_next terminus.n.03\n",
      "terrm interaction.n.02 term_next outline.n.02\n",
      "frankincense.n.01\n",
      "terrm frankincense.n.01 term_next graphics.n.02\n",
      "terrm frankincense.n.01 term_next mathematics.n.01\n",
      "terrm frankincense.n.01 term_next scientist.n.01\n",
      "terrm frankincense.n.01 term_next practice.n.04\n",
      "terrm frankincense.n.01 term_next logic.n.05\n",
      "terrm frankincense.n.01 term_next span.n.04\n",
      "terrm frankincense.n.01 term_next terminus.n.03\n",
      "terrm frankincense.n.01 term_next outline.n.02\n",
      "focus.n.07\n",
      "terrm focus.n.07 term_next graphics.n.02\n",
      "terrm focus.n.07 term_next mathematics.n.01\n",
      "terrm focus.n.07 term_next scientist.n.01\n",
      "terrm focus.n.07 term_next practice.n.04\n",
      "terrm focus.n.07 term_next logic.n.05\n",
      "terrm focus.n.07 term_next span.n.04\n",
      "terrm focus.n.07 term_next terminus.n.03\n",
      "terrm focus.n.07 term_next outline.n.02\n",
      "particular.n.03\n",
      "terrm particular.n.03 term_next graphics.n.02\n",
      "terrm particular.n.03 term_next mathematics.n.01\n",
      "terrm particular.n.03 term_next scientist.n.01\n",
      "terrm particular.n.03 term_next practice.n.04\n",
      "terrm particular.n.03 term_next logic.n.05\n",
      "terrm particular.n.03 term_next span.n.04\n",
      "terrm particular.n.03 term_next terminus.n.03\n",
      "terrm particular.n.03 term_next outline.n.02\n",
      "well.n.05\n",
      "terrm well.n.05 term_next graphics.n.02\n",
      "terrm well.n.05 term_next mathematics.n.01\n",
      "terrm well.n.05 term_next scientist.n.01\n",
      "terrm well.n.05 term_next practice.n.04\n",
      "terrm well.n.05 term_next logic.n.05\n",
      "terrm well.n.05 term_next span.n.04\n",
      "terrm well.n.05 term_next terminus.n.03\n",
      "terrm well.n.05 term_next outline.n.02\n",
      "understanding.n.01\n",
      "terrm understanding.n.01 term_next graphics.n.02\n",
      "terrm understanding.n.01 term_next mathematics.n.01\n",
      "terrm understanding.n.01 term_next scientist.n.01\n",
      "terrm understanding.n.01 term_next practice.n.04\n",
      "terrm understanding.n.01 term_next logic.n.05\n",
      "terrm understanding.n.01 term_next span.n.04\n",
      "terrm understanding.n.01 term_next terminus.n.03\n",
      "terrm understanding.n.01 term_next outline.n.02\n",
      "intelligence.n.02\n",
      "terrm intelligence.n.02 term_next graphics.n.02\n",
      "terrm intelligence.n.02 term_next mathematics.n.01\n",
      "terrm intelligence.n.02 term_next scientist.n.01\n",
      "terrm intelligence.n.02 term_next practice.n.04\n",
      "terrm intelligence.n.02 term_next logic.n.05\n",
      "terrm intelligence.n.02 term_next span.n.04\n",
      "terrm intelligence.n.02 term_next terminus.n.03\n",
      "terrm intelligence.n.02 term_next outline.n.02\n",
      "stipulation.n.03\n",
      "terrm stipulation.n.03 term_next graphics.n.02\n",
      "terrm stipulation.n.03 term_next mathematics.n.01\n",
      "terrm stipulation.n.03 term_next scientist.n.01\n",
      "terrm stipulation.n.03 term_next practice.n.04\n",
      "terrm stipulation.n.03 term_next logic.n.05\n",
      "terrm stipulation.n.03 term_next span.n.04\n",
      "terrm stipulation.n.03 term_next terminus.n.03\n",
      "terrm stipulation.n.03 term_next outline.n.02\n",
      "finesed sim_terms_previous_file\n",
      "/home/fsg/Desktop/csv-fatma/sub_word_tf/cscs3.csv\n",
      "takno 3\n",
      "************Started Session GPU *************\n",
      "task_no 3 gpu_name /gpu:1\n",
      "cscs3.csv\n",
      "sim_terms_previous_file cscs3.csv\n",
      "previous_file_names_list ['/home/fsg/Desktop/csv-fatma/sub_word_tf/cscs1.csv', '/home/fsg/Desktop/csv-fatma/sub_word_tf/cscs2.csv']\n",
      "list_terms ['design.n.06', 'discipline.n.02', 'psychoanalysis.n.01', 'computer_architecture.n.02', 'technique.n.01', 'software.n.01', 'trouble.n.01', 'algorithm.n.01', 'communication.n.03', 'system.n.08', 'engineering.n.03']\n",
      "previous_file_name /home/fsg/Desktop/csv-fatma/sub_word_tf/cscs1.csv\n",
      "pure_file_name cscs1.csv\n",
      "design.n.06\n",
      "terrm design.n.06 term_next graphics.n.02\n",
      "terrm design.n.06 term_next mathematics.n.01\n",
      "terrm design.n.06 term_next scientist.n.01\n",
      "terrm design.n.06 term_next practice.n.04\n",
      "terrm design.n.06 term_next logic.n.05\n",
      "terrm design.n.06 term_next span.n.04\n",
      "terrm design.n.06 term_next terminus.n.03\n",
      "terrm design.n.06 term_next outline.n.02\n",
      "discipline.n.02\n",
      "terrm discipline.n.02 term_next graphics.n.02\n",
      "terrm discipline.n.02 term_next mathematics.n.01\n",
      "terrm discipline.n.02 term_next scientist.n.01\n",
      "terrm discipline.n.02 term_next practice.n.04\n",
      "terrm discipline.n.02 term_next logic.n.05\n",
      "terrm discipline.n.02 term_next span.n.04\n",
      "terrm discipline.n.02 term_next terminus.n.03\n",
      "terrm discipline.n.02 term_next outline.n.02\n",
      "psychoanalysis.n.01\n",
      "terrm psychoanalysis.n.01 term_next graphics.n.02\n",
      "terrm psychoanalysis.n.01 term_next mathematics.n.01\n",
      "terrm psychoanalysis.n.01 term_next scientist.n.01\n",
      "terrm psychoanalysis.n.01 term_next practice.n.04\n",
      "terrm psychoanalysis.n.01 term_next logic.n.05\n",
      "terrm psychoanalysis.n.01 term_next span.n.04\n",
      "terrm psychoanalysis.n.01 term_next terminus.n.03\n",
      "terrm psychoanalysis.n.01 term_next outline.n.02\n",
      "computer_architecture.n.02\n",
      "terrm computer_architecture.n.02 term_next graphics.n.02\n",
      "terrm computer_architecture.n.02 term_next mathematics.n.01\n",
      "terrm computer_architecture.n.02 term_next scientist.n.01\n",
      "terrm computer_architecture.n.02 term_next practice.n.04\n",
      "terrm computer_architecture.n.02 term_next logic.n.05\n",
      "terrm computer_architecture.n.02 term_next span.n.04\n",
      "terrm computer_architecture.n.02 term_next terminus.n.03\n",
      "terrm computer_architecture.n.02 term_next outline.n.02\n",
      "technique.n.01\n",
      "terrm technique.n.01 term_next graphics.n.02\n",
      "terrm technique.n.01 term_next mathematics.n.01\n",
      "terrm technique.n.01 term_next scientist.n.01\n",
      "terrm technique.n.01 term_next practice.n.04\n",
      "terrm technique.n.01 term_next logic.n.05\n",
      "terrm technique.n.01 term_next span.n.04\n",
      "terrm technique.n.01 term_next terminus.n.03\n",
      "terrm technique.n.01 term_next outline.n.02\n",
      "software.n.01\n",
      "terrm software.n.01 term_next graphics.n.02\n",
      "terrm software.n.01 term_next mathematics.n.01\n",
      "terrm software.n.01 term_next scientist.n.01\n",
      "terrm software.n.01 term_next practice.n.04\n",
      "terrm software.n.01 term_next logic.n.05\n",
      "terrm software.n.01 term_next span.n.04\n",
      "terrm software.n.01 term_next terminus.n.03\n",
      "terrm software.n.01 term_next outline.n.02\n",
      "trouble.n.01\n",
      "terrm trouble.n.01 term_next graphics.n.02\n",
      "terrm trouble.n.01 term_next mathematics.n.01\n",
      "terrm trouble.n.01 term_next scientist.n.01\n",
      "terrm trouble.n.01 term_next practice.n.04\n",
      "terrm trouble.n.01 term_next logic.n.05\n",
      "terrm trouble.n.01 term_next span.n.04\n",
      "terrm trouble.n.01 term_next terminus.n.03\n",
      "terrm trouble.n.01 term_next outline.n.02\n",
      "algorithm.n.01\n",
      "terrm algorithm.n.01 term_next graphics.n.02\n",
      "terrm algorithm.n.01 term_next mathematics.n.01\n",
      "terrm algorithm.n.01 term_next scientist.n.01\n",
      "terrm algorithm.n.01 term_next practice.n.04\n",
      "terrm algorithm.n.01 term_next logic.n.05\n",
      "terrm algorithm.n.01 term_next span.n.04\n",
      "terrm algorithm.n.01 term_next terminus.n.03\n",
      "terrm algorithm.n.01 term_next outline.n.02\n",
      "communication.n.03\n",
      "terrm communication.n.03 term_next graphics.n.02\n",
      "terrm communication.n.03 term_next mathematics.n.01\n",
      "terrm communication.n.03 term_next scientist.n.01\n",
      "terrm communication.n.03 term_next practice.n.04\n",
      "terrm communication.n.03 term_next logic.n.05\n",
      "terrm communication.n.03 term_next span.n.04\n",
      "terrm communication.n.03 term_next terminus.n.03\n",
      "terrm communication.n.03 term_next outline.n.02\n",
      "system.n.08\n",
      "terrm system.n.08 term_next graphics.n.02\n",
      "terrm system.n.08 term_next mathematics.n.01\n",
      "terrm system.n.08 term_next scientist.n.01\n",
      "terrm system.n.08 term_next practice.n.04\n",
      "terrm system.n.08 term_next logic.n.05\n",
      "terrm system.n.08 term_next span.n.04\n",
      "terrm system.n.08 term_next terminus.n.03\n",
      "terrm system.n.08 term_next outline.n.02\n",
      "engineering.n.03\n",
      "terrm engineering.n.03 term_next graphics.n.02\n",
      "terrm engineering.n.03 term_next mathematics.n.01\n",
      "terrm engineering.n.03 term_next scientist.n.01\n",
      "terrm engineering.n.03 term_next practice.n.04\n",
      "terrm engineering.n.03 term_next logic.n.05\n",
      "terrm engineering.n.03 term_next span.n.04\n",
      "terrm engineering.n.03 term_next terminus.n.03\n",
      "terrm engineering.n.03 term_next outline.n.02\n",
      "previous_file_name /home/fsg/Desktop/csv-fatma/sub_word_tf/cscs2.csv\n",
      "pure_file_name cscs2.csv\n",
      "design.n.06\n",
      "terrm design.n.06 term_next meet.n.01\n",
      "terrm design.n.06 term_next one.n.02\n",
      "terrm design.n.06 term_next fundamental.n.02\n",
      "terrm design.n.06 term_next concept.n.01\n",
      "terrm design.n.06 term_next lotion.n.02\n",
      "terrm design.n.06 term_next interaction.n.02\n",
      "terrm design.n.06 term_next frankincense.n.01\n",
      "terrm design.n.06 term_next focus.n.07\n",
      "terrm design.n.06 term_next particular.n.03\n",
      "terrm design.n.06 term_next well.n.05\n",
      "terrm design.n.06 term_next understanding.n.01\n",
      "terrm design.n.06 term_next intelligence.n.02\n",
      "terrm design.n.06 term_next stipulation.n.03\n",
      "discipline.n.02\n",
      "terrm discipline.n.02 term_next meet.n.01\n",
      "terrm discipline.n.02 term_next one.n.02\n",
      "terrm discipline.n.02 term_next fundamental.n.02\n",
      "terrm discipline.n.02 term_next concept.n.01\n",
      "terrm discipline.n.02 term_next lotion.n.02\n",
      "terrm discipline.n.02 term_next interaction.n.02\n",
      "terrm discipline.n.02 term_next frankincense.n.01\n",
      "terrm discipline.n.02 term_next focus.n.07\n",
      "terrm discipline.n.02 term_next particular.n.03\n",
      "terrm discipline.n.02 term_next well.n.05\n",
      "terrm discipline.n.02 term_next understanding.n.01\n",
      "terrm discipline.n.02 term_next intelligence.n.02\n",
      "terrm discipline.n.02 term_next stipulation.n.03\n",
      "psychoanalysis.n.01\n",
      "terrm psychoanalysis.n.01 term_next meet.n.01\n",
      "terrm psychoanalysis.n.01 term_next one.n.02\n",
      "terrm psychoanalysis.n.01 term_next fundamental.n.02\n",
      "terrm psychoanalysis.n.01 term_next concept.n.01\n",
      "terrm psychoanalysis.n.01 term_next lotion.n.02\n",
      "terrm psychoanalysis.n.01 term_next interaction.n.02\n",
      "terrm psychoanalysis.n.01 term_next frankincense.n.01\n",
      "terrm psychoanalysis.n.01 term_next focus.n.07\n",
      "terrm psychoanalysis.n.01 term_next particular.n.03\n",
      "terrm psychoanalysis.n.01 term_next well.n.05\n",
      "terrm psychoanalysis.n.01 term_next understanding.n.01\n",
      "terrm psychoanalysis.n.01 term_next intelligence.n.02\n",
      "terrm psychoanalysis.n.01 term_next stipulation.n.03\n",
      "computer_architecture.n.02\n",
      "terrm computer_architecture.n.02 term_next meet.n.01\n",
      "terrm computer_architecture.n.02 term_next one.n.02\n",
      "terrm computer_architecture.n.02 term_next fundamental.n.02\n",
      "terrm computer_architecture.n.02 term_next concept.n.01\n",
      "terrm computer_architecture.n.02 term_next lotion.n.02\n",
      "terrm computer_architecture.n.02 term_next interaction.n.02\n",
      "terrm computer_architecture.n.02 term_next frankincense.n.01\n",
      "terrm computer_architecture.n.02 term_next focus.n.07\n",
      "terrm computer_architecture.n.02 term_next particular.n.03\n",
      "terrm computer_architecture.n.02 term_next well.n.05\n",
      "terrm computer_architecture.n.02 term_next understanding.n.01\n",
      "terrm computer_architecture.n.02 term_next intelligence.n.02\n",
      "terrm computer_architecture.n.02 term_next stipulation.n.03\n",
      "technique.n.01\n",
      "terrm technique.n.01 term_next meet.n.01\n",
      "terrm technique.n.01 term_next one.n.02\n",
      "terrm technique.n.01 term_next fundamental.n.02\n",
      "terrm technique.n.01 term_next concept.n.01\n",
      "terrm technique.n.01 term_next lotion.n.02\n",
      "terrm technique.n.01 term_next interaction.n.02\n",
      "terrm technique.n.01 term_next frankincense.n.01\n",
      "terrm technique.n.01 term_next focus.n.07\n",
      "terrm technique.n.01 term_next particular.n.03\n",
      "terrm technique.n.01 term_next well.n.05\n",
      "terrm technique.n.01 term_next understanding.n.01\n",
      "terrm technique.n.01 term_next intelligence.n.02\n",
      "terrm technique.n.01 term_next stipulation.n.03\n",
      "software.n.01\n",
      "terrm software.n.01 term_next meet.n.01\n",
      "terrm software.n.01 term_next one.n.02\n",
      "terrm software.n.01 term_next fundamental.n.02\n",
      "terrm software.n.01 term_next concept.n.01\n",
      "terrm software.n.01 term_next lotion.n.02\n",
      "terrm software.n.01 term_next interaction.n.02\n",
      "terrm software.n.01 term_next frankincense.n.01\n",
      "terrm software.n.01 term_next focus.n.07\n",
      "terrm software.n.01 term_next particular.n.03\n",
      "terrm software.n.01 term_next well.n.05\n",
      "terrm software.n.01 term_next understanding.n.01\n",
      "terrm software.n.01 term_next intelligence.n.02\n",
      "terrm software.n.01 term_next stipulation.n.03\n",
      "trouble.n.01\n",
      "terrm trouble.n.01 term_next meet.n.01\n",
      "terrm trouble.n.01 term_next one.n.02\n",
      "terrm trouble.n.01 term_next fundamental.n.02\n",
      "terrm trouble.n.01 term_next concept.n.01\n",
      "terrm trouble.n.01 term_next lotion.n.02\n",
      "terrm trouble.n.01 term_next interaction.n.02\n",
      "terrm trouble.n.01 term_next frankincense.n.01\n",
      "terrm trouble.n.01 term_next focus.n.07\n",
      "terrm trouble.n.01 term_next particular.n.03\n",
      "terrm trouble.n.01 term_next well.n.05\n",
      "terrm trouble.n.01 term_next understanding.n.01\n",
      "terrm trouble.n.01 term_next intelligence.n.02\n",
      "terrm trouble.n.01 term_next stipulation.n.03\n",
      "algorithm.n.01\n",
      "terrm algorithm.n.01 term_next meet.n.01\n",
      "terrm algorithm.n.01 term_next one.n.02\n",
      "terrm algorithm.n.01 term_next fundamental.n.02\n",
      "terrm algorithm.n.01 term_next concept.n.01\n",
      "terrm algorithm.n.01 term_next lotion.n.02\n",
      "terrm algorithm.n.01 term_next interaction.n.02\n",
      "terrm algorithm.n.01 term_next frankincense.n.01\n",
      "terrm algorithm.n.01 term_next focus.n.07\n",
      "terrm algorithm.n.01 term_next particular.n.03\n",
      "terrm algorithm.n.01 term_next well.n.05\n",
      "terrm algorithm.n.01 term_next understanding.n.01\n",
      "terrm algorithm.n.01 term_next intelligence.n.02\n",
      "terrm algorithm.n.01 term_next stipulation.n.03\n",
      "communication.n.03\n",
      "terrm communication.n.03 term_next meet.n.01\n",
      "terrm communication.n.03 term_next one.n.02\n",
      "terrm communication.n.03 term_next fundamental.n.02\n",
      "terrm communication.n.03 term_next concept.n.01\n",
      "terrm communication.n.03 term_next lotion.n.02\n",
      "terrm communication.n.03 term_next interaction.n.02\n",
      "terrm communication.n.03 term_next frankincense.n.01\n",
      "terrm communication.n.03 term_next focus.n.07\n",
      "terrm communication.n.03 term_next particular.n.03\n",
      "terrm communication.n.03 term_next well.n.05\n",
      "terrm communication.n.03 term_next understanding.n.01\n",
      "terrm communication.n.03 term_next intelligence.n.02\n",
      "terrm communication.n.03 term_next stipulation.n.03\n",
      "system.n.08\n",
      "terrm system.n.08 term_next meet.n.01\n",
      "terrm system.n.08 term_next one.n.02\n",
      "terrm system.n.08 term_next fundamental.n.02\n",
      "terrm system.n.08 term_next concept.n.01\n",
      "terrm system.n.08 term_next lotion.n.02\n",
      "terrm system.n.08 term_next interaction.n.02\n",
      "terrm system.n.08 term_next frankincense.n.01\n",
      "terrm system.n.08 term_next focus.n.07\n",
      "terrm system.n.08 term_next particular.n.03\n",
      "terrm system.n.08 term_next well.n.05\n",
      "terrm system.n.08 term_next understanding.n.01\n",
      "terrm system.n.08 term_next intelligence.n.02\n",
      "terrm system.n.08 term_next stipulation.n.03\n",
      "engineering.n.03\n",
      "terrm engineering.n.03 term_next meet.n.01\n",
      "terrm engineering.n.03 term_next one.n.02\n",
      "terrm engineering.n.03 term_next fundamental.n.02\n",
      "terrm engineering.n.03 term_next concept.n.01\n",
      "terrm engineering.n.03 term_next lotion.n.02\n",
      "terrm engineering.n.03 term_next interaction.n.02\n",
      "terrm engineering.n.03 term_next frankincense.n.01\n",
      "terrm engineering.n.03 term_next focus.n.07\n",
      "terrm engineering.n.03 term_next particular.n.03\n",
      "terrm engineering.n.03 term_next well.n.05\n",
      "terrm engineering.n.03 term_next understanding.n.01\n",
      "terrm engineering.n.03 term_next intelligence.n.02\n",
      "terrm engineering.n.03 term_next stipulation.n.03\n",
      "finesed sim_terms_previous_file\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(device_count={processor.upper():int(all_gpus)},\n",
    "                        allow_soft_placement=True,\n",
    "                        inter_op_parallelism_threads=1,\n",
    "                        intra_op_parallelism_threads=1,\n",
    "                        use_per_session_threads=True,\n",
    "                        log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "#import threading\n",
    "import timeit\n",
    "from tensorflow.python.client import timeline\n",
    "#start_gpu_memory=gpu_memory_map(gpuN+\"gpu_befor_session_memory.txt\",gpuN+\"gpu_befor_session_out.txt\")\n",
    "\n",
    "#save_txt(str(start_gpu_memory),gpuN+\"gpu_befor_session_memory_map.txt\")\n",
    "\n",
    "#print(\"gpu_memory_map\",start_gpu_memory)\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    print(\"************Started Session Main Process*************\")\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #start_gpu_memory=gpu_memory_map(gpuN+\"gpu_after_start_session_memory.txt\",gpuN+\"gpu_after_start_session_out.txt\")\n",
    "    gpu_name='/'+processor+':0'    \n",
    "    #print(\"gpu_memory_map_after_session\",start_gpu_memory)       \n",
    "    with tf.device(gpu_name):#'/cpu'):\n",
    "        print(\"************Started Session CPU *************\")\n",
    "        #start = timeit.default_timer()\n",
    "          \n",
    "        file_list_task=read_last_file_list(path_data_base+path_non_redundant,\".csv\")\n",
    "        for i in range (len(file_list_task)):\n",
    "            print(file_list_task[i])\n",
    "            task_no=name_file_no(file_list_task[i])\n",
    "            print(\"takno\",task_no)\n",
    "            #print(\"file_list_task\",file_list_task)\n",
    "            #sub_file_list_task=sub_list_file(file_list_task,int(all_gpus))[int(gpuN)]\n",
    "            #print(\"sub_file_list_task\",sub_file_list_task)\n",
    "            #gpu_no=distrupited_task_gpu(int(task_no),int(all_gpus))\n",
    "            #gpu_name='/gpu:'+str(gpu_no)\n",
    "\n",
    "            #with tf.device(gpu_name):\n",
    "            print(\"************Started Session GPU *************\")\n",
    "            #print(\"sublist\",sub_file_list_task,\"gpu_name\",gpu_name)\n",
    "            #print(\"task_no\",task_no,\"gpu_name\",gpu_name)\n",
    "            #print(\"In session\",file_path_sim)\n",
    "            #filename=\"split\"+task_no+\".csv\" #\"cs\"+task_no+\".txt\" \n",
    "            #totals_filename=file_path+filename\n",
    "            filename=full_name_file(file_list_task[i])#full_name_file(file_list_task[int(task_no)])\n",
    "            #print(filename)\n",
    "            gpu_full_process(filename,path_data_base,path_tf,path_non_redundant,path_idf,file_path_sim,str(task_no))\n",
    "            #Your statements here\n",
    "\n",
    "            #stop = timeit.default_timer()\n",
    "\n",
    "            #print (\"tttttttttttttt\",stop - start )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tf.summary.FileWriter(\"/home/fsg/logs_tf\", g).close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
